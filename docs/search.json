[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for Introduction to Computational Linguistics–a course given by Aaron Steven White at the University of Rochester."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "About",
    "section": "About the course",
    "text": "About the course\nThis course covers foundational concepts in computational linguistics. Major focus is placed on the use of formal languages as a tool for understanding natural language as well as on developing students’ ability to implement foundational algorithms pertaining to those formal languages. Topics include basic formal language theory, finite state phonological and morphological parsing, and syntactic parsing for context free grammars and mildly context sensitive formalisms."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "About",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course relies on concepts covered in an introductory linguistics course and an introductory programming course. With respect to the latter, it specifically assumes that you can competently write scripts that do non-trivial things and can work competently with Python’s object-oriented programming facilities but maybe not develop a package on your own."
  },
  {
    "objectID": "index.html#about-the-instructor",
    "href": "index.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nAaron Steven White is an Associate Professor of Linguistics and Computer Science at the University of Rochester, where he directs the Formal and Computational Semantics lab (FACTS.lab). His research investigates the relationship between linguistic expressions and conceptual categories that undergird the human ability to convey information about possible past, present, and future configurations of things in the world.\nIn addition to being a principal investigator on numerous federally funded grants and contracts, White is the recipient of a National Science Foundation Faculty Early Career Development (CAREER) award. His work has appeared in a variety linguistics, cognitive science, and natural language processing venues, including Semantics & Pragmatics, Glossa, Language Acquisition, Cognitive Science, Cognitive Psychology, Transactions of the Association for Computational Linguistics, and Empirical Methods in Natural Language Processing."
  },
  {
    "objectID": "index.html#about-the-site",
    "href": "index.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/intro-to-cl. See Installation for information on how to run the code documented here."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe development of these materials was supported by the University of Rochester and a National Science Foundation grant: CAREER: Logical Form Induction (BCS/IIS-2237175)."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nIntroduction to Computational Linguistics by Aaron Steven White is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on a work at https://github.com/aaronstevenwhite/intro-to-cl."
  },
  {
    "objectID": "installation.html",
    "href": "installation.html",
    "title": "Installation",
    "section": "",
    "text": "The site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/intro-to-cl. You can obtain the files by cloning this repo.\nAll further code on this page assumes that you are inside of this cloned repo."
  },
  {
    "objectID": "installation.html#installing-quarto-and-extensions",
    "href": "installation.html#installing-quarto-and-extensions",
    "title": "Installation",
    "section": "Installing Quarto and extensions",
    "text": "Installing Quarto and extensions\nTo build this site, you will need to install Quarto as well as its include-code-files and line-highlight extensions.\nquarto add quarto-ext/include-code-files\nquarto add shafayetShafee/line-highlight\nThese extensions are mainly used for including and highlighting parts of external files."
  },
  {
    "objectID": "installation.html#building-the-docker-container",
    "href": "installation.html#building-the-docker-container",
    "title": "Installation",
    "section": "Building the Docker container",
    "text": "Building the Docker container\nAll pages that have executed code blocks are generated from jupyter notebooks, which were run within a Docker container constructed using the Dockerfile contained in this repo.\nFROM jupyter/minimal-notebook:x86_64-python-3.11.6\n\n\n\nRUN conda install -c conda-forge numpy pynini stanza rdflib\nAssuming you have Docker installed, the image can be built using:\ndocker build --platform linux/amd64 -t intro-to-cl .\nA container based on this image can then be constructed using:\ndocker run -it --rm -p 8888:8888 -v \"${PWD}\":/home/jovyan/work intro-to-cl\nTo access jupyter, simply copy the link provided when running this command. It should look something like this (though your access tokens will differ):\nTo access the server, open this file in a browser:\n    file:///home/jovyan/.local/share/jupyter/runtime/jpserver-8-open.html\nOr copy and paste one of these URLs:\n    http://4738b6192fb0:8888/lab?token=8fc165776e7e99c98ec19883f750071a187e85a0a9253b81\n    http://127.0.0.1:8888/lab?token=8fc165776e7e99c98ec19883f750071a187e85a0a9253b81\nYou can change the port that docker forwards to by changing the first 8888 in the -p 8888:8888 option–e.g. to redirect port 10000 -p 10000:8888. Just remember to correspondingly change the port you attempt to access in your browser: so even though the message above has you accessing port 8888, that’s the docker container’s port 8888, which forwards to your machine’s 10000."
  },
  {
    "objectID": "formal-and-practical-preliminaries/index.html",
    "href": "formal-and-practical-preliminaries/index.html",
    "title": "Overview",
    "section": "",
    "text": "In this first module of the course, we are going to focus on developing a set of formal and practical tools that we will use through the rest of the course."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/index.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/index.html",
    "title": "Overview",
    "section": "",
    "text": "Before getting to the fun parts of this course, we need to develop some basic formal tools. In this submodule, we’ll focus on some core concepts in naïve set theory and objects—such as relations and functions—constructed with these concepts. We’ll then use these concepts to develop a formal concept of strings on an alphabet and the set of all languages constructed from those strings.\nAs we develop these formal tools, we will also see how they are implemented in Python. I’m going to assume that you have some basic Python under your belt: that you can competently write scripts that do non-trivial things and can work competently with Python’s object-oriented programming facilities but maybe not develop a package on your own."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sets.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sets.html",
    "title": "Sets",
    "section": "",
    "text": "Sets are unordered, uniqued collections of things. One way to represent sets is by placing (representations of) their elements between curly braces.\nFor instance, we can represent the set of vowel phonemes in English in the following way.\n\\[V_1 \\equiv \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\nWe express that something is an element of a set using the notation \\(\\cdot \\in \\cdot\\) and that it is not an element using the notation \\(\\cdot \\not\\in \\cdot\\). So for instance, \\(\\text{e}\\) is an element of \\(V_1\\), while \\(\\text{t}\\) is not.\n\\[\\text{e} \\in V_1 \\quad \\text{t} \\not\\in V_1\\]\nTo work with sets in Python, we can use the standard library’s set type. These objects function as we would expect in terms of elementhood.\nvowels_1: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\n\nif \"e\" in vowels_1:\n    print(\"e ∈ V_1\")\nelse:\n    print(\"e ∉ V_1\")\n    \nif \"t\" in vowels_1:\n    print(\"t ∈ V_3\")\nelse:\n    print(\"t ∉ V_3\")\n\ne ∈ V_1\nt ∉ V_3\nSets can be represented in many ways. For instance, we could also represent the set \\(V_1\\) in this way:\n\\[V_2 \\equiv \\{\\text{o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ, e, i}\\}\\]\nBecause sets are unordered, both \\(V_1\\) and \\(V_2\\) are representations of the exact same set (\\(V_1 = V_2\\)). And because sets are uniqued, \\(V_3\\) is also a representation of the same set as \\(V_1\\) and \\(V_2\\)–i.e. \\(V_1=V_2=V_3\\)–even though there are multiple copies of some vowels in this representation.\n\\[V_3 \\equiv \\{\\text{o, o, o, u, u, æ, ɑ, ɔ, ə, ə, ə, ɛ, ɪ, ʊ, e, i}\\}\\]\nPython sets work as we would expect in terms of equality.\nvowels_2: set[str] = {\"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\", \"e\", \"i\"}\nvowels_3: set[str] = {\"o\", \"o\", \"o\", \"u\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ə\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\", \"e\", \"i\"}\n\nif vowels_1 == vowels_2:\n    print(\"V_1 = V_2\")\nelse:\n    print(\"V_1 ≠ V_2\")\n    \nif vowels_1 == vowels_3:\n    print(\"V_1 = V_3\")\nelse:\n    print(\"V_1 ≠ V_3\")\n\nV_1 = V_2\nV_1 = V_3\nI’ll just call this set \\(V \\equiv V_1 = V_2 = V_3\\) moving forward.\nvowels: set[str] = vowels_1\nPython has another way of representing sets that we will have reason to use: frozenset. These work very similarly to sets in a lot of ways.\nvowels_frozen: frozenset[str] = frozenset(vowels)\n\nif vowels == vowels_frozen:\n    print(\"V = V_frozen\")\nelse:\n    print(\"V ≠ V_frozen\")\n\nV = V_frozen\nOne big difference between the two is that sets are mutable, while frozensets are immutable. Basically, we can alter sets, but we can’t alter frozensets. For instance, we can add elements to a set but not a frozenset.\ntry:\n    vowels.add(\"t\")\n    print(\"Successfully added 't' to vowels.\")\nexcept AttributeError:\n    print(\"Failed to add 't' to vowels.\")\n\ntry:\n    vowels_frozen.add(\"t\")\n    print(\"Successfully added 't' to vowels_frozen.\")\nexcept AttributeError:\n    print(\"Failed to add 't' to vowels_frozen.\")\n\nSuccessfully added 't' to vowels.\nFailed to add 't' to vowels_frozen.\nSimilarly, we can remove elements from sets but not frozensets.\ntry:\n    vowels.remove(\"t\")\n    print(\"Successfully removed 't' to vowels.\")\nexcept AttributeError:\n    print(\"Failed to remove 't' to vowels.\")\n\ntry:\n    vowels_frozen.remove(\"ə\")\n    print(\"Successfully removed 'ə' to vowels_frozen.\")\nexcept AttributeError:\n    print(\"Failed to remove 'ə' to vowels_frozen.\")\n\nSuccessfully removed 't' to vowels.\nFailed to remove 'ə' to vowels_frozen.\nThis behavior makes frozensets seem pretty useless, since it would seem they can do fewer things with them. But frozensets turn out to have a really useful property: they can be elements of other sets or frozensets.\ntry:\n    vowels_singleton: set[set[str]] = {vowels}\n    print(\"Successfully constructed the set {V}.\")\nexcept TypeError:\n    print(\"Failed to construct the set {V}.\")\n\ntry:\n    vowels_frozen_singleton: set[frozenset[str]] = {vowels_frozen}\n    print(\"Successfully constructed the set {V_frozen}.\")\nexcept TypeError:\n    print(\"Failed to construct the set {V_frozen}.\")\n\nFailed to construct the set {V}.\nSuccessfully constructed the set {V_frozen}.\nThe reason frozensets can be elements of sets or frozensets is that they are hashable, while sets are not.1 We are going to use this property extensively throughout this course."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sets.html#footnotes",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sets.html#footnotes",
    "title": "Sets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere is a correlation between hashability and immutability, but they are not the same thing.↩︎"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/multisets.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/multisets.html",
    "title": "Multisets",
    "section": "",
    "text": "Multisets (or bags) are unordered, nonuniqued collections. In this course, we won’t spend too much time with these sorts of objects, but it’s useful to know the terminology.\nMultisets are often (somewhat confusingly) represented using the same notation as sets. For instance, the following is a multiset containing only vowels.\n\\[\\bar{V}_1 \\equiv \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\nAnd this is a representation of the same multiset, since multisets are unordered.\n\\[\\bar{V}_2 \\equiv \\{\\text{o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ, e, i}\\}\\]\nBut this is not a representation of a multiset, since multisets are nonuniqued.\n\\[\\bar{V}_3 \\equiv \\{\\text{o, o, o, u, u, æ, ɑ, ɔ, ə, ə, ə, ɛ, ɪ, ʊ, e, i}\\}\\]\nAnother way of saying this is that the multiplicity of a particular element matters in a multiset in a way it doesn’t matter in a set.\nWe often work with multisets in Python using dicts or specialized subclasses thereof. One special subclass of dict that is useful for representing multisets (and that you should know) is collections.Counter.\nThe nice thing about Counter is that it can be initialized with an iterable or mapping (such as a set) containing hashable objects (such as strs) and it will make a dictionary mapping the elements of that iterable/mapping to their multiplicity–i.e. how many times they show up in that iterable/mapping.1\nfrom pprint import pprint\nfrom collections import Counter\n\nvowels_bar_1: Counter[str] = Counter(\n    [\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"]\n)\n\npprint(vowels_bar_1)\n\nCounter({'e': 1,\n         'i': 1,\n         'o': 1,\n         'u': 1,\n         'æ': 1,\n         'ɑ': 1,\n         'ɔ': 1,\n         'ə': 1,\n         'ɛ': 1,\n         'ɪ': 1,\n         'ʊ': 1})\nvowels_bar_2: Counter[str] = Counter(\n    [\"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\", \"e\", \"i\"]\n)\n\npprint(vowels_bar_2)\n\nCounter({'o': 1,\n         'u': 1,\n         'æ': 1,\n         'ɑ': 1,\n         'ɔ': 1,\n         'ə': 1,\n         'ɛ': 1,\n         'ɪ': 1,\n         'ʊ': 1,\n         'e': 1,\n         'i': 1})\nvowels_bar_3: Counter[str] = Counter(\n    [\"o\", \"o\", \"o\", \"u\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ə\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\", \"e\", \"i\"]\n)\n\npprint(vowels_bar_3)\n\nCounter({'o': 3,\n         'ə': 3,\n         'u': 2,\n         'æ': 1,\n         'ɑ': 1,\n         'ɔ': 1,\n         'ɛ': 1,\n         'ɪ': 1,\n         'ʊ': 1,\n         'e': 1,\n         'i': 1})\nAnd Counters behave as we would expect of a multiset–at least in terms of equality.\nif vowels_bar_1 == vowels_bar_2:\n    print(\"Vbar_1 = Vbar_2\")\nelse:\n    print(\"Vbar_1 ≠ Vbar_2\")\n    \nif vowels_bar_1 == vowels_bar_3:\n    print(\"Vbar_1 = Vbar_3\")\nelse:\n    print(\"Vbar_1 ≠ Vbar_3\")\n\nVbar_1 = Vbar_2\nVbar_1 ≠ Vbar_3"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/multisets.html#footnotes",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/multisets.html#footnotes",
    "title": "Multisets",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nNote that I’m passing the vowels in the multiset as a list to Counter. We crucially don’t want to pass them as a set, because that would destroy the multiplicities of the items.↩︎"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/cardinality.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/cardinality.html",
    "title": "Cardinality",
    "section": "",
    "text": "The number of things in a set is its cardinality.\n\\[|V| = |\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}| = 11\\]\nIn Python, we compute the cardinality using len.\n\nvowels: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\n\nprint(f\"|V| = {len(vowels)}\")\n\n|V| = 11\n\n\nSets can have infinite cardinality. For instance, the set of natural numbers is infinite.\n\\[\\mathbb{N} = \\{0, 1, 2, 3, \\ldots\\}\\text{ (or }\\{1, 2, 3, \\ldots\\})\\]\nWe unfortunately can’t use set to work with infinite sets in Python. Instead, we have to use generators. One way to initialize a generator is to define a function containing a yield statement.\n\nfrom collections.abc import Generator\n\ndef natural_numbers() -&gt; int:\n    \"\"\"Initialize a generator for the natural numbers\"\"\"\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n# initialize a generator of the natural numbers\nN: Generator[int] = natural_numbers()\n\n# print the first 10 natural numbers\nfor i in N:\n  if i &lt; 10:\n    print(i)\n  else:\n    break\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html",
    "title": "Set relations",
    "section": "",
    "text": "Subcollections of elements of a set are subsets (of that set)\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\subseteq \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nvowels: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\nhigh_vowels: set[str] = {'u', 'ʊ', 'i', 'ɪ'}\n\nif high_vowels &lt;= vowels:\n    print(f\"{high_vowels} ⊆ {vowels}\")\nelse:\n    print(f\"{high_vowels} ⊄ {vowels} ∨ {high_vowels} ≠ {vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} ⊆ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n\n\nA set is an improper subset of itself\n\\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\subseteq \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nif vowels &lt;= vowels:\n    print(f\"{vowels} ⊆ {vowels}\")\nelse:\n    print(f\"{vowels} ⊄ {vowels} ∨ {vowels} ≠ {vowels}\")\n\n{'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'} ⊆ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n\n\nAll other sets of a subset are proper subsets.\n\\[\\{\\text{i}\\} \\subset \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nif {'i'} &lt; vowels:\n    print(f\"{{'i'}} ⊂ {vowels}\")\nelse:\n    print(f\"{{'i'}} ⊄ {vowels}\")\n\nif vowels &lt; vowels:\n    print(f\"{vowels} ⊂ {vowels}\")\nelse:\n    print(f\"{vowels} ⊄ {vowels}\")\n\n{'i'} ⊂ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n{'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'} ⊄ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#subsets",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#subsets",
    "title": "Set relations",
    "section": "",
    "text": "Subcollections of elements of a set are subsets (of that set)\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\subseteq \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nvowels: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\nhigh_vowels: set[str] = {'u', 'ʊ', 'i', 'ɪ'}\n\nif high_vowels &lt;= vowels:\n    print(f\"{high_vowels} ⊆ {vowels}\")\nelse:\n    print(f\"{high_vowels} ⊄ {vowels} ∨ {high_vowels} ≠ {vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} ⊆ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n\n\nA set is an improper subset of itself\n\\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\subseteq \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nif vowels &lt;= vowels:\n    print(f\"{vowels} ⊆ {vowels}\")\nelse:\n    print(f\"{vowels} ⊄ {vowels} ∨ {vowels} ≠ {vowels}\")\n\n{'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'} ⊆ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n\n\nAll other sets of a subset are proper subsets.\n\\[\\{\\text{i}\\} \\subset \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nif {'i'} &lt; vowels:\n    print(f\"{{'i'}} ⊂ {vowels}\")\nelse:\n    print(f\"{{'i'}} ⊄ {vowels}\")\n\nif vowels &lt; vowels:\n    print(f\"{vowels} ⊂ {vowels}\")\nelse:\n    print(f\"{vowels} ⊄ {vowels}\")\n\n{'i'} ⊂ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n{'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'} ⊄ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#supersets",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#supersets",
    "title": "Set relations",
    "section": "Supersets",
    "text": "Supersets\nThe dual of subset is superset.\n\\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\supseteq \\{\\text{i}\\}\\] \\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\supseteq \\{\\text{i, u, ɪ, ʊ}\\}\\]\nA set is an improper superset of itself\n\\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\supseteq \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\nAll other supersets of a set are proper supersets.\n\\[\\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\} \\supset \\{\\text{i}\\}\\]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#empty-set",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#empty-set",
    "title": "Set relations",
    "section": "Empty set",
    "text": "Empty set\nThe empty set \\(\\emptyset\\) is a set containing no elements.\n\\[\\emptyset \\equiv \\{\\}\\]\n\nemptyset: set = set()\n\nThe empty set is a subset of all sets.\n\\[\\emptyset \\subset \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\n\nif emptyset &lt; vowels:\n    print(f\"∅ ⊂ {vowels}\")\nelse:\n    print(f\"∅ ⊄ {vowels}\")\n\n∅ ⊂ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n\n\nThe empty set is not in all sets, though it is in some sets\n\\[\\emptyset \\not\\in \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\] \\[\\emptyset \\in \\{\\emptyset, \\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\]\n\nvowels_with_empty: set[str] = set(vowels)\nvowels_with_empty.add(frozenset(emptyset))\n\nif emptyset in vowels:\n    print(f\"∅ ∈ {vowels}\")\nelse:\n    print(f\"∅ ∉ {vowels}\")\n    \nif emptyset in vowels_with_empty:\n    print(f\"∅ ∈ {vowels_with_empty}\")\nelse:\n    print(f\"∅ ∉ {vowels_with_empty}\")\n\n∅ ∉ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', 'ɑ', 'ɔ', 'o', 'e'}\n∅ ∈ {'i', 'ɛ', 'æ', 'ɪ', 'ə', 'ʊ', 'u', frozenset(), 'ɑ', 'ɔ', 'o', 'e'}"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#intersection",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#intersection",
    "title": "Set relations",
    "section": "Intersection",
    "text": "Intersection\nThe intersection of two sets is the set of their shared elements. For instance, if we take the intersection of the set of high vowels with the set of back vowels, we get the high back vowels.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\cap \\{\\text{u, ʊ, o, ɔ, ɑ}\\} = \\{\\text{u, ʊ}\\}\\]\n\nback_vowels: set[str] = {'u', 'ʊ', 'ɑ', 'ɔ', 'o'}\n\nprint(f\"{high_vowels} ∩ {back_vowels} = {high_vowels & back_vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} ∩ {'u', 'ɑ', 'ɔ', 'o', 'ʊ'} = {'u', 'ʊ'}\n\n\nThe intersection of a set with a subset of that set is that subset.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\cap \\{\\text{u, ʊ}\\} = \\{\\text{u, ʊ}\\}\\] \\[\\{\\text{i, u, ɪ, ʊ}\\} \\cap \\emptyset = \\emptyset\\]\n\nhigh_back_vowels: set[str] = {'u', 'ʊ'}\n\nprint(f\"{high_vowels} ∩ {high_back_vowels} = {high_vowels & high_back_vowels}\") \n\n{'u', 'i', 'ɪ', 'ʊ'} ∩ {'u', 'ʊ'} = {'u', 'ʊ'}\n\n\nIntersection can yield the empty set.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\cap \\{\\text{o, ɔ}\\} = \\emptyset\\]\n\nmid_back_vowels: set[str] = {'o', 'ɔ'}\n\nprint(f\"{high_vowels} ∩ {mid_back_vowels} = {high_vowels & mid_back_vowels}\") \n\n{'u', 'i', 'ɪ', 'ʊ'} ∩ {'ɔ', 'o'} = set()"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#union",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#union",
    "title": "Set relations",
    "section": "Union",
    "text": "Union\nThe union of two sets is the set of elements in both. For instance, if we take the union of the set of high vowels with the set of back vowels, we get the set of high and/or back vowels.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\cup \\{\\text{u, ʊ, o, ɔ, ɑ}\\} = \\{\\text{i, ɪ, u, ʊ, o, ɔ, ɑ}\\}\\]\n\nprint(f\"{high_vowels} ∪ {back_vowels} = {high_vowels | back_vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} ∪ {'u', 'ɑ', 'ɔ', 'o', 'ʊ'} = {'i', 'ɪ', 'ʊ', 'u', 'ɑ', 'ɔ', 'o'}\n\n\nThe union of a set with itself or one of its subsets (including the empty set) is that set.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} \\cup \\{\\text{i, u, ɪ, ʊ}\\} = \\{\\text{i, u, ɪ, ʊ}\\}\\] \\[\\{\\text{i, u, ɪ, ʊ}\\} \\cup \\{\\text{u, ʊ}\\} = \\{\\text{i, u, ɪ, ʊ}\\}\\]\n\nprint(f\"{high_vowels} ∪ {high_back_vowels} = {high_vowels | high_back_vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} ∪ {'u', 'ʊ'} = {'u', 'i', 'ɪ', 'ʊ'}\n\n\nThe + operator does not work for sets like it does for lists! You need to use | or union() explicitly.\n\ntry:\n    high_vowels + high_back_vowels\nexcept TypeError:\n    print(\"+ for sets does not implement union!\")\n\n+ for sets does not implement union!"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#set-builder-notation",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#set-builder-notation",
    "title": "Set relations",
    "section": "Set builder notation",
    "text": "Set builder notation\nIt is commonly the case that we want to filter a larger set–e.g. the vowels–down to a set containing only elements of that set with particular properties. For instance, suppose we want the high front vowels and we know how to check whether a vowel is high and whether it is front. We could describe the high front vowels using set-builder notation.\n\\[\\{x \\in \\text{vowels} \\mid x \\text{ is high and } x \\text{ is front}\\}\\]\nSet-builder notation can be implemented using set comprehensions.\n\nvowels: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\n\ndef is_high(x: str) -&gt; bool:\n  return x in {\"i\", \"u\", \"ɪ\", \"ʊ\"}\n\ndef is_front(x: str) -&gt; bool:\n  return x in {\"i\", \"ɪ\", \"e\", \"æ\", \"ɛ\"}\n\n{v for v in vowels if is_high(v) and is_front(v)}\n\n{'i', 'ɪ'}\n\n\nNote that:\n\\[\\{x \\in \\text{vowels} \\mid x \\text{ is high and } x \\text{ is front}\\} = \\{x \\in \\text{vowels} \\mid x \\text{ is high}\\} \\cap \\{x \\in \\text{vowels} \\mid x \\text{ is front}\\}\\]\n\n{v for v in vowels if is_high(v)} & {v for v in vowels if is_front(v)}\n\n{'i', 'ɪ'}\n\n\nThis fact will be important for your first homework."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#complement",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#complement",
    "title": "Set relations",
    "section": "Complement",
    "text": "Complement\nThe (absolute) complement of a set \\(A\\) relative to a universe \\(U\\) (a possibly improper superset of \\(A\\)) is all elements in \\(U\\) that are not in \\(A\\).\n\\[A^\\complement = \\overline{A} = A' = \\{x\\;|\\;x \\in U \\land x \\not\\in A\\}\\]\nFor instance, if \\(U \\equiv \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ}\\}\\), then the complement of the high vowels is the non-high vowels.\n\\[\\{\\text{i, u, ɪ, ʊ}\\}^\\complement = \\{\\text{e, o, æ, ɑ, ɔ, ə, ɛ}\\}\\]\nNote that \\(U = A \\cup \\overline{A}\\)."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#set-difference",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/set-relations.html#set-difference",
    "title": "Set relations",
    "section": "Set difference",
    "text": "Set difference\nThe set difference (or relative complement) of a set \\(A\\) relative to another set \\(B\\) is all elements in \\(B\\) that are not in \\(A\\)\n\\[B - A = \\{x\\;|\\;x \\in B \\land x \\not\\in A\\}\\]\nFor instance, the difference of the set of high vowels relative to the set of high back vowels, is the high non-back vowels.\n\\[\\{\\text{i, u, ɪ, ʊ}\\} - \\{\\text{u, ʊ}\\} = \\{\\text{i, ɪ}\\}\\]\n\nprint(f\"{high_vowels} - {high_back_vowels} = {high_vowels - high_back_vowels}\")\n\n{'u', 'i', 'ɪ', 'ʊ'} - {'u', 'ʊ'} = {'i', 'ɪ'}"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/power-sets.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/power-sets.html",
    "title": "Power Sets",
    "section": "",
    "text": "The set of all subsets of a set is its power set.\n\\[\\mathcal{P}(A) = 2^A = \\{X \\mid X \\subseteq A\\}\\]\nFor example, for the set \\(\\{\\text{i}, \\text{u}, \\text{ə}\\}\\), we have:\n\\[\\mathcal{P}(\\{\\text{i},\\text{u},\\text{ə}\\}) = 2^{\\{\\text{i},\\text{u},\\text{ə}\\}} = \\{\\emptyset, \\{\\text{i}\\}, \\{\\text{u}\\}, \\{\\text{ə}\\}, \\{\\text{i}, \\text{u}\\}, \\{\\text{u},\\text{ə}\\}, \\{\\text{i},\\text{ə}\\}, \\{\\text{i}, \\text{u}, \\text{ə}\\}\\}\\]\nTo obtain the power set of some set, we can loop through all possible subset cardinalities, and use the itertools.combinations function to obtain all subsets of our set of interest (the high vowels) of a particular cardinality. To do this, we need a second loop over the output of itertools.combinations at each cardinality is necessary to flatten the sets.\n\nfrom itertools import combinations\n\nhigh_vowels: set[str] = {'u', 'ʊ', 'i', 'ɪ'}\n\npowerset_of_high_vowels = {subset \n                           for cardinality in range(len(high_vowels)+1) \n                           for subset in combinations(high_vowels, cardinality)}\n\npowerset_of_high_vowels\n\n{(),\n ('i',),\n ('i', 'ɪ'),\n ('i', 'ɪ', 'ʊ'),\n ('i', 'ʊ'),\n ('u',),\n ('u', 'i'),\n ('u', 'i', 'ɪ'),\n ('u', 'i', 'ɪ', 'ʊ'),\n ('u', 'i', 'ʊ'),\n ('u', 'ɪ'),\n ('u', 'ɪ', 'ʊ'),\n ('u', 'ʊ'),\n ('ɪ',),\n ('ɪ', 'ʊ'),\n ('ʊ',)}\n\n\nOne slightly weird thing about this output is that the set we get has tuples as elements. For most purposes, this result is fine, but sometimes we want the elements to themselves be sets, so we can do set operations on them easily. The issue is that, as we’ve already seen, sets can’t be elements of sets in Python. This is a case where we need frozensets.\n\npowerset_of_high_vowels = {frozenset(subset) \n                           for cardinality in range(len(high_vowels)+1) \n                           for subset in combinations(high_vowels, cardinality)}\n\npowerset_of_high_vowels\n\n{frozenset(),\n frozenset({'u', 'ʊ'}),\n frozenset({'i', 'u', 'ʊ'}),\n frozenset({'u', 'ɪ'}),\n frozenset({'i', 'ɪ'}),\n frozenset({'ɪ', 'ʊ'}),\n frozenset({'i', 'ʊ'}),\n frozenset({'ɪ'}),\n frozenset({'i'}),\n frozenset({'ʊ'}),\n frozenset({'u'}),\n frozenset({'i', 'u'}),\n frozenset({'i', 'u', 'ɪ'}),\n frozenset({'u', 'ɪ', 'ʊ'}),\n frozenset({'i', 'ɪ', 'ʊ'}),\n frozenset({'i', 'u', 'ɪ', 'ʊ'})}\n\n\nSo if we wanted to be able to take the power set of anything we can represent in python as a set, we could wrap this comprehension in a function.\n\nfrom typing import Set\n\ndef powerset(x: set) -&gt; Set[frozenset]:\n  return {\n      frozenset(subset) \n      for cardinality in range(len(x)+1) \n      for subset in combinations(x, cardinality)\n  }\n\npowerset(high_vowels)\n\n{frozenset(),\n frozenset({'u', 'ʊ'}),\n frozenset({'i', 'u', 'ʊ'}),\n frozenset({'u', 'ɪ'}),\n frozenset({'i', 'ɪ'}),\n frozenset({'ɪ', 'ʊ'}),\n frozenset({'i', 'ʊ'}),\n frozenset({'ɪ'}),\n frozenset({'i'}),\n frozenset({'ʊ'}),\n frozenset({'u'}),\n frozenset({'i', 'u'}),\n frozenset({'i', 'u', 'ɪ'}),\n frozenset({'u', 'ɪ', 'ʊ'}),\n frozenset({'i', 'ɪ', 'ʊ'}),\n frozenset({'i', 'u', 'ɪ', 'ʊ'})}\n\n\nAlternatively, we can use the following itertools recipe. The main difference here is that we don’t have the explicit for loop over subsets of a particular cardinality, which we needed for the purposes of flattening sets. That’s what itertools.chain.from_iterable does for us. This returns an itertools.chain object, which you can treat as a generator.\n\nfrom typing import Iterable\nfrom itertools import chain\n\ndef powerset(iterable: Iterable) -&gt; chain:\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\npowerset(high_vowels)\n\n&lt;itertools.chain at 0x106193070&gt;\n\n\nTo get a set of frozensets, we need to some explicit type casting (so we don’t really avoid the second for loop…).\n\n{frozenset(subset) for subset in powerset(high_vowels)}\n\n{frozenset(),\n frozenset({'u', 'ʊ'}),\n frozenset({'i', 'u', 'ʊ'}),\n frozenset({'u', 'ɪ'}),\n frozenset({'i', 'ɪ'}),\n frozenset({'ɪ', 'ʊ'}),\n frozenset({'i', 'ʊ'}),\n frozenset({'ɪ'}),\n frozenset({'i'}),\n frozenset({'ʊ'}),\n frozenset({'u'}),\n frozenset({'i', 'u'}),\n frozenset({'i', 'u', 'ɪ'}),\n frozenset({'u', 'ɪ', 'ʊ'}),\n frozenset({'i', 'ɪ', 'ʊ'}),\n frozenset({'i', 'u', 'ɪ', 'ʊ'})}\n\n\nNote that the thing we’re taking the power set of needs to be of finite size in both implementations–i.e. it can’t be a generator that runs forever. To see this, let’s create a generator for the natural numbers using yield statements. If we create a generator by calling natural_numbers with no arguments, it would run forever. (Below I break it after 10 iterations.)\nAnd if I pass this generator (an iterable) to powerset, it will hang.\n\nfrom collections.abc import Generator\nfrom multiprocessing import Process\n\ndef natural_numbers() -&gt; int:\n    \"\"\"Initialize a generator for the natural numbers\"\"\"\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n# initialize a generator of the natural numbers\nN: Generator[int] = natural_numbers()\n\n# this will hang\n# powerset(N)\n\nIf we want to be able to generate elements of the power set of an infinite set, we will have to do it in a slightly smarter way.\n\nfrom typing import TypeVar, Set, Iterable\n\nT = TypeVar(\"T\")\n\nemptyset = frozenset()\n\ndef powerset(iterable: Iterable[T]) -&gt; Set[T]:\n    yield emptyset\n\n    seen = {emptyset}\n\n    for r in iterable:\n        new = {s | frozenset({r}) for s in seen}\n        for n in new:\n            yield n\n            seen.add(n)\n\nThis will still get us the correct result for finite sets.\n\n{s for s in powerset(high_vowels)}\n\n{frozenset(),\n frozenset({'u', 'ʊ'}),\n frozenset({'i', 'u', 'ʊ'}),\n frozenset({'u', 'ɪ'}),\n frozenset({'i', 'ɪ'}),\n frozenset({'ɪ', 'ʊ'}),\n frozenset({'i', 'ʊ'}),\n frozenset({'ɪ'}),\n frozenset({'i'}),\n frozenset({'ʊ'}),\n frozenset({'u'}),\n frozenset({'i', 'u'}),\n frozenset({'i', 'u', 'ɪ'}),\n frozenset({'u', 'ɪ', 'ʊ'}),\n frozenset({'i', 'ɪ', 'ʊ'}),\n frozenset({'i', 'u', 'ɪ', 'ʊ'})}\n\n\nAnd it will also work for infinite sets.\n\nN = natural_numbers()\n\nfor i, s in enumerate(powerset(N)):\n  if i &lt; 100:\n    print(s)\n  else:\n    break\n\nfrozenset()\nfrozenset({0})\nfrozenset({0, 1})\nfrozenset({1})\nfrozenset({2})\nfrozenset({1, 2})\nfrozenset({0, 2})\nfrozenset({0, 1, 2})\nfrozenset({2, 3})\nfrozenset({0, 2, 3})\nfrozenset({0, 3})\nfrozenset({3})\nfrozenset({0, 1, 2, 3})\nfrozenset({1, 3})\nfrozenset({1, 2, 3})\nfrozenset({0, 1, 3})\nfrozenset({0, 3, 4})\nfrozenset({3, 4})\nfrozenset({0, 1, 4})\nfrozenset({2, 3, 4})\nfrozenset({1, 4})\nfrozenset({1, 2, 4})\nfrozenset({0, 1, 2, 3, 4})\nfrozenset({0, 2, 3, 4})\nfrozenset({0, 4})\nfrozenset({2, 4})\nfrozenset({0, 2, 4})\nfrozenset({1, 2, 3, 4})\nfrozenset({0, 1, 3, 4})\nfrozenset({0, 1, 2, 4})\nfrozenset({1, 3, 4})\nfrozenset({4})\nfrozenset({1, 3, 5})\nfrozenset({4, 5})\nfrozenset({0, 2, 5})\nfrozenset({0, 5})\nfrozenset({0, 3, 4, 5})\nfrozenset({0, 1, 3, 4, 5})\nfrozenset({0, 3, 5})\nfrozenset({0, 4, 5})\nfrozenset({0, 2, 3, 4, 5})\nfrozenset({0, 2, 3, 5})\nfrozenset({1, 2, 3, 4, 5})\nfrozenset({2, 3, 5})\nfrozenset({3, 5})\nfrozenset({3, 4, 5})\nfrozenset({1, 2, 4, 5})\nfrozenset({0, 1, 2, 4, 5})\nfrozenset({0, 1, 4, 5})\nfrozenset({0, 2, 4, 5})\nfrozenset({0, 1, 3, 5})\nfrozenset({2, 3, 4, 5})\nfrozenset({1, 2, 5})\nfrozenset({1, 5})\nfrozenset({1, 4, 5})\nfrozenset({5})\nfrozenset({0, 1, 2, 5})\nfrozenset({1, 3, 4, 5})\nfrozenset({1, 2, 3, 5})\nfrozenset({0, 1, 2, 3, 4, 5})\nfrozenset({2, 4, 5})\nfrozenset({2, 5})\nfrozenset({0, 1, 5})\nfrozenset({0, 1, 2, 3, 5})\nfrozenset({1, 4, 5, 6})\nfrozenset({4, 6})\nfrozenset({2, 3, 5, 6})\nfrozenset({2, 6})\nfrozenset({0, 1, 2, 4, 5, 6})\nfrozenset({0, 2, 4, 6})\nfrozenset({0, 1, 4, 5, 6})\nfrozenset({0, 5, 6})\nfrozenset({0, 2, 3, 6})\nfrozenset({0, 1, 2, 5, 6})\nfrozenset({1, 2, 3, 4, 5, 6})\nfrozenset({0, 3, 4, 6})\nfrozenset({0, 1, 3, 4, 5, 6})\nfrozenset({2, 3, 4, 6})\nfrozenset({4, 5, 6})\nfrozenset({0, 2, 3, 4, 5, 6})\nfrozenset({1, 3, 4, 6})\nfrozenset({1, 2, 3, 4, 6})\nfrozenset({1, 4, 6})\nfrozenset({0, 1, 2, 3, 4, 6})\nfrozenset({2, 4, 6})\nfrozenset({3, 4, 5, 6})\nfrozenset({0, 4, 6})\nfrozenset({1, 3, 4, 5, 6})\nfrozenset({0, 6})\nfrozenset({0, 3, 5, 6})\nfrozenset({0, 2, 6})\nfrozenset({0, 1, 2, 4, 6})\nfrozenset({0, 2, 5, 6})\nfrozenset({0, 3, 6})\nfrozenset({0, 4, 5, 6})\nfrozenset({0, 2, 4, 5, 6})\nfrozenset({3, 4, 6})\nfrozenset({5, 6})\nfrozenset({1, 2, 4, 5, 6})\nfrozenset({2, 4, 5, 6})"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/products.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/products.html",
    "title": "Products",
    "section": "",
    "text": "The cartesian product of a set \\(A\\) with a set \\(B\\) is the set of all pairs of some element in \\(A\\) with some element in \\(B\\) (in that order).\n\\[A \\times B = \\{\\langle x, y \\rangle\\;|\\;x \\in A \\land y \\in B\\}\\]\nThe caridinality of a cartesian product of two sets is the product of their cardinalities.\n\\[|A \\times B| = |A| \\times |B|\\]\n\nfrom itertools import product\n\nx = {1, 2, 3}\ny = {\"d\", \"u\", \"r\"}\n\n# cartesian product using nested for loop\n# in set comprehension\nz = {(i, j) for i in x for j in y}\nset(product(x, y)) == z # evaluates to True\n\nTrue\n\n\nThe cartesian product can be iterated – notated using exponentiation notation\n\\[A^3 = A \\times (A \\times A)\\] \\[A^4 = A \\times (A \\times (A \\times A))\\]\nSince we know \\(|A \\times B|\\), we also know the cardinality of \\(|A^N| = |\\times_{i=1}^N A| = |A| \\times |A^{N-1}| = |A|^N\\).\n\nvowels: set[str] = {\"e\", \"i\", \"o\", \"u\", \"æ\", \"ɑ\", \"ɔ\", \"ə\", \"ɛ\", \"ɪ\", \"ʊ\"}\n\ndef exponentiate(a, n):\n    if n == 1:\n        return a\n    else:\n        return {(x, t) for t in exponentiate(a, n-1) for x in a}\n        \n    \nexponentiate(vowels, 5)\n\n{('u', ('æ', ('o', ('u', 'i')))),\n ('ɪ', ('ɑ', ('ɑ', ('ə', 'ɛ')))),\n ('ɪ', ('ʊ', ('u', ('o', 'ɛ')))),\n ('ʊ', ('æ', ('æ', ('i', 'ɔ')))),\n ('ə', ('ʊ', ('e', ('ɑ', 'æ')))),\n ('ɑ', ('o', ('ɪ', ('ɔ', 'u')))),\n ('ɔ', ('ɛ', ('æ', ('e', 'ɪ')))),\n ('æ', ('ə', ('ɑ', ('u', 'u')))),\n ('u', ('æ', ('ɪ', ('æ', 'ʊ')))),\n ('i', ('ʊ', ('ɔ', ('u', 'ʊ')))),\n ('ɔ', ('ʊ', ('ɔ', ('ə', 'u')))),\n ('e', ('ɑ', ('u', ('ɑ', 'o')))),\n ('ɪ', ('ɔ', ('e', ('i', 'æ')))),\n ('ɔ', ('æ', ('ɑ', ('ə', 'ɔ')))),\n ('ɪ', ('u', ('ə', ('o', 'i')))),\n ('o', ('ʊ', ('ə', ('æ', 'ə')))),\n ('u', ('æ', ('æ', ('ɪ', 'u')))),\n ('e', ('o', ('u', ('e', 'o')))),\n ('ɔ', ('æ', ('æ', ('ɔ', 'ɔ')))),\n ('ʊ', ('ə', ('ʊ', ('ʊ', 'æ')))),\n ('ɔ', ('ɔ', ('u', ('i', 'e')))),\n ('ɛ', ('ɪ', ('i', ('æ', 'ʊ')))),\n ('ə', ('ʊ', ('ɔ', ('ə', 'i')))),\n ('ɪ', ('u', ('o', ('ɛ', 'o')))),\n ('ɑ', ('o', ('ɛ', ('æ', 'ɪ')))),\n ('i', ('ɛ', ('ɪ', ('i', 'ɛ')))),\n ('ə', ('ɔ', ('u', ('ɔ', 'ə')))),\n ('ɪ', ('i', ('ɛ', ('ʊ', 'æ')))),\n ('o', ('u', ('ʊ', ('ʊ', 'ɛ')))),\n ('ɑ', ('i', ('o', ('ɑ', 'ʊ')))),\n ('i', ('ʊ', ('ɛ', ('o', 'o')))),\n ('ɑ', ('ə', ('æ', ('i', 'ɪ')))),\n ('e', ('ʊ', ('ə', ('ɛ', 'ɑ')))),\n ('ʊ', ('ə', ('ɔ', ('ɔ', 'i')))),\n ('e', ('e', ('ə', ('ɪ', 'u')))),\n ('o', ('ʊ', ('e', ('e', 'ɛ')))),\n ('ɛ', ('ʊ', ('ɑ', ('ɛ', 'ə')))),\n ('o', ('e', ('ʊ', ('ɔ', 'ə')))),\n ('ɔ', ('e', ('o', ('ɔ', 'æ')))),\n ('ʊ', ('ə', ('ɛ', ('æ', 'o')))),\n ('ɪ', ('i', ('ɛ', ('ɔ', 'u')))),\n ('i', ('u', ('i', ('æ', 'u')))),\n ('u', ('ʊ', ('ɑ', ('e', 'ɑ')))),\n ('æ', ('e', ('ʊ', ('ə', 'o')))),\n ('ɔ', ('o', ('o', ('o', 'ʊ')))),\n ('ɑ', ('u', ('ɑ', ('i', 'ɛ')))),\n ('o', ('ɪ', ('ə', ('e', 'ɑ')))),\n ('ʊ', ('ə', ('e', ('ɑ', 'o')))),\n ('æ', ('i', ('ɔ', ('ɔ', 'o')))),\n ('o', ('æ', ('o', ('ɑ', 'ɛ')))),\n ('ʊ', ('ɔ', ('u', ('ɑ', 'ɑ')))),\n ('æ', ('æ', ('ɪ', ('ʊ', 'ʊ')))),\n ('ə', ('o', ('ʊ', ('æ', 'ə')))),\n ('ɪ', ('u', ('æ', ('ɑ', 'i')))),\n ('u', ('o', ('ɔ', ('ɪ', 'ɑ')))),\n ('ɔ', ('ɪ', ('ɛ', ('ɛ', 'ə')))),\n ('ə', ('æ', ('ɛ', ('ɑ', 'ʊ')))),\n ('u', ('ɑ', ('ɑ', ('u', 'e')))),\n ('ɔ', ('ɔ', ('æ', ('i', 'ɛ')))),\n ('ə', ('i', ('ɛ', ('ʊ', 'u')))),\n ('ɛ', ('æ', ('ɑ', ('o', 'ə')))),\n ('æ', ('ɑ', ('e', ('ʊ', 'æ')))),\n ('i', ('ʊ', ('ə', ('e', 'ɪ')))),\n ('u', ('o', ('ɛ', ('ɔ', 'ə')))),\n ('ɛ', ('æ', ('æ', ('æ', 'æ')))),\n ('æ', ('o', ('e', ('æ', 'ɑ')))),\n ('u', ('ɑ', ('o', ('ʊ', 'ə')))),\n ('æ', ('ʊ', ('o', ('i', 'ɛ')))),\n ('e', ('e', ('i', ('ɑ', 'æ')))),\n ('ʊ', ('ʊ', ('ə', ('u', 'æ')))),\n ('ɑ', ('ɑ', ('i', ('ə', 'ɛ')))),\n ('ɪ', ('o', ('ʊ', ('ɪ', 'i')))),\n ('o', ('e', ('ʊ', ('ɪ', 'i')))),\n ('u', ('o', ('ɑ', ('ɔ', 'ɑ')))),\n ('ə', ('ɪ', ('i', ('ɑ', 'u')))),\n ('e', ('ɪ', ('ə', ('ɔ', 'ɪ')))),\n ('ə', ('ɔ', ('ʊ', ('o', 'ɪ')))),\n ('o', ('ə', ('ɑ', ('u', 'ɛ')))),\n ('e', ('i', ('e', ('ɔ', 'i')))),\n ('ɛ', ('ɪ', ('ɔ', ('ɛ', 'ʊ')))),\n ('o', ('æ', ('e', ('o', 'ɑ')))),\n ('ɛ', ('i', ('ə', ('ɛ', 'i')))),\n ('i', ('ɑ', ('æ', ('ɑ', 'æ')))),\n ('ə', ('æ', ('o', ('ɛ', 'u')))),\n ('e', ('ɑ', ('ə', ('i', 'æ')))),\n ('ə', ('ɪ', ('ɔ', ('ə', 'ə')))),\n ('u', ('ə', ('o', ('æ', 'o')))),\n ('ɪ', ('ɔ', ('e', ('ɪ', 'ɪ')))),\n ('ɛ', ('ɔ', ('æ', ('ɛ', 'o')))),\n ('æ', ('e', ('ʊ', ('ə', 'ɑ')))),\n ('ɑ', ('ɔ', ('æ', ('ɔ', 'u')))),\n ('ɪ', ('e', ('o', ('ɑ', 'ə')))),\n ('ɪ', ('ɛ', ('e', ('ɑ', 'ɪ')))),\n ('ɪ', ('ɪ', ('ə', ('ɪ', 'ɛ')))),\n ('o', ('e', ('ɪ', ('i', 'ʊ')))),\n ('ɪ', ('ɑ', ('i', ('æ', 'u')))),\n ('ɔ', ('u', ('ə', ('ɔ', 'ʊ')))),\n ('ə', ('ɔ', ('ʊ', ('ʊ', 'ɛ')))),\n ('u', ('u', ('ə', ('e', 'ɛ')))),\n ('ɔ', ('i', ('ɛ', ('e', 'e')))),\n ('ʊ', ('u', ('ɑ', ('i', 'i')))),\n ('ʊ', ('e', ('ɛ', ('ə', 'ə')))),\n ('ɪ', ('ʊ', ('ɪ', ('e', 'e')))),\n ('æ', ('u', ('e', ('ʊ', 'ɔ')))),\n ('o', ('i', ('ɑ', ('o', 'ʊ')))),\n ('i', ('ʊ', ('ʊ', ('ɛ', 'ɛ')))),\n ('æ', ('ɛ', ('ə', ('i', 'e')))),\n ('o', ('e', ('ɑ', ('ə', 'i')))),\n ('ɑ', ('ɔ', ('o', ('u', 'o')))),\n ('ə', ('ɪ', ('ɑ', ('i', 'e')))),\n ('æ', ('u', ('e', ('ə', 'ɛ')))),\n ('ʊ', ('u', ('ɛ', ('ɪ', 'ɛ')))),\n ('u', ('o', ('ʊ', ('ɑ', 'i')))),\n ('ɛ', ('e', ('æ', ('i', 'æ')))),\n ('e', ('ɪ', ('ə', ('u', 'ɔ')))),\n ('ɑ', ('ʊ', ('e', ('ɪ', 'ɑ')))),\n ('u', ('ɔ', ('ɛ', ('ʊ', 'ɪ')))),\n ('e', ('i', ('u', ('ɪ', 'ɑ')))),\n ('ʊ', ('ɔ', ('i', ('ɔ', 'ə')))),\n ('ɑ', ('i', ('ɔ', ('e', 'u')))),\n ('ɑ', ('ɑ', ('e', ('ə', 'u')))),\n ('æ', ('ʊ', ('u', ('ʊ', 'o')))),\n ('ʊ', ('e', ('ʊ', ('ʊ', 'o')))),\n ('o', ('ɑ', ('o', ('ɛ', 'ə')))),\n ('o', ('o', ('ɔ', ('i', 'ɑ')))),\n ('ɔ', ('ɑ', ('ɑ', ('ə', 'u')))),\n ('ɔ', ('ɔ', ('ɪ', ('ə', 'ɛ')))),\n ('ɛ', ('ə', ('ɛ', ('ʊ', 'ɛ')))),\n ('o', ('e', ('i', ('i', 'i')))),\n ('ʊ', ('ɔ', ('ʊ', ('ə', 'e')))),\n ('o', ('ə', ('ɪ', ('ɛ', 'æ')))),\n ('ɪ', ('ɛ', ('ɑ', ('ɔ', 'i')))),\n ('i', ('o', ('ɛ', ('u', 'ə')))),\n ('e', ('ɛ', ('ɔ', ('u', 'ɪ')))),\n ('ɑ', ('æ', ('i', ('ɪ', 'ɔ')))),\n ('ʊ', ('ɪ', ('o', ('ɛ', 'e')))),\n ('u', ('ɑ', ('i', ('æ', 'i')))),\n ('ɛ', ('ɪ', ('ʊ', ('æ', 'ɔ')))),\n ('i', ('æ', ('o', ('ɪ', 'e')))),\n ('ʊ', ('u', ('o', ('ə', 'u')))),\n ('ɔ', ('i', ('ɛ', ('e', 'ʊ')))),\n ('ə', ('ɛ', ('o', ('e', 'ʊ')))),\n ('e', ('ɑ', ('ʊ', ('e', 'ə')))),\n ('ɑ', ('i', ('ɪ', ('ɔ', 'i')))),\n ('ɔ', ('æ', ('ɔ', ('o', 'ə')))),\n ('u', ('ɔ', ('æ', ('ɛ', 'i')))),\n ('u', ('ɪ', ('ə', ('e', 'u')))),\n ('e', ('ɪ', ('ə', ('ʊ', 'æ')))),\n ('æ', ('æ', ('ɪ', ('ʊ', 'u')))),\n ('ɔ', ('ʊ', ('e', ('ə', 'ɪ')))),\n ('ɑ', ('ɑ', ('ɛ', ('o', 'u')))),\n ('ə', ('æ', ('ɛ', ('ɑ', 'u')))),\n ('e', ('ʊ', ('ɔ', ('ʊ', 'u')))),\n ('ɛ', ('ɔ', ('ɛ', ('ɔ', 'ɛ')))),\n ('ɔ', ('ə', ('ɔ', ('i', 'ɑ')))),\n ('u', ('ɛ', ('æ', ('ʊ', 'ɪ')))),\n ('ɛ', ('æ', ('i', ('æ', 'ɪ')))),\n ('e', ('ʊ', ('æ', ('æ', 'ɔ')))),\n ('o', ('ɛ', ('u', ('ɪ', 'ə')))),\n ('ə', ('æ', ('u', ('ɛ', 'ɛ')))),\n ('ə', ('ɔ', ('ə', ('i', 'i')))),\n ('ɛ', ('ɔ', ('ʊ', ('e', 'ɔ')))),\n ('ə', ('o', ('ɑ', ('i', 'ɑ')))),\n ('i', ('ɪ', ('ɔ', ('ɪ', 'ɑ')))),\n ('i', ('e', ('o', ('ə', 'ə')))),\n ('ɪ', ('ə', ('ɔ', ('i', 'ɛ')))),\n ('i', ('ɛ', ('ɑ', ('ɛ', 'u')))),\n ('e', ('ʊ', ('ɑ', ('ɑ', 'ɛ')))),\n ('æ', ('ʊ', ('ə', ('æ', 'e')))),\n ('u', ('ɑ', ('e', ('ɪ', 'i')))),\n ('u', ('ɛ', ('o', ('o', 'u')))),\n ('i', ('ə', ('o', ('ɑ', 'e')))),\n ('i', ('u', ('ɔ', ('ɔ', 'ʊ')))),\n ('ɛ', ('ɔ', ('u', ('ə', 'ɪ')))),\n ('o', ('e', ('æ', ('ɪ', 'ɑ')))),\n ('ʊ', ('ɪ', ('æ', ('ɛ', 'u')))),\n ('i', ('ɪ', ('ɑ', ('ɔ', 'ɑ')))),\n ('ə', ('e', ('ɔ', ('e', 'i')))),\n ('o', ('o', ('u', ('ɪ', 'æ')))),\n ('ə', ('i', ('æ', ('æ', 'ɑ')))),\n ('u', ('ʊ', ('e', ('u', 'o')))),\n ('ɔ', ('u', ('ə', ('ɔ', 'u')))),\n ('e', ('ʊ', ('u', ('i', 'u')))),\n ('ɛ', ('æ', ('ɑ', ('ə', 'ə')))),\n ('æ', ('ɛ', ('ʊ', ('ʊ', 'æ')))),\n ('æ', ('ɑ', ('ɑ', ('ɪ', 'ɪ')))),\n ('ɪ', ('o', ('ɛ', ('ʊ', 'ɔ')))),\n ('ɛ', ('ʊ', ('e', ('ə', 'u')))),\n ('ɪ', ('ɪ', ('ɪ', ('æ', 'e')))),\n ('u', ('ɪ', ('æ', ('ɑ', 'ʊ')))),\n ('ɪ', ('o', ('ɛ', ('ə', 'ɛ')))),\n ('o', ('e', ('ɛ', ('ə', 'ɛ')))),\n ('o', ('æ', ('æ', ('æ', 'ʊ')))),\n ('æ', ('ɛ', ('ɪ', ('ɪ', 'ɔ')))),\n ('o', ('u', ('ɪ', ('ɔ', 'ə')))),\n ('ə', ('i', ('ə', ('ɛ', 'æ')))),\n ('æ', ('ɑ', ('æ', ('ɑ', 'e')))),\n ('i', ('ɑ', ('e', ('u', 'ə')))),\n ('æ', ('æ', ('æ', ('ʊ', 'ɔ')))),\n ('ɛ', ('æ', ('ɛ', ('ɪ', 'ɪ')))),\n ('ɑ', ('ə', ('ʊ', ('ɛ', 'ɔ')))),\n ('æ', ('u', ('ɪ', ('ə', 'o')))),\n ('ə', ('æ', ('ɛ', ('ɔ', 'ʊ')))),\n ('ɔ', ('i', ('e', ('æ', 'ɪ')))),\n ('ɪ', ('ʊ', ('æ', ('e', 'ɔ')))),\n ('u', ('e', ('ɪ', ('ʊ', 'e')))),\n ('ə', ('ɑ', ('ə', ('ɛ', 'ɑ')))),\n ('ɛ', ('e', ('ɔ', ('e', 'ɛ')))),\n ('u', ('æ', ('æ', ('o', 'æ')))),\n ('ɑ', ('ɑ', ('u', ('ʊ', 'ɑ')))),\n ('ɛ', ('i', ('i', ('æ', 'ɛ')))),\n ('i', ('ə', ('ɪ', ('u', 'i')))),\n ('e', ('ɔ', ('u', ('ɪ', 'æ')))),\n ('o', ('ʊ', ('ɪ', ('ɔ', 'ʊ')))),\n ('o', ('ɔ', ('ɑ', ('ɛ', 'e')))),\n ('u', ('ʊ', ('u', ('ɛ', 'o')))),\n ('i', ('æ', ('ɛ', ('ɪ', 'ʊ')))),\n ('ɪ', ('ʊ', ('ə', ('ə', 'i')))),\n ('o', ('u', ('i', ('ɪ', 'ɔ')))),\n ('ɔ', ('o', ('e', ('æ', 'u')))),\n ('ə', ('u', ('ɪ', ('e', 'o')))),\n ('u', ('u', ('ə', ('æ', 'ʊ')))),\n ('u', ('ɑ', ('æ', ('æ', 'o')))),\n ('o', ('u', ('ɪ', ('ɪ', 'i')))),\n ('æ', ('ɪ', ('o', ('u', 'ɪ')))),\n ('ʊ', ('o', ('ɑ', ('æ', 'ʊ')))),\n ('o', ('ɔ', ('u', ('ʊ', 'ʊ')))),\n ('ʊ', ('i', ('ɔ', ('i', 'u')))),\n ('æ', ('e', ('ɪ', ('u', 'ɪ')))),\n ('ʊ', ('o', ('ɛ', ('ɛ', 'ɑ')))),\n ('i', ('ɛ', ('ɛ', ('ɔ', 'ɑ')))),\n ('ʊ', ('ɑ', ('e', ('ɔ', 'æ')))),\n ('o', ('o', ('ʊ', ('e', 'i')))),\n ('ɪ', ('e', ('ʊ', ('ɪ', 'e')))),\n ('ɪ', ('ʊ', ('e', ('ɪ', 'o')))),\n ('ɔ', ('ɑ', ('o', ('ɪ', 'ʊ')))),\n ('ɪ', ('ʊ', ('e', ('u', 'ɑ')))),\n ('ɔ', ('o', ('o', ('o', 'ə')))),\n ('ɔ', ('e', ('ʊ', ('ə', 'u')))),\n ('ʊ', ('ɑ', ('ɪ', ('u', 'ɑ')))),\n ('ʊ', ('e', ('o', ('ɑ', 'æ')))),\n ('ɔ', ('ʊ', ('ʊ', ('ɔ', 'ɑ')))),\n ('o', ('ɑ', ('ɔ', ('ɔ', 'ɪ')))),\n ('æ', ('i', ('ə', ('ɔ', 'æ')))),\n ('i', ('ɛ', ('ɪ', ('ʊ', 'i')))),\n ('ʊ', ('o', ('e', ('i', 'ɪ')))),\n ('ɪ', ('o', ('ɪ', ('æ', 'ɑ')))),\n ('ɪ', ('ə', ('ɑ', ('ɑ', 'i')))),\n ('i', ('o', ('ɪ', ('e', 'e')))),\n ('ɪ', ('ɛ', ('ɛ', ('ʊ', 'e')))),\n ('ɛ', ('ʊ', ('u', ('æ', 'o')))),\n ('ə', ('i', ('ʊ', ('o', 'ʊ')))),\n ('i', ('ɔ', ('ɪ', ('o', 'ʊ')))),\n ('ʊ', ('ɔ', ('ɛ', ('æ', 'ə')))),\n ('æ', ('ɑ', ('i', ('ɔ', 'o')))),\n ('ɪ', ('ʊ', ('ɔ', ('ɛ', 'o')))),\n ('i', ('ə', ('ʊ', ('ɔ', 'ə')))),\n ('u', ('e', ('ɪ', ('ɛ', 'ʊ')))),\n ('u', ('e', ('ɛ', ('ɪ', 'e')))),\n ('ɪ', ('æ', ('ʊ', ('ɪ', 'u')))),\n ('i', ('e', ('e', ('o', 'o')))),\n ('ɪ', ('ɑ', ('e', ('ə', 'ɛ')))),\n ('ɪ', ('ʊ', ('i', ('ɑ', 'e')))),\n ('u', ('æ', ('ɪ', ('e', 'ɪ')))),\n ('u', ('o', ('o', ('ʊ', 'ɑ')))),\n ('e', ('e', ('ɔ', ('æ', 'e')))),\n ('ɪ', ('u', ('u', ('u', 'ɑ')))),\n ('ɪ', ('i', ('ʊ', ('ɛ', 'ʊ')))),\n ('ɪ', ('ɑ', ('e', ('ɛ', 'ɔ')))),\n ('u', ('ɑ', ('u', ('ɔ', 'u')))),\n ('ɑ', ('æ', ('ʊ', ('ɑ', 'ɑ')))),\n ('e', ('ɔ', ('ʊ', ('e', 'i')))),\n ('æ', ('u', ('ə', ('ʊ', 'ʊ')))),\n ('o', ('æ', ('u', ('u', 'æ')))),\n ('ʊ', ('u', ('ɪ', ('ʊ', 'o')))),\n ('o', ('ɪ', ('ɪ', ('o', 'ɔ')))),\n ('ɪ', ('ə', ('ɛ', ('ɔ', 'o')))),\n ('o', ('ɑ', ('ɔ', ('u', 'ɔ')))),\n ('u', ('o', ('ɑ', ('ʊ', 'ɪ')))),\n ('u', ('ɪ', ('æ', ('ɑ', 'u')))),\n ('o', ('ʊ', ('e', ('ɪ', 'ɪ')))),\n ('æ', ('i', ('æ', ('u', 'i')))),\n ('o', ('æ', ('æ', ('æ', 'u')))),\n ('ə', ('ɛ', ('e', ('æ', 'e')))),\n ('u', ('æ', ('ɛ', ('ɪ', 'u')))),\n ('i', ('ʊ', ('e', ('ʊ', 'ʊ')))),\n ('e', ('æ', ('ɔ', ('æ', 'u')))),\n ('e', ('ɪ', ('ʊ', ('i', 'ɑ')))),\n ('e', ('e', ('e', ('ɪ', 'ʊ')))),\n ('ə', ('ə', ('ɛ', ('ʊ', 'o')))),\n ('ə', ('ɔ', ('ɪ', ('ɪ', 'i')))),\n ('ə', ('u', ('ɔ', ('i', 'ə')))),\n ('u', ('e', ('o', ('ɔ', 'ə')))),\n ('o', ('ə', ('æ', ('i', 'e')))),\n ('ɪ', ('o', ('o', ('i', 'o')))),\n ('e', ('o', ('ɪ', ('o', 'ɔ')))),\n ('ɪ', ('o', ('ə', ('ɪ', 'o')))),\n ('æ', ('ə', ('o', ('æ', 'i')))),\n ('i', ('i', ('i', ('i', 'ɔ')))),\n ('ʊ', ('æ', ('ɪ', ('ʊ', 'æ')))),\n ('æ', ('ɔ', ('u', ('u', 'ɛ')))),\n ('i', ('u', ('ɑ', ('ɛ', 'e')))),\n ('ʊ', ('ɑ', ('e', ('ɛ', 'ɑ')))),\n ('ɪ', ('æ', ('ɔ', ('ə', 'e')))),\n ('e', ('ɔ', ('ɪ', ('ɛ', 'æ')))),\n ('ɛ', ('ʊ', ('æ', ('ɛ', 'ɑ')))),\n ('ʊ', ('i', ('ɪ', ('ə', 'ɔ')))),\n ('o', ('e', ('ə', ('ɑ', 'i')))),\n ('ɛ', ('e', ('æ', ('ɪ', 'u')))),\n ('u', ('u', ('ʊ', ('i', 'ɛ')))),\n ('e', ('o', ('o', ('æ', 'ʊ')))),\n ('i', ('ɑ', ('o', ('i', 'ɑ')))),\n ('ɛ', ('ɑ', ('ə', ('ɑ', 'ɛ')))),\n ('ʊ', ('ʊ', ('æ', ('o', 'o')))),\n ('o', ('ɛ', ('ɪ', ('i', 'ɑ')))),\n ('o', ('ə', ('e', ('ɔ', 'ɪ')))),\n ('o', ('ɑ', ('ɔ', ('ʊ', 'æ')))),\n ('ɔ', ('æ', ('ə', ('e', 'ɔ')))),\n ('ɑ', ('ɔ', ('ɪ', ('ɛ', 'i')))),\n ('ʊ', ('u', ('ɪ', ('ʊ', 'ɑ')))),\n ('o', ('o', ('o', ('ɛ', 'ɑ')))),\n ('ɛ', ('æ', ('e', ('o', 'ə')))),\n ('u', ('ɔ', ('ʊ', ('ɑ', 'o')))),\n ('ə', ('i', ('ʊ', ('e', 'ɑ')))),\n ('ə', ('ɑ', ('ɔ', ('ʊ', 'u')))),\n ('æ', ('ɑ', ('o', ('i', 'u')))),\n ('ɛ', ('o', ('ɪ', ('ɔ', 'ɔ')))),\n ('i', ('ɛ', ('ʊ', ('u', 'ə')))),\n ('i', ('ɑ', ('ɑ', ('i', 'ɪ')))),\n ('e', ('e', ('æ', ('e', 'ʊ')))),\n ('æ', ('o', ('i', ('æ', 'ɪ')))),\n ('ɪ', ('u', ('æ', ('ɔ', 'ə')))),\n ('ɔ', ('ɑ', ('o', ('ɪ', 'u')))),\n ('u', ('i', ('ʊ', ('i', 'i')))),\n ('ɔ', ('o', ('e', ('æ', 'ə')))),\n ('u', ('e', ('o', ('ɪ', 'i')))),\n ('o', ('o', ('ɑ', ('ɛ', 'ɪ')))),\n ('ʊ', ('ɔ', ('i', ('u', 'ʊ')))),\n ('ə', ('ə', ('ɛ', ('ʊ', 'ɑ')))),\n ('ɔ', ('e', ('i', ('o', 'ə')))),\n ('æ', ('ɛ', ('ə', ('i', 'ə')))),\n ('ə', ('ʊ', ('i', ('e', 'ɛ')))),\n ('o', ('ɔ', ('o', ('æ', 'ə')))),\n ('e', ('e', ('i', ('ɪ', 'ɛ')))),\n ('ɑ', ('ɑ', ('o', ('ə', 'ɛ')))),\n ('ɔ', ('æ', ('ɛ', ('u', 'i')))),\n ('ɔ', ('æ', ('o', ('ɪ', 'ə')))),\n ('ɛ', ('ɑ', ('æ', ('o', 'i')))),\n ('e', ('u', ('i', ('i', 'ɔ')))),\n ('i', ('ɔ', ('e', ('i', 'o')))),\n ('ɑ', ('e', ('i', ('ɔ', 'ə')))),\n ('o', ('æ', ('ɛ', ('i', 'u')))),\n ('e', ('ɔ', ('o', ('ɛ', 'ɑ')))),\n ('æ', ('æ', ('ʊ', ('ɪ', 'i')))),\n ('ɛ', ('ɔ', ('ɔ', ('æ', 'o')))),\n ('e', ('u', ('ɑ', ('e', 'ə')))),\n ('o', ('ɛ', ('i', ('ɔ', 'ʊ')))),\n ('ə', ('ɑ', ('u', ('i', 'u')))),\n ('ɪ', ('ʊ', ('ɛ', ('i', 'ɪ')))),\n ('ʊ', ('ɛ', ('æ', ('e', 'ɪ')))),\n ('ɛ', ('æ', ('u', ('e', 'i')))),\n ('ʊ', ('ʊ', ('ɔ', ('ə', 'u')))),\n ('e', ('ɔ', ('ɑ', ('ɛ', 'ɪ')))),\n ('e', ('ɪ', ('i', ('ʊ', 'æ')))),\n ('ɔ', ('ɪ', ('ɑ', ('ə', 'ɑ')))),\n ('i', ('ɪ', ('o', ('ʊ', 'ɑ')))),\n ('ɛ', ('ə', ('u', ('æ', 'ɛ')))),\n ('ɪ', ('ɛ', ('ɛ', ('u', 'ɪ')))),\n ('ɪ', ('æ', ('u', ('æ', 'ə')))),\n ('ɔ', ('ə', ('ɔ', ('e', 'ə')))),\n ('ə', ('ɛ', ('ɛ', ('ə', 'e')))),\n ('ɪ', ('ə', ('ʊ', ('ɑ', 'ɛ')))),\n ('ɪ', ('ə', ('ɔ', ('ɛ', 'ɛ')))),\n ('ʊ', ('æ', ('ɑ', ('ə', 'ɔ')))),\n ('ʊ', ('æ', ('æ', ('ɔ', 'ɔ')))),\n ('i', ('o', ('ə', ('ə', 'i')))),\n ('ə', ('ɪ', ('e', ('u', 'ʊ')))),\n ('ʊ', ('ɔ', ('u', ('i', 'e')))),\n ('æ', ('ɔ', ('ə', ('u', 'ɔ')))),\n ('e', ('ɪ', ('i', ('ɔ', 'u')))),\n ('ɔ', ('æ', ('æ', ('ɛ', 'ʊ')))),\n ('i', ('ɔ', ('o', ('e', 'ɪ')))),\n ('ɑ', ('e', ('i', ('ɪ', 'i')))),\n ('ɑ', ('ə', ('ɔ', ('e', 'ɑ')))),\n ('ɔ', ('ə', ('æ', ('o', 'ɪ')))),\n ('ɛ', ('æ', ('ɑ', ('u', 'ə')))),\n ('e', ('e', ('ɔ', ('ɑ', 'ɔ')))),\n ('e', ('e', ('u', ('o', 'e')))),\n ('o', ('ɑ', ('u', ('ʊ', 'ɪ')))),\n ('i', ('ɔ', ('u', ('ɔ', 'ɪ')))),\n ('u', ('ʊ', ('æ', ('u', 'æ')))),\n ('ɑ', ('ɪ', ('ɛ', ('o', 'ɑ')))),\n ('ɪ', ('u', ('ɪ', ('ɪ', 'e')))),\n ('ə', ('o', ('e', ('ɔ', 'ɔ')))),\n ('e', ('i', ('o', ('ɛ', 'ɪ')))),\n ('i', ('ə', ('e', ('e', 'o')))),\n ('ə', ('i', ('æ', ('ɛ', 'ɛ')))),\n ('o', ('æ', ('e', ('ɑ', 'ɔ')))),\n ('i', ('o', ('e', ('u', 'ɑ')))),\n ('u', ('ɑ', ('ɛ', ('æ', 'ɑ')))),\n ('e', ('o', ('o', ('æ', 'u')))),\n ('æ', ('u', ('ɪ', ('i', 'æ')))),\n ('ɑ', ('ɛ', ('ɛ', ('ɔ', 'e')))),\n ('ɔ', ('o', ('ɔ', ('ɔ', 'ɛ')))),\n ('ə', ('e', ('ɔ', ('ɑ', 'u')))),\n ('ɔ', ('ɑ', ('ɔ', ('e', 'ʊ')))),\n ('o', ('ɛ', ('ɔ', ('ə', 'ʊ')))),\n ('æ', ('ɑ', ('ɛ', ('æ', 'o')))),\n ('æ', ('ɔ', ('ə', ('ɑ', 'ɛ')))),\n ('ʊ', ('o', ('o', ('o', 'ʊ')))),\n ('ə', ('ɛ', ('æ', ('u', 'o')))),\n ('æ', ('ʊ', ('o', ('u', 'u')))),\n ('u', ('ɑ', ('e', ('ɑ', 'ɑ')))),\n ('i', ('i', ('æ', ('ɑ', 'ɛ')))),\n ('e', ('i', ('ə', ('i', 'ɛ')))),\n ('æ', ('ʊ', ('ɪ', ('ɑ', 'e')))),\n ('ʊ', ('ɪ', ('ɛ', ('ɛ', 'ə')))),\n ('æ', ('ɔ', ('ə', ('ʊ', 'æ')))),\n ('ʊ', ('ɔ', ('æ', ('i', 'ɛ')))),\n ('e', ('æ', ('u', ('o', 'u')))),\n ('ə', ('ɪ', ('u', ('ʊ', 'u')))),\n ('u', ('ɑ', ('ə', ('ɔ', 'ɛ')))),\n ('i', ('æ', ('o', ('o', 'ɛ')))),\n ('ɔ', ('ɪ', ('ɪ', ('i', 'ə')))),\n ('ɔ', ('ɔ', ('ɑ', ('ʊ', 'ɔ')))),\n ('i', ('u', ('o', ('æ', 'ə')))),\n ('ɔ', ('ə', ('e', ('e', 'e')))),\n ('ə', ('ʊ', ('u', ('ɪ', 'e')))),\n ('ɛ', ('ə', ('e', ('o', 'e')))),\n ('o', ('æ', ('ʊ', ('e', 'u')))),\n ('ɑ', ('ɑ', ('ɛ', ('ə', 'æ')))),\n ('ɑ', ('u', ('ɪ', ('ɔ', 'ɔ')))),\n ('e', ('ɑ', ('o', ('ə', 'ɑ')))),\n ('ɑ', ('ə', ('ɛ', ('e', 'i')))),\n ('u', ('ʊ', ('i', ('ə', 'i')))),\n ('ə', ('ɔ', ('ʊ', ('e', 'æ')))),\n ('ɛ', ('o', ('e', ('ʊ', 'i')))),\n ('u', ('i', ('æ', ('i', 'ɑ')))),\n ('æ', ('ʊ', ('æ', ('ə', 'o')))),\n ('ɔ', ('ə', ('u', ('o', 'o')))),\n ('ɪ', ('e', ('ʊ', ('i', 'ɔ')))),\n ('ɛ', ('æ', ('u', ('i', 'ɛ')))),\n ('i', ('ʊ', ('ɪ', ('ɔ', 'æ')))),\n ('æ', ('i', ('æ', ('ɔ', 'ɛ')))),\n ('ɔ', ('ɪ', ('ɔ', ('ɛ', 'e')))),\n ('ʊ', ('ɔ', ('ɑ', ('ɑ', 'ə')))),\n ('ɪ', ('i', ('ə', ('ɔ', 'ɔ')))),\n ('ɪ', ('æ', ('ə', ('u', 'e')))),\n ('ʊ', ('u', ('ə', ('ɔ', 'ʊ')))),\n ('ɪ', ('i', ('ʊ', ('ʊ', 'ə')))),\n ('u', ('u', ('ɛ', ('o', 'o')))),\n ('æ', ('e', ('ə', ('u', 'æ')))),\n ('ɔ', ('ʊ', ('ʊ', ('ʊ', 'ɪ')))),\n ('ɛ', ('e', ('e', ('o', 'u')))),\n ('u', ('ɪ', ('o', ('ɛ', 'ə')))),\n ('u', ('e', ('ɪ', ('ɛ', 'ə')))),\n ('æ', ('æ', ('æ', ('ɪ', 'ɑ')))),\n ('e', ('ɑ', ('ə', ('i', 'ɑ')))),\n ('ɔ', ('ɪ', ('i', ('i', 'u')))),\n ('ɑ', ('ɪ', ('ɪ', ('e', 'ʊ')))),\n ('ɛ', ('æ', ('i', ('ɔ', 'ɔ')))),\n ('ɔ', ('o', ('ɪ', ('e', 'ə')))),\n ('ə', ('i', ('ə', ('ʊ', 'ɔ')))),\n ('u', ('i', ('u', ('ɑ', 'i')))),\n ('u', ('ɔ', ('ɑ', ('e', 'e')))),\n ('æ', ('ɑ', ('ɛ', ('ɑ', 'e')))),\n ('ɪ', ('æ', ('e', ('e', 'ɔ')))),\n ('æ', ('æ', ('ɛ', ('ʊ', 'ɔ')))),\n ('ɛ', ('ɔ', ('ɪ', ('o', 'ə')))),\n ('o', ('ɛ', ('ɔ', ('æ', 'ɪ')))),\n ('ə', ('e', ('o', ('e', 'u')))),\n ('æ', ('æ', ('ɛ', ('ə', 'ɛ')))),\n ('ʊ', ('o', ('o', ('e', 'ɑ')))),\n ('ʊ', ('ɑ', ('ɑ', ('ə', 'u')))),\n ('ʊ', ('ɔ', ('ɪ', ('ə', 'ɛ')))),\n ('æ', ('æ', ('ɛ', ('ɛ', 'ɔ')))),\n ('u', ('æ', ('ɛ', ('o', 'æ')))),\n ('i', ('ɪ', ('ɛ', ('ɑ', 'ʊ')))),\n ('ɛ', ('ɑ', ('u', ('ʊ', 'e')))),\n ('e', ('ɪ', ('ʊ', ('e', 'ɛ')))),\n ('ɔ', ('i', ('u', ('ə', 'æ')))),\n ('u', ('u', ('ə', ('e', 'ɪ')))),\n ('e', ('e', ('i', ('æ', 'u')))),\n ('u', ('ɪ', ('i', ('ə', 'ə')))),\n ('æ', ('ə', ('i', ('u', 'ɑ')))),\n ('ʊ', ('o', ('ɑ', ('e', 'ɪ')))),\n ('æ', ('ɪ', ('æ', ('ɔ', 'æ')))),\n ('ʊ', ('i', ('ɛ', ('e', 'ʊ')))),\n ('ʊ', ('i', ('i', ('æ', 'ə')))),\n ('ʊ', ('æ', ('ɔ', ('o', 'ə')))),\n ('ɑ', ('æ', ('e', ('o', 'u')))),\n ('ɑ', ('e', ('ɛ', ('æ', 'ə')))),\n ('ʊ', ('ʊ', ('e', ('ə', 'ɪ')))),\n ('ɔ', ('æ', ('ɑ', ('o', 'u')))),\n ('ʊ', ('ʊ', ('i', ('ɪ', 'e')))),\n ('i', ('æ', ('e', ('æ', 'ɛ')))),\n ('ɛ', ('æ', ('ɔ', ('ə', 'ɔ')))),\n ('ʊ', ('ɪ', ('ɔ', ('o', 'ɪ')))),\n ('ɛ', ('u', ('e', ('ɔ', 'ɑ')))),\n ('ʊ', ('ə', ('ɔ', ('i', 'ɑ')))),\n ('æ', ('ɛ', ('o', ('u', 'ə')))),\n ('ə', ('ɔ', ('ɛ', ('ʊ', 'ə')))),\n ('i', ('ɪ', ('o', ('ɛ', 'u')))),\n ('o', ('i', ('æ', ('u', 'æ')))),\n ('ɛ', ('o', ('o', ('ʊ', 'ɛ')))),\n ('e', ('ɛ', ('ɪ', ('i', 'ə')))),\n ('i', ('ɑ', ('ɔ', ('ɔ', 'i')))),\n ('i', ('e', ('i', ('ɛ', 'u')))),\n ('ɑ', ('æ', ('ɑ', ('ɛ', 'æ')))),\n ('ə', ('ɛ', ('ɔ', ('i', 'ɛ')))),\n ('e', ('i', ('ɔ', ('u', 'æ')))),\n ('u', ('u', ('ʊ', ('ɛ', 'ɛ')))),\n ('ɔ', ('ɑ', ('o', ('o', 'æ')))),\n ('ɑ', ('i', ('ɑ', ('o', 'ɔ')))),\n ('e', ('æ', ('u', ('o', 'ə')))),\n ('ɪ', ('ə', ('ɑ', ('ɔ', 'ə')))),\n ('ɪ', ('ɑ', ('ə', ('ʊ', 'u')))),\n ('i', ('i', ('u', ('ɔ', 'e')))),\n ('ɛ', ('u', ('u', ('ɑ', 'æ')))),\n ('ɔ', ('ə', ('ə', ('ɑ', 'æ')))),\n ('e', ('ɑ', ('ə', ('ə', 'e')))),\n ('æ', ('æ', ('ɪ', ('æ', 'ɑ')))),\n ('ɑ', ('u', ('e', ('ʊ', 'i')))),\n ('ə', ('o', ('ɑ', ('u', 'o')))),\n ('o', ('i', ('o', ('e', 'e')))),\n ('ɪ', ('o', ('e', ('ɑ', 'e')))),\n ('ɔ', ('æ', ('u', ('ɑ', 'ɪ')))),\n ('ɑ', ('ɛ', ('e', ('ɑ', 'ɑ')))),\n ('u', ('i', ('o', ('e', 'ʊ')))),\n ('ɔ', ('u', ('ɑ', ('ʊ', 'i')))),\n ('ʊ', ('u', ('ə', ('ɔ', 'u')))),\n ('ə', ('ə', ('ɑ', ('ʊ', 'ə')))),\n ('u', ('ɛ', ('e', ('i', 'e')))),\n ('u', ('ɔ', ('æ', ('i', 'æ')))),\n ('u', ('ɛ', ('u', ('e', 'ɑ')))),\n ('ɪ', ('ɛ', ('æ', ('æ', 'ɔ')))),\n ('i', ('ɪ', ('e', ('u', 'ɛ')))),\n ('ɔ', ('o', ('i', ('æ', 'o')))),\n ('o', ('i', ('i', ('ə', 'i')))),\n ('ʊ', ('i', ('ɑ', ('ɪ', 'ɑ')))),\n ('ɛ', ('e', ('ɪ', ('e', 'ɪ')))),\n ('o', ('ɔ', ('ɪ', ('ʊ', 'u')))),\n ('ɪ', ('ɛ', ('æ', ('æ', 'ɪ')))),\n ('ɛ', ('u', ('ʊ', ('æ', 'ɛ')))),\n ('i', ('ɔ', ('ə', ('u', 'ə')))),\n ('ɪ', ('ɛ', ('ɑ', ('ɑ', 'ɛ')))),\n ('ɛ', ('æ', ('ʊ', ('i', 'ə')))),\n ('ə', ('i', ('ɔ', ('ʊ', 'e')))),\n ('i', ('i', ('ɛ', ('ɛ', 'æ')))),\n ('æ', ('ʊ', ('ɪ', ('ɔ', 'e')))),\n ('ɪ', ('æ', ('ɔ', ('ɔ', 'o')))),\n ('o', ('ə', ('u', ('o', 'ɔ')))),\n ('u', ('æ', ('u', ('ɔ', 'ɑ')))),\n ('æ', ('i', ('ɛ', ('i', 'o')))),\n ('e', ('ɛ', ('e', ('ɛ', 'i')))),\n ('ɪ', ('i', ('ɪ', ('ʊ', 'ɛ')))),\n ('i', ('ɑ', ('ɑ', ('æ', 'ʊ')))),\n ('ə', ('ɑ', ('ɛ', ('ɛ', 'e')))),\n ('ɔ', ('i', ('i', ('ɛ', 'ɪ')))),\n ('ɑ', ('ʊ', ('ɛ', ('i', 'æ')))),\n ('ɛ', ('ɑ', ('i', ('u', 'ə')))),\n ('ɑ', ('ɛ', ('ɪ', ('ɪ', 'ɑ')))),\n ('æ', ('ɔ', ('u', ('ʊ', 'e')))),\n ('ɑ', ('ʊ', ('ʊ', ('i', 'i')))),\n ('ə', ('æ', ('ɔ', ('ʊ', 'o')))),\n ('æ', ('æ', ('o', ('i', 'o')))),\n ('ɑ', ('æ', ('u', ('æ', 'e')))),\n ('ɛ', ('i', ('e', ('ɑ', 'æ')))),\n ('æ', ('æ', ('ə', ('ɪ', 'o')))),\n ('i', ('ɛ', ('i', ('ɔ', 'æ')))),\n ('ʊ', ('o', ('e', ('æ', 'u')))),\n ('æ', ('ʊ', ('u', ('æ', 'ɪ')))),\n ('u', ('ɪ', ('ɔ', ('ɔ', 'ɪ')))),\n ('ʊ', ('e', ('ʊ', ('æ', 'ɪ')))),\n ('e', ('ʊ', ('i', ('ʊ', 'e')))),\n ('ɪ', ('e', ('ɪ', ('ɪ', 'ɛ')))),\n ('ɪ', ('u', ('ɑ', ('æ', 'ɑ')))),\n ('ɪ', ('o', ('ɪ', ('u', 'u')))),\n ('ə', ('o', ('ə', ('æ', 'ə')))),\n ('ɛ', ('u', ('o', ('ɛ', 'æ')))),\n ('e', ('ʊ', ('u', ('u', 'u')))),\n ('ɪ', ('i', ('ə', ('i', 'ʊ')))),\n ('o', ('ɛ', ('u', ('o', 'ɪ')))),\n ('ʊ', ('ɑ', ('o', ('ɪ', 'ʊ')))),\n ('ɪ', ('u', ('ɪ', ('i', 'ɔ')))),\n ('ɔ', ('æ', ('i', ('e', 'ə')))),\n ('ɔ', ('ʊ', ('e', ('ɛ', 'æ')))),\n ('ɪ', ('ə', ('u', ('ɑ', 'ɔ')))),\n ('ʊ', ('e', ('ʊ', ('ə', 'u')))),\n ('o', ('ʊ', ('o', ('o', 'ɔ')))),\n ('ɛ', ('ɑ', ('u', ('u', 'ɪ')))),\n ('ɛ', ('i', ('æ', ('e', 'i')))),\n ('u', ('ɔ', ('ɪ', ('ə', 'æ')))),\n ('ʊ', ('ʊ', ('ʊ', ('ɔ', 'ɑ')))),\n ('ɑ', ('æ', ('ʊ', ('e', 'o')))),\n ('ə', ('u', ('e', ('i', 'i')))),\n ('i', ('ɔ', ('e', ('e', 'ə')))),\n ('æ', ('ʊ', ('ʊ', ('ɑ', 'ə')))),\n ('æ', ('ɔ', ('æ', ('ʊ', 'ɛ')))),\n ('ə', ('o', ('e', ('e', 'ɛ')))),\n ('ɑ', ('ɛ', ('ə', ('ə', 'o')))),\n ('ɑ', ('u', ('u', ('æ', 'ɛ')))),\n ('ə', ('e', ('e', ('ə', 'ɑ')))),\n ('u', ('ɪ', ('ɔ', ('u', 'ɔ')))),\n ('o', ('ɔ', ('u', ('ɔ', 'ɛ')))),\n ('ə', ('æ', ('ɔ', ('ʊ', 'ɑ')))),\n ('e', ('u', ('o', ('i', 'ɔ')))),\n ('ɑ', ('e', ('u', ('i', 'e')))),\n ('u', ('ɑ', ('ɔ', ('u', 'ɪ')))),\n ('ɔ', ('æ', ('u', ('e', 'ɪ')))),\n ('e', ('ʊ', ('ɑ', ('ɑ', 'i')))),\n ('ɑ', ('ɑ', ('ɪ', ('ɑ', 'ə')))),\n ('ɔ', ('u', ('ɛ', ('ʊ', 'ɛ')))),\n ('ə', ('ɛ', ('ɛ', ('ɔ', 'o')))),\n ('ʊ', ('ɔ', ('æ', ('ʊ', 'i')))),\n ('u', ('e', ('ɪ', ('e', 'u')))),\n ('ɛ', ('e', ('i', ('o', 'o')))),\n ('e', ('e', ('ɪ', ('ʊ', 'æ')))),\n ('o', ('ɪ', ('ɑ', ('o', 'o')))),\n ('ə', ('u', ('i', ('u', 'e')))),\n ('ə', ('ə', ('i', ('u', 'ɔ')))),\n ('ɪ', ('o', ('e', ('ɑ', 'i')))),\n ('ɑ', ('ə', ('ɛ', ('ɑ', 'u')))),\n ('e', ('ɔ', ('e', ('o', 'i')))),\n ('ɪ', ('ʊ', ('i', ('ɔ', 'ə')))),\n ('e', ('i', ('æ', ('o', 'u')))),\n ('i', ('ɛ', ('ɑ', ('ɪ', 'e')))),\n ('i', ('ɑ', ('ɑ', ('æ', 'u')))),\n ('o', ('e', ('ɔ', ('ɪ', 'ɛ')))),\n ('ɑ', ('i', ('ɑ', ('ə', 'i')))),\n ('o', ('ɛ', ('u', ('ɛ', 'æ')))),\n ('ə', ('ɑ', ('æ', ('o', 'ʊ')))),\n ('ɛ', ('ʊ', ('i', ('ʊ', 'ʊ')))),\n ('æ', ('ɔ', ('i', ('u', 'ə')))),\n ('ə', ('u', ('ɔ', ('o', 'e')))),\n ('e', ('o', ('ɑ', ('o', 'o')))),\n ('e', ('e', ('e', ('ɛ', 'ɔ')))),\n ('e', ('ɪ', ('ʊ', ('u', 'o')))),\n ('ɛ', ('æ', ('ɪ', ('i', 'ɛ')))),\n ('ɛ', ('ɑ', ('ə', ('ɑ', 'o')))),\n ('o', ('ɪ', ('ɔ', ('ɑ', 'ɛ')))),\n ('e', ('u', ('ʊ', ('i', 'o')))),\n ('ɑ', ('ɑ', ('ɔ', ('ɑ', 'ə')))),\n ('e', ('ʊ', ('ə', ('e', 'o')))),\n ('ə', ('ə', ('e', ('u', 'æ')))),\n ('ɑ', ('ɛ', ('ʊ', ('i', 'ʊ')))),\n ('o', ('e', ('æ', ('ʊ', 'æ')))),\n ('ʊ', ('ɑ', ('o', ('ɪ', 'u')))),\n ('ʊ', ('o', ('e', ('æ', 'ə')))),\n ('e', ('ɔ', ('æ', ('ɛ', 'e')))),\n ('ɪ', ('o', ('e', ('ɔ', 'e')))),\n ('æ', ('ə', ('ɔ', ('ɪ', 'u')))),\n ('ɑ', ('e', ('ɑ', ('ɑ', 'ə')))),\n ('u', ('ə', ('e', ('u', 'ɪ')))),\n ('æ', ('o', ('ɪ', ('ɪ', 'ʊ')))),\n ('ɪ', ('ɪ', ('ɑ', ('ə', 'ɔ')))),\n ('o', ('ɔ', ('ɔ', ('e', 'ɔ')))),\n ('ʊ', ('e', ('i', ('o', 'ə')))),\n ('ɛ', ('ʊ', ('ə', ('ɪ', 'ɑ')))),\n ('æ', ('ɔ', ('u', ('u', 'ɪ')))),\n ('æ', ('ɛ', ('i', ('ɔ', 'e')))),\n ('æ', ('ʊ', ('ɛ', ('ə', 'o')))),\n ('u', ('æ', ('ə', ('e', 'ʊ')))),\n ('ɛ', ('ɔ', ('ə', ('e', 'ɔ')))),\n ('ʊ', ('æ', ('ɛ', ('u', 'i')))),\n ('u', ('o', ('e', ('ə', 'i')))),\n ('e', ('o', ('ɔ', ('ɑ', 'ɛ')))),\n ('o', ('u', ('i', ('e', 'ɑ')))),\n ('ɪ', ('ɛ', ('o', ('ə', 'ɪ')))),\n ('e', ('e', ('ɔ', ('ɔ', 'u')))),\n ('e', ('æ', ('ʊ', ('i', 'æ')))),\n ('ə', ('ɪ', ('æ', ('æ', 'ɔ')))),\n ('e', ('ɪ', ('ʊ', ('u', 'ɑ')))),\n ('ɪ', ('o', ('ɛ', ('ʊ', 'ʊ')))),\n ('ə', ('ɪ', ('ɔ', ('ɛ', 'u')))),\n ('u', ('o', ('o', ('ə', 'u')))),\n ('e', ('ɔ', ('u', ('i', 'ə')))),\n ('ɔ', ('e', ('ɛ', ('ɛ', 'e')))),\n ('u', ('u', ('e', ('ʊ', 'ʊ')))),\n ('ɑ', ('ʊ', ('u', ('ɑ', 'i')))),\n ('e', ('e', ('æ', ('ɔ', 'ɔ')))),\n ('ɪ', ('ɔ', ('ɛ', ('u', 'ʊ')))),\n ('ə', ('ɪ', ('æ', ('æ', 'ɪ')))),\n ('e', ('ɑ', ('ʊ', ('æ', 'e')))),\n ('ə', ('ɪ', ('ɑ', ('ɑ', 'ɛ')))),\n ('ɑ', ('ɔ', ('æ', ('e', 'ə')))),\n ('e', ('ɑ', ('i', ('u', 'æ')))),\n ('æ', ('e', ('ɛ', ('ʊ', 'i')))),\n ('ɑ', ('ɪ', ('o', ('ɛ', 'ɔ')))),\n ('o', ('o', ('i', ('ɑ', 'æ')))),\n ('ʊ', ('ɪ', ('ɑ', ('ə', 'ɑ')))),\n ('ɪ', ('æ', ('i', ('u', 'u')))),\n ('ɔ', ('u', ('ʊ', ('ɪ', 'o')))),\n ('i', ('u', ('ɪ', ('ə', 'æ')))),\n ('ɛ', ('ɪ', ('ɑ', ('o', 'ʊ')))),\n ('ə', ('ɛ', ('ʊ', ('ɑ', 'ɛ')))),\n ('ə', ('ʊ', ('i', ('e', 'ɪ')))),\n ('e', ('u', ('i', ('ɔ', 'ɪ')))),\n ('ʊ', ('ə', ('ɔ', ('e', 'ə')))),\n ('ɑ', ('o', ('o', ('ɑ', 'i')))),\n ('ɪ', ('i', ('i', ('ɔ', 'ə')))),\n ('i', ('ɔ', ('e', ('ɪ', 'æ')))),\n ('ə', ('ɑ', ('æ', ('e', 'ɑ')))),\n ('ɔ', ('ɛ', ('ʊ', ('æ', 'æ')))),\n ('ɑ', ('ɑ', ('æ', ('ɛ', 'u')))),\n ('ɛ', ('e', ('e', ('ə', 'æ')))),\n ('ʊ', ('æ', ('æ', ('ɛ', 'ʊ')))),\n ('ʊ', ('ə', ('æ', ('o', 'ɪ')))),\n ('ɑ', ('ʊ', ('ɪ', ('ə', 'ɑ')))),\n ('e', ('ɑ', ('u', ('e', 'e')))),\n ('u', ('ʊ', ('ɑ', ('æ', 'o')))),\n ('o', ('i', ('i', ('u', 'i')))),\n ('æ', ('e', ('o', ('ə', 'æ')))),\n ('æ', ('u', ('ə', ('æ', 'ɑ')))),\n ('ɑ', ('i', ('ɛ', ('ə', 'ɛ')))),\n ('ɔ', ('ə', ('i', ('ɑ', 'æ')))),\n ('ʊ', ('o', ('ɔ', ('ɔ', 'ɛ')))),\n ('ɪ', ('i', ('ə', ('ʊ', 'e')))),\n ('i', ('i', ('ɪ', ('ɔ', 'e')))),\n ('o', ('æ', ('ə', ('i', 'ɔ')))),\n ('ʊ', ('ɑ', ('ɔ', ('e', 'ʊ')))),\n ('ɪ', ('e', ('o', ('æ', 'e')))),\n ('ɪ', ('ɔ', ('e', ('ɔ', 'ɑ')))),\n ('ʊ', ('ɪ', ('ʊ', ('ɪ', 'u')))),\n ('o', ('ɛ', ('ʊ', ('ɪ', 'o')))),\n ('i', ('ɑ', ('ɑ', ('æ', 'ə')))),\n ('ɑ', ('o', ('i', ('ə', 'u')))),\n ('e', ('ɛ', ('ʊ', ('ə', 'ə')))),\n ('ɛ', ('ɔ', ('ɔ', ('i', 'o')))),\n ('i', ('ʊ', ('e', ('æ', 'ɑ')))),\n ('ɪ', ('i', ('ɛ', ('æ', 'u')))),\n ('u', ('ɛ', ('ɪ', ('e', 'ɑ')))),\n ('ɔ', ('i', ('æ', ('ɑ', 'ɪ')))),\n ('ɑ', ('ɔ', ('ʊ', ('ɛ', 'ə')))),\n ('ɔ', ('ɪ', ('i', ('ɛ', 'ə')))),\n ('ə', ('ɔ', ('e', ('ʊ', 'ɑ')))),\n ('ʊ', ('ɪ', ('ɪ', ('i', 'ə')))),\n ('o', ('ɪ', ('u', ('ɛ', 'ɪ')))),\n ('ɪ', ('e', ('ə', ('ɪ', 'e')))),\n ('ɪ', ('ɔ', ('u', ('ɑ', 'æ')))),\n ('ʊ', ('ə', ('e', ('e', 'e')))),\n ('e', ('ɑ', ('o', ('æ', 'æ')))),\n ('æ', ('u', ('æ', ('i', 'e')))),\n ('ɔ', ('æ', ('ɑ', ('ə', 'æ')))),\n ('ɑ', ('ʊ', ('o', ('e', 'ʊ')))),\n ('ɑ', ('ɪ', ('ʊ', ('ɛ', 'o')))),\n ('i', ('e', ('ə', ('u', 'ʊ')))),\n ('ɪ', ('æ', ('o', ('æ', 'u')))),\n ('ɔ', ('e', ('ə', ('ə', 'u')))),\n ('o', ('ɪ', ('ɛ', ('ɪ', 'ə')))),\n ('i', ('ɪ', ('o', ('æ', 'ɔ')))),\n ('e', ('ɑ', ('ə', ('ɔ', 'o')))),\n ('ɑ', ('ə', ('ɛ', ('ɪ', 'ɔ')))),\n ('ʊ', ('ə', ('u', ('o', 'o')))),\n ('ɔ', ('æ', ('ɪ', ('æ', 'ə')))),\n ('ʊ', ('ɛ', ('ɑ', ('ɪ', 'ɛ')))),\n ('ɛ', ('u', ('u', ('ɪ', 'ɛ')))),\n ('ɔ', ('ə', ('ə', ('ɪ', 'ɛ')))),\n ('ʊ', ('ɪ', ('ɔ', ('ɛ', 'e')))),\n ('ɪ', ('ʊ', ('ə', ('ɛ', 'ɔ')))),\n ('i', ('i', ('ʊ', ('ɑ', 'ə')))),\n ('ə', ('ɪ', ('u', ('æ', 'ɑ')))),\n ('u', ('i', ('æ', ('u', 'o')))),\n ('ɪ', ('ɔ', ('ʊ', ('æ', 'ɛ')))),\n ('u', ('ə', ('u', ('ɔ', 'ɔ')))),\n ('ɪ', ('i', ('ə', ('ɛ', 'ʊ')))),\n ('ə', ('ɔ', ('ʊ', ('o', 'ɔ')))),\n ('ɪ', ('ɪ', ('ɔ', ('o', 'ə')))),\n ('o', ('u', ('æ', ('ɔ', 'ʊ')))),\n ('ɔ', ('e', ('ɑ', ('i', 'æ')))),\n ('ʊ', ('ʊ', ('ʊ', ('ʊ', 'ɪ')))),\n ('u', ('e', ('ɔ', ('ə', 'ə')))),\n ('ɛ', ('e', ('æ', ('ɔ', 'o')))),\n ('ʊ', ('ɪ', ('i', ('i', 'u')))),\n ('ɛ', ('ə', ('i', ('o', 'ɔ')))),\n ('ə', ('e', ('e', ('u', 'ɑ')))),\n ('æ', ('ɑ', ('ɔ', ('u', 'i')))),\n ('ə', ('i', ('e', ('ʊ', 'ɪ')))),\n ('ɛ', ('i', ('æ', ('ə', 'u')))),\n ('ʊ', ('o', ('ɪ', ('e', 'ə')))),\n ('ɑ', ('o', ('o', ('e', 'i')))),\n ('ɛ', ('ɛ', ('e', ('i', 'ə')))),\n ('æ', ('ɑ', ('o', ('o', 'ə')))),\n ('æ', ('ʊ', ('ʊ', ('ɪ', 'æ')))),\n ('ɪ', ('e', ('ʊ', ('ɔ', 'ɪ')))),\n ('æ', ('ə', ('æ', ('e', 'o')))),\n ('ə', ('æ', ('u', ('ə', 'o')))),\n ('ɛ', ('ɛ', ('o', ('e', 'ə')))),\n ('i', ('ʊ', ('æ', ('ɔ', 'e')))),\n ('ɛ', ('ʊ', ('ɔ', ('ɛ', 'æ')))),\n ('o', ('e', ('æ', ('e', 'u')))),\n ('ʊ', ('e', ('ɑ', ('o', 'e')))),\n ('e', ('ɪ', ('u', ('ɛ', 'e')))),\n ('u', ('ɔ', ('ɛ', ('æ', 'ɛ')))),\n ('ɔ', ('u', ('ɔ', ('ɪ', 'i')))),\n ('ɪ', ('ʊ', ('ɪ', ('ʊ', 'e')))),\n ('e', ('ɑ', ('ʊ', ('ɑ', 'ɔ')))),\n ('ʊ', ('i', ('u', ('ə', 'æ')))),\n ('i', ('o', ('i', ('ɪ', 'i')))),\n ('æ', ('u', ('ɛ', ('æ', 'ɛ')))),\n ('o', ('ɑ', ('ʊ', ('ɑ', 'æ')))),\n ('ɪ', ('ɪ', ('æ', ('i', 'ʊ')))),\n ('o', ('i', ('i', ('i', 'ʊ')))),\n ('æ', ('ɛ', ('ɔ', ('ə', 'o')))),\n ('ə', ('u', ('ɑ', ('e', 'i')))),\n ('u', ('e', ('ɛ', ('ɔ', 'ɪ')))),\n ('ɪ', ('u', ('i', ('ɑ', 'ɑ')))),\n ('u', ('o', ('ɪ', ('ʊ', 'o')))),\n ('ɪ', ('e', ('e', ('ə', 'ʊ')))),\n ('ɛ', ('i', ('e', ('ɪ', 'ɛ')))),\n ('æ', ('ə', ('ʊ', ('ɔ', 'æ')))),\n ('ə', ('ɑ', ('ʊ', ('ɑ', 'u')))),\n ('o', ('e', ('o', ('o', 'ɪ')))),\n ('æ', ('e', ('ɛ', ('e', 'o')))),\n ('ɛ', ('ɛ', ('ɛ', ('e', 'ɑ')))),\n ('ɑ', ('o', ('ɛ', ('ɔ', 'ʊ')))),\n ('ɔ', ('i', ('u', ('ə', 'ɑ')))),\n ('ʊ', ('æ', ('ɑ', ('o', 'u')))),\n ('i', ('ɛ', ('ɑ', ('i', 'ɔ')))),\n ('i', ('æ', ('ɪ', ('u', 'ɑ')))),\n ('e', ('ə', ('ɪ', ('ɔ', 'ə')))),\n ('ɪ', ('ɑ', ('æ', ('ʊ', 'ə')))),\n ('u', ('ə', ('ɑ', ('æ', 'ɛ')))),\n ('u', ('ɪ', ('e', ('o', 'o')))),\n ('ɪ', ('ɔ', ('ə', ('o', 'o')))),\n ('e', ('ɪ', ('ɔ', ('i', 'ʊ')))),\n ('æ', ('i', ('u', ('u', 'ʊ')))),\n ('o', ('o', ('ɪ', ('æ', 'e')))),\n ('o', ('o', ('ə', ('ɑ', 'ɑ')))),\n ('æ', ('ʊ', ('i', ('i', 'æ')))),\n ('ə', ('ʊ', ('ɪ', ('o', 'ʊ')))),\n ('æ', ('æ', ('ɪ', ('u', 'u')))),\n ('ə', ('æ', ('u', ('e', 'ə')))),\n ('e', ('ɛ', ('ɑ', ('ʊ', 'ɛ')))),\n ('ɑ', ('e', ('æ', ('ʊ', 'i')))),\n ('ʊ', ('ɑ', ('o', ('o', 'æ')))),\n ('ɪ', ('i', ('ɛ', ('æ', 'ə')))),\n ('ɛ', ('æ', ('ɛ', ('ɔ', 'ɑ')))),\n ('æ', ('ə', ('ɔ', ('o', 'æ')))),\n ('ɛ', ('ɛ', ('ə', ('ɪ', 'i')))),\n ('ʊ', ('ə', ('ə', ('ɑ', 'æ')))),\n ('ɑ', ('i', ('ɛ', ('i', 'e')))),\n ('ɔ', ('u', ('u', ('æ', 'ɔ')))),\n ('ɑ', ('i', ('æ', ('i', 'u')))),\n ('e', ('ə', ('i', ('ɪ', 'ɔ')))),\n ('o', ('i', ('æ', ('u', 'ɑ')))),\n ('æ', ('ɪ', ('o', ('u', 'ʊ')))),\n ('o', ('ɔ', ('ɛ', ('ɛ', 'ɔ')))),\n ('ɑ', ('ɑ', ('ɔ', ('ɪ', 'æ')))),\n ('e', ('ɛ', ('ɪ', ('ɑ', 'ɛ')))),\n ('ʊ', ('æ', ('u', ('ɑ', 'ɪ')))),\n ('ʊ', ('u', ('ɑ', ('ʊ', 'i')))),\n ('i', ('o', ('ɪ', ('ɪ', 'u')))),\n ('i', ('i', ('ɛ', ('ə', 'o')))),\n ('ɔ', ('ɛ', ('ə', ('e', 'ɛ')))),\n ('ɔ', ('ə', ('ɪ', ('æ', 'e')))),\n ('ɪ', ('ɑ', ('o', ('ɛ', 'ɪ')))),\n ('ə', ('o', ('ɛ', ('æ', 'ɪ')))),\n ('ʊ', ('o', ('i', ('æ', 'o')))),\n ('i', ('u', ('ɔ', ('æ', 'ʊ')))),\n ('e', ('ɑ', ('i', ('ɑ', 'ɑ')))),\n ('ɑ', ('ɔ', ('ə', ('i', 'ʊ')))),\n ('i', ('ɪ', ('ʊ', ('o', 'u')))),\n ('ɪ', ('ɑ', ('ə', ('æ', 'ɑ')))),\n ('e', ('u', ('e', ('ʊ', 'ə')))),\n ('o', ('ʊ', ('ɛ', ('u', 'ʊ')))),\n ('ɪ', ('e', ('e', ('ɑ', 'o')))),\n ('u', ('i', ('ɔ', ('i', 'ɛ')))),\n ('æ', ('ə', ('i', ('æ', 'ɔ')))),\n ('æ', ('ɛ', ('ʊ', ('u', 'æ')))),\n ('æ', ('ɪ', ('ɑ', ('æ', 'u')))),\n ('ɪ', ('ə', ('u', ('ɔ', 'u')))),\n ('ɪ', ('u', ('ɛ', ('e', 'ɔ')))),\n ('æ', ('ɛ', ('o', ('ɑ', 'ɔ')))),\n ('æ', ('ə', ('ʊ', ('ɛ', 'ɑ')))),\n ('e', ('i', ('ɛ', ('o', 'u')))),\n ('ɔ', ('ɪ', ('ə', ('ʊ', 'o')))),\n ('o', ('ɛ', ('u', ('ʊ', 'ɔ')))),\n ('ɪ', ('ʊ', ('o', ('ɔ', 'ə')))),\n ('ɪ', ('o', ('e', ('ɔ', 'ə')))),\n ('æ', ('ə', ('ɪ', ('æ', 'i')))),\n ('ɑ', ('ɑ', ('e', ('ɔ', 'ʊ')))),\n ('æ', ('ɔ', ('o', ('ɔ', 'u')))),\n ('i', ('u', ('ɪ', ('u', 'æ')))),\n ('æ', ('æ', ('e', ('ɑ', 'i')))),\n ('ɑ', ('u', ('o', ('i', 'ɛ')))),\n ('e', ('i', ('u', ('ɔ', 'æ')))),\n ('ɑ', ('æ', ('ɔ', ('ɑ', 'u')))),\n ('e', ('i', ('ʊ', ('o', 'i')))),\n ('ɔ', ('ɪ', ('æ', ('u', 'i')))),\n ('ə', ('o', ('e', ('ʊ', 'ə')))),\n ('o', ('ɔ', ('u', ('o', 'ʊ')))),\n ('i', ('æ', ('i', ('ɪ', 'ʊ')))),\n ('æ', ('ɛ', ('e', ('e', 'e')))),\n ('e', ('ʊ', ('ɛ', ('ʊ', 'ʊ')))),\n ('ɛ', ('ɛ', ('o', ('ɪ', 'æ')))),\n ('ə', ('ɔ', ('u', ('ɪ', 'ɔ')))),\n ('ɛ', ('ɔ', ('u', ('ʊ', 'ə')))),\n ('ə', ('e', ('æ', ('i', 'i')))),\n ('ʊ', ('æ', ('i', ('e', 'ə')))),\n ('ʊ', ('ʊ', ('e', ('ɛ', 'æ')))),\n ('e', ('ɪ', ('ɔ', ('i', 'u')))),\n ('ɪ', ('ʊ', ('o', ('ɪ', 'i')))),\n ('o', ('ʊ', ('e', ('ɔ', 'ɑ')))),\n ('o', ('o', ('ə', ('ə', 'ə')))),\n ('ɛ', ('ɔ', ('u', ('ɛ', 'ə')))),\n ('o', ('ɛ', ('ɑ', ('ɪ', 'æ')))),\n ('ə', ('ɔ', ('æ', ('ɔ', 'u')))),\n ('ɔ', ('ɪ', ('ə', ('ʊ', 'ɑ')))),\n ('æ', ('ʊ', ('ɑ', ('æ', 'i')))),\n ('æ', ('u', ('ə', ('ɛ', 'ɛ')))),\n ('i', ('u', ('e', ('ɑ', 'ɛ')))),\n ('ɛ', ('ɛ', ('ɪ', ('ɑ', 'æ')))),\n ('æ', ('æ', ('e', ('ɔ', 'e')))),\n ('u', ('u', ('ɔ', ('æ', 'u')))),\n ('ɪ', ('ɛ', ('i', ('ʊ', 'e')))),\n ('o', ('i', ('ɔ', ('e', 'æ')))),\n ('ɑ', ('o', ('ɔ', ('o', 'ɔ')))),\n ('ɪ', ('ɑ', ('ɪ', ('u', 'æ')))),\n ('ɔ', ('ɛ', ('u', ('ɪ', 'æ')))),\n ('o', ('ɑ', ('ɔ', ('ɔ', 'ɔ')))),\n ('æ', ('ɔ', ('ʊ', ('u', 'ɑ')))),\n ('ɪ', ('u', ('ɪ', ('ɔ', 'ɪ')))),\n ('ʊ', ('æ', ('u', ('e', 'ɪ')))),\n ('ɔ', ('æ', ('ɑ', ('u', 'æ')))),\n ('ɔ', ('ɑ', ('e', ('i', 'ɛ')))),\n ('ə', ('i', ('ɔ', ('e', 'u')))),\n ('i', ('ɑ', ('ɔ', ('i', 'ɑ')))),\n ('ɑ', ('ɑ', ('ɪ', ('e', 'æ')))),\n ('u', ('ɔ', ('æ', ('ə', 'e')))),\n ('æ', ('ɛ', ('e', ('e', 'ʊ')))),\n ('æ', ('i', ('o', ('ɑ', 'ə')))),\n ('ʊ', ('u', ('ɛ', ('ʊ', 'ɛ')))),\n ('o', ('ɔ', ('o', ('i', 'o')))),\n ('ɛ', ('e', ('u', ('u', 'ə')))),\n ('o', ('ɔ', ('ə', ('ɪ', 'o')))),\n ('e', ('i', ('u', ('ʊ', 'ɑ')))),\n ('ɛ', ('ɑ', ('ɪ', ('ɑ', 'i')))),\n ('æ', ('u', ('æ', ('ə', 'e')))),\n ('ə', ('u', ('ɪ', ('ʊ', 'ɪ')))),\n ('ɛ', ('ɪ', ('ɛ', ('i', 'æ')))),\n ('ɔ', ('e', ('ɛ', ('e', 'e')))),\n ('e', ('ɔ', ('o', ('e', 'o')))),\n ('ə', ('ʊ', ('o', ('e', 'ɪ')))),\n ('i', ('ʊ', ('e', ('o', 'ʊ')))),\n ('ɛ', ('ɪ', ('ʊ', ('i', 'i')))),\n ('ə', ('o', ('e', ('e', 'ɪ')))),\n ('i', ('e', ('i', ('ɪ', 'e')))),\n ('i', ('ɪ', ('ɔ', ('ʊ', 'o')))),\n ('o', ('ə', ('o', ('ɔ', 'i')))),\n ('ɔ', ('ʊ', ('ə', ('ɛ', 'ɪ')))),\n ('ɑ', ('ɑ', ('æ', ('ɔ', 'ə')))),\n ('ɔ', ('ɔ', ('e', ('ɔ', 'i')))),\n ('æ', ('ə', ('u', ('ɔ', 'i')))),\n ('u', ('ʊ', ('i', ('ɛ', 'ɔ')))),\n ('ɑ', ('æ', ('o', ('e', 'u')))),\n ('ɛ', ('ɔ', ('o', ('ɪ', 'ɑ')))),\n ('ə', ('ɛ', ('i', ('o', 'ʊ')))),\n ('ə', ('i', ('ɪ', ('ɔ', 'i')))),\n ('ɛ', ('ɔ', ('u', ('e', 'ɪ')))),\n ('ɪ', ('u', ('ɪ', ('u', 'ɔ')))),\n ('e', ('ʊ', ('i', ('e', 'u')))),\n ('i', ('ɪ', ('ʊ', ('o', 'ə')))),\n ('u', ('o', ('ɑ', ('ɛ', 'ɛ')))),\n ('i', ('i', ('ʊ', ('ɪ', 'æ')))),\n ('ɪ', ('ɪ', ('o', ('ɪ', 'ə')))),\n ('ɛ', ('i', ('æ', ('u', 'u')))),\n ('ɑ', ('ʊ', ('ɛ', ('ə', 'e')))),\n ('ə', ('i', ('ʊ', ('æ', 'o')))),\n ('u', ('ɛ', ('o', ('ɪ', 'o')))),\n ('u', ('ə', ('u', ('e', 'ɛ')))),\n ('æ', ('ɪ', ('ɑ', ('æ', 'ə')))),\n ('o', ('i', ('u', ('e', 'o')))),\n ('ɑ', ('e', ('ɑ', ('ə', 'ɛ')))),\n ('e', ('e', ('ɪ', ('ɪ', 'ɪ')))),\n ('i', ('ɑ', ('ɛ', ('i', 'i')))),\n ('e', ('ɪ', ('ɪ', ('ə', 'ɔ')))),\n ('u', ('i', ('ɛ', ('ɔ', 'o')))),\n ('e', ('ɛ', ('æ', ('u', 'i')))),\n ('u', ('ɔ', ('ɔ', ('e', 'ɪ')))),\n ('ɪ', ('ə', ('ə', ('ɪ', 'ə')))),\n ('ɛ', ('ɛ', ('i', ('i', 'o')))),\n ('ɔ', ('ɔ', ('u', ('ɪ', 'ɑ')))),\n ('e', ('ʊ', ('e', ('æ', 'æ')))),\n ('ɪ', ('ɪ', ('æ', ('æ', 'ʊ')))),\n ('æ', ('ʊ', ('ɛ', ('o', 'ɪ')))),\n ('ɔ', ('i', ('ɛ', ('ɑ', 'ɪ')))),\n ('i', ('ɔ', ('ɔ', ('æ', 'æ')))),\n ('ɑ', ('i', ('e', ('ɛ', 'o')))),\n ('e', ('ə', ('ə', ('ɪ', 'ɪ')))),\n ('i', ('ə', ('ɛ', ('e', 'e')))),\n ('i', ('ə', ('æ', ('e', 'u')))),\n ('ɔ', ('i', ('ɑ', ('ɛ', 'o')))),\n ('ɪ', ('ɔ', ('o', ('ə', 'ʊ')))),\n ('ɪ', ('ɔ', ('u', ('ɪ', 'ɛ')))),\n ('æ', ('ɔ', ('i', ('i', 'ɪ')))),\n ('ɪ', ('ɪ', ('æ', ('ɛ', 'ʊ')))),\n ('o', ('æ', ('ɪ', ('ɛ', 'ɪ')))),\n ('i', ('ʊ', ('ɑ', ('ə', 'ə')))),\n ('ə', ('i', ('e', ('ɪ', 'ɑ')))),\n ...}\n\n\nWhile exponentiation is defined in terms of the application of a bunch of binary \\(\\times\\), resulting in pairs of an element of \\(A\\) with pairs of an element of \\(A\\) with pairs of…we can always treat \\(A^N\\) as a set of \\(N\\)-tuples because we can always map elements of \\(A^N\\) to \\(N\\)-tuples.\n\\[\\mathrm{flatten}_A(a) = \\begin{cases}\\langle a \\rangle & \\text{if } a \\in A\\\\\n\\langle x\\;:\\;x \\in \\mathrm{flatten}(y) \\land y \\in a \\rangle & \\text{otherwise}\\end{cases}\\]\n\ndef flatten(t, a):\n    if t in a:\n        return (t,)\n    else:\n        return tuple(x for y in t for x in flatten(y, a))\n \n{flatten(x, vowels) for x in exponentiate(vowels, 2)}\n\n{('e', 'e'),\n ('e', 'i'),\n ('e', 'o'),\n ('e', 'u'),\n ('e', 'æ'),\n ('e', 'ɑ'),\n ('e', 'ɔ'),\n ('e', 'ə'),\n ('e', 'ɛ'),\n ('e', 'ɪ'),\n ('e', 'ʊ'),\n ('i', 'e'),\n ('i', 'i'),\n ('i', 'o'),\n ('i', 'u'),\n ('i', 'æ'),\n ('i', 'ɑ'),\n ('i', 'ɔ'),\n ('i', 'ə'),\n ('i', 'ɛ'),\n ('i', 'ɪ'),\n ('i', 'ʊ'),\n ('o', 'e'),\n ('o', 'i'),\n ('o', 'o'),\n ('o', 'u'),\n ('o', 'æ'),\n ('o', 'ɑ'),\n ('o', 'ɔ'),\n ('o', 'ə'),\n ('o', 'ɛ'),\n ('o', 'ɪ'),\n ('o', 'ʊ'),\n ('u', 'e'),\n ('u', 'i'),\n ('u', 'o'),\n ('u', 'u'),\n ('u', 'æ'),\n ('u', 'ɑ'),\n ('u', 'ɔ'),\n ('u', 'ə'),\n ('u', 'ɛ'),\n ('u', 'ɪ'),\n ('u', 'ʊ'),\n ('æ', 'e'),\n ('æ', 'i'),\n ('æ', 'o'),\n ('æ', 'u'),\n ('æ', 'æ'),\n ('æ', 'ɑ'),\n ('æ', 'ɔ'),\n ('æ', 'ə'),\n ('æ', 'ɛ'),\n ('æ', 'ɪ'),\n ('æ', 'ʊ'),\n ('ɑ', 'e'),\n ('ɑ', 'i'),\n ('ɑ', 'o'),\n ('ɑ', 'u'),\n ('ɑ', 'æ'),\n ('ɑ', 'ɑ'),\n ('ɑ', 'ɔ'),\n ('ɑ', 'ə'),\n ('ɑ', 'ɛ'),\n ('ɑ', 'ɪ'),\n ('ɑ', 'ʊ'),\n ('ɔ', 'e'),\n ('ɔ', 'i'),\n ('ɔ', 'o'),\n ('ɔ', 'u'),\n ('ɔ', 'æ'),\n ('ɔ', 'ɑ'),\n ('ɔ', 'ɔ'),\n ('ɔ', 'ə'),\n ('ɔ', 'ɛ'),\n ('ɔ', 'ɪ'),\n ('ɔ', 'ʊ'),\n ('ə', 'e'),\n ('ə', 'i'),\n ('ə', 'o'),\n ('ə', 'u'),\n ('ə', 'æ'),\n ('ə', 'ɑ'),\n ('ə', 'ɔ'),\n ('ə', 'ə'),\n ('ə', 'ɛ'),\n ('ə', 'ɪ'),\n ('ə', 'ʊ'),\n ('ɛ', 'e'),\n ('ɛ', 'i'),\n ('ɛ', 'o'),\n ('ɛ', 'u'),\n ('ɛ', 'æ'),\n ('ɛ', 'ɑ'),\n ('ɛ', 'ɔ'),\n ('ɛ', 'ə'),\n ('ɛ', 'ɛ'),\n ('ɛ', 'ɪ'),\n ('ɛ', 'ʊ'),\n ('ɪ', 'e'),\n ('ɪ', 'i'),\n ('ɪ', 'o'),\n ('ɪ', 'u'),\n ('ɪ', 'æ'),\n ('ɪ', 'ɑ'),\n ('ɪ', 'ɔ'),\n ('ɪ', 'ə'),\n ('ɪ', 'ɛ'),\n ('ɪ', 'ɪ'),\n ('ɪ', 'ʊ'),\n ('ʊ', 'e'),\n ('ʊ', 'i'),\n ('ʊ', 'o'),\n ('ʊ', 'u'),\n ('ʊ', 'æ'),\n ('ʊ', 'ɑ'),\n ('ʊ', 'ɔ'),\n ('ʊ', 'ə'),\n ('ʊ', 'ɛ'),\n ('ʊ', 'ɪ'),\n ('ʊ', 'ʊ')}\n\n\nAnd we can always map back to the element of \\(A^N\\).\n\\[\\mathrm{reconstruct}_A(a) = \\begin{cases}a & \\text{if } a \\in A\\\\\n\\langle \\mathrm{head}(a), \\mathrm{reconstruct}_A(\\mathrm{tail}(a))\\rangle & \\text{otherwise}\\end{cases}\\]\nwhere \\(\\mathrm{head}\\) returns the first element of a tuple and \\(\\mathrm{tail}\\) returns the tuple with the first element removed."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html",
    "title": "Relations and Functions",
    "section": "",
    "text": "A relation \\(R\\) is a subset of the product of sets \\(A_1 \\times A_2 \\times \\ldots \\times A_N\\). That means that elements of a relation are \\(N\\)-tuples. Including a particular tuple in the relation represents that the things in the tuple have that relationship to each other.\nFor example, the relation between vowel height \\(H = \\{\\mathrm{high}, \\mathrm{mid}, \\mathrm{low}\\}\\) and the English vowels \\(V = \\{\\mathrm{i}, \\mathrm{u}, ...\\}\\) that have that height and can be represented as a relation \\(R_\\text{height-of}\\).\n\nheight_of: set[tuple[str, str]] = {\n    ('high', 'i'),\n    ('high', 'ɪ'),\n    ('high', 'u'),\n    ('high', 'ʊ'),\n    ('mid', 'e'),\n    ('mid', 'ɛ'),\n    ('mid', 'o'),\n    ('mid', 'ɔ'),\n    ('mid', 'ə'),\n    ('low', 'ɑ'),\n    ('low', 'æ')\n}\n\n\\(R_\\text{height-of}\\) is a relation, since it is the subset of \\(H \\times V\\).\n\\[R_\\text{height-of} \\subseteq H \\times V\\]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html#relations",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html#relations",
    "title": "Relations and Functions",
    "section": "",
    "text": "A relation \\(R\\) is a subset of the product of sets \\(A_1 \\times A_2 \\times \\ldots \\times A_N\\). That means that elements of a relation are \\(N\\)-tuples. Including a particular tuple in the relation represents that the things in the tuple have that relationship to each other.\nFor example, the relation between vowel height \\(H = \\{\\mathrm{high}, \\mathrm{mid}, \\mathrm{low}\\}\\) and the English vowels \\(V = \\{\\mathrm{i}, \\mathrm{u}, ...\\}\\) that have that height and can be represented as a relation \\(R_\\text{height-of}\\).\n\nheight_of: set[tuple[str, str]] = {\n    ('high', 'i'),\n    ('high', 'ɪ'),\n    ('high', 'u'),\n    ('high', 'ʊ'),\n    ('mid', 'e'),\n    ('mid', 'ɛ'),\n    ('mid', 'o'),\n    ('mid', 'ɔ'),\n    ('mid', 'ə'),\n    ('low', 'ɑ'),\n    ('low', 'æ')\n}\n\n\\(R_\\text{height-of}\\) is a relation, since it is the subset of \\(H \\times V\\).\n\\[R_\\text{height-of} \\subseteq H \\times V\\]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html#functions",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/relations-and-functions.html#functions",
    "title": "Relations and Functions",
    "section": "Functions",
    "text": "Functions\nFunctions–or more specifically, their graphs–are particular kinds of relations. The (graph of a) function \\(F \\subseteq X \\times Y\\) is a relation s.t. if \\(\\langle x, y \\rangle \\in F \\land \\langle x, z \\rangle \\in F\\), then \\(y = z\\).\nFor instance, the relation \\(R_\\text{height-of}\\) is not a function, but its inverse relation \\(R_\\text{height-of}^{-1} = R_\\text{has-height}\\) is.\n\\[R^{-1}_\\text{height-of} = R_\\text{has-height} = \\{\\langle y, x \\rangle \\;|\\; \\langle x, y \\rangle \\in R_\\text{height-of}\\}\\]\n\nhas_height: set[tuple[str, str]] = {(y, x) for x, y in height_of}\n\nhas_height\n\n{('e', 'mid'),\n ('i', 'high'),\n ('o', 'mid'),\n ('u', 'high'),\n ('æ', 'low'),\n ('ɑ', 'low'),\n ('ɔ', 'mid'),\n ('ə', 'mid'),\n ('ɛ', 'mid'),\n ('ɪ', 'high'),\n ('ʊ', 'high')}\n\n\nWe can check that it is a function by checking that:\n\nlen({x for x, _ in has_height}) == len(has_height)\n\nTrue\n\n\n\nlen({x for x, y in height_of}) == len(height_of)\n\nFalse\n\n\n\nPartiality\nA function is total if \\(X = \\{x \\;|\\; \\langle x, y \\rangle \\in F\\}\\); otherwise it is partial.\n\n\nDomain and codomain\nWe often think of functions as mapping inputs to outputs. The inputs of the function come from its domain and the outputs come from its codomain.\nFor instance, if we view \\(R_\\text{has-height}\\) as a function \\(f_\\text{has-height}\\), the domain of \\(f_\\text{has-height}\\) is the set of vowels \\(V\\) and the codomain is the set of heights \\(H\\). We will often express this using the following notation.\n\\[f_\\text{has-height}: V \\rightarrow H\\]\nSometimes we will express it using two other functions \\(\\text{dom}\\) and \\(\\text{cod}\\).\n\\[\\text{dom}(f_\\text{has-height}) = V\\] \\[\\text{cod}(f_\\text{has-height}) = H\\]\nFor a function \\(f\\) with graph \\(R\\), we denote each \\(y\\) s.t. \\(\\langle x, y \\rangle \\in F\\) as \\(f(x)\\).\n\n\nImage\nThe image of \\(W \\subseteq X\\) under \\(f\\) is \\(f(W) = \\{f(x) \\;|\\; x \\in W\\}\\).\n\nfrom typing import TypeVar\n\nT = TypeVar(\"T\")\n\ndef image(w: set[T], f: set[tuple[T, T]]) -&gt; set[T]:\n    return {y for x, y in f if x in w}\n\nimage({'i'}, has_height)\n\n{'high'}\n\n\n\nimage({'i', 'e'}, has_height)\n\n{'high', 'mid'}\n\n\n\n\nPreimage\nThe preimage of \\(Z \\subseteq Y\\) under \\(f\\) is \\(f^{-1}(Z) = \\{x \\;|\\; f(x) \\in Z\\}\\).\n\ndef preimage(z: set[T], f: set[tuple[T, T]]) -&gt; set[T]:\n    return {x for x, y in f if y in z}\n\npreimage({'high'}, has_height)\n\n{'i', 'u', 'ɪ', 'ʊ'}\n\n\n\nheights: set[str] = {h for h, _ in height_of}\n\npreimage(heights - {'low'}, has_height)\n\n{'e', 'i', 'o', 'u', 'ɔ', 'ə', 'ɛ', 'ɪ', 'ʊ'}\n\n\n\n\nRange\nThe range of \\(X\\) under \\(f\\) is the image of \\(X\\) under \\(f\\): \\(f(X)\\)."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sequences.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/sequences.html",
    "title": "Sequences",
    "section": "",
    "text": "A sequence \\(S\\) is a partial function from the natural numbers \\(\\mathbb{N}\\) to a set \\(\\Sigma\\).\n\\[\\mathbb{N} = \\{0, 1, 2, 3, \\ldots\\}\\] \\[\\Sigma = \\{\\text{e, i, o, u, æ, ɑ, ɔ, ə, ɛ, ɪ, ʊ, ɹ, d, t}\\}\\] \\[S = \\begin{Bmatrix}\n0 & \\rightarrow & \\text{d}\\\\\n1 & \\rightarrow & \\text{u}\\\\\n2 & \\rightarrow & \\text{d}\\\\\n\\end{Bmatrix}\\]\nThis definition admits of sequences with gaps in the natural numbers.\n\\[\\begin{Bmatrix}\n0 & \\rightarrow & \\text{d}\\\\\n1 & \\rightarrow & \\text{u}\\\\\n205 & \\rightarrow & \\text{d}\\\\\n\\end{Bmatrix}\\]\nWe will generally assume that our sequences map the first \\(|S^{-1}(\\Sigma)|\\) natural numbers to \\(\\Sigma\\), because these “gappy” sequences can always be mapped to one where \\(S^{-1}(\\Sigma) = \\{0, ..., |S^{-1}(\\Sigma)|-1\\}\\). But there are cases—one discussed below—where we don’t necessarily want our sequences to start at 0.\nWe often denote the \\(i^{th}\\) element of a sequence using a subscript rather than function notation.\n\\[s_i \\equiv S(i)\\]\nFunctions can be represented as sets of pairs and therefore sequences can be too.\n\\[S = \\{\\langle 0, \\text{d} \\rangle, \\langle 1, \\text{u} \\rangle, \\langle 2, \\text{d} \\rangle\\} \\subseteq \\mathbb{N}\\times \\Sigma\\]\nWe can also represent sequences as elements of \\(\\Sigma^{|S^{-1}(\\Sigma)|}\\).\n\\[\\mathrm{func2tuple}(S, i) = \\begin{cases}\\langle S(i), \\mathrm{func2tuple}(S, i+1) \\rangle & \\text{if } i + 1 \\in S^{-1}(\\Sigma) \\\\\nS(i) & \\text{otherwise}\n           \\end{cases}\\]\nAnd as we saw, we can always flatten these elements of \\(\\Sigma^{|S^{-1}(\\Sigma)|}\\) to \\(|S^{-1}(\\Sigma)|\\)-tuples. Because of this, we (often) implement sequences in Python using lists and tuples, though we can implement them using dicts as well.\n\nx = [\"d\", \"u\", \"d\"]\ny = (\"d\", \"u\", \"d\")\nz = {0: \"d\",\n     1: \"u\",\n     2: \"d\"}"
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/strings.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/strings.html",
    "title": "Strings",
    "section": "",
    "text": "We build strings from some alphabet/lexicon \\(\\Sigma\\).\nStrings of length \\(N\\) are given by \\(\\Sigma^N\\) and the Kleene closure of \\(\\Sigma\\)—notated \\(\\Sigma^*\\)—gives us the set of all such sets.\n\\[\\Sigma^* \\equiv \\bigcup_{i\\in\\mathbb{N}} \\Sigma^i\\]\nAs an infinite set, we need to implement \\(\\Sigma^*\\) using a generator. We’ll start by defining a function that produces \\(\\Sigma^i\\).\n\nfrom itertools import product\n\ndef sigma_i(sigma: set[str], i: int) -&gt; product:\n    sigma_repeated = [sigma]*i\n    return product(*sigma_repeated)\n\nsigma = [\"ɹ\", \"d\", \"u\"]\n\nfor s in sigma_i(sigma, 3):\n    print(''.join(s))\n\nɹɹɹ\nɹɹd\nɹɹu\nɹdɹ\nɹdd\nɹdu\nɹuɹ\nɹud\nɹuu\ndɹɹ\ndɹd\ndɹu\nddɹ\nddd\nddu\nduɹ\ndud\nduu\nuɹɹ\nuɹd\nuɹu\nudɹ\nudd\nudu\nuuɹ\nuud\nuuu\n\n\nWe can then define \\(\\Sigma^*\\) using a generator comprehension.\n\n\nGenerator for natural numbers\ndef natural_numbers() -&gt; int:\n    \"\"\"Initialize a generator for the natural numbers\"\"\"\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n\n\nsigma_star = (''.join(s) \n              for i in natural_numbers() \n              for s in sigma_n(sigma, i))\n\nfor s in sigma_star:\n    if len(s) &lt; 4:\n        print(s)\n    else:\n        break\n\n\nɹ\nd\nu\nɹɹ\nɹd\nɹu\ndɹ\ndd\ndu\nuɹ\nud\nuu\nɹɹɹ\nɹɹd\nɹɹu\nɹdɹ\nɹdd\nɹdu\nɹuɹ\nɹud\nɹuu\ndɹɹ\ndɹd\ndɹu\nddɹ\nddd\nddu\nduɹ\ndud\nduu\nuɹɹ\nuɹd\nuɹu\nudɹ\nudd\nudu\nuuɹ\nuud\nuuu\n\n\n\n\n\n\n\n\nQuestion\n\n\n\nHow many strings are there in \\(\\Sigma^*\\) (assuming that \\(\\Sigma\\) is finite)? That is, what is \\(|\\Sigma^*| = |\\bigcup_{i\\in\\mathbb{N}} \\Sigma^i|\\)?\n\n\nIt must be at least at least as big as \\(|\\mathbb{N}|\\), since we have a nonempty set \\(\\Sigma^i\\) corresponding to each natural number \\(i\\).\nSurprisingly, \\(|\\Sigma^*|\\) turns out to be exactly as big as \\(|\\mathbb{N}|\\), which we can show by demonstrating that there is a bijection from \\(\\mathbb{N}\\) to \\(\\Sigma^*\\): for each \\(i \\in \\mathbb{N}\\) we can map \\(i\\) to a unique string in \\(\\Sigma^*\\) and for each string in \\(\\Sigma^*\\), we can map that string to a unique natural number \\(i\\). This bijection is a total function from \\(\\mathbb{N}\\) to \\(\\Sigma^*\\).\nThe trick is to notice that each \\(\\Sigma^i\\) is itself of finite cardinality. This means that we can always break off a chunk of the natural numbers to allocate for building a sequence of all strings in \\(\\Sigma^i\\). The idea is then that we can then stitch those sequences together to get a sequence of all \\(\\Sigma^*\\) that never repeats strings.\nYou can get an idea for how this works by enumerating the strings we generate from sigma_star.\n\nN = natural_numbers()\n\nsigma_star = (''.join(s) \n              for i in N \n              for s in sigma_n(sigma, i))\n\nfor j, s in enumerate(sigma_star):\n    if len(s) &lt; 5:\n        print(j, s)\n    else:\n        break\n\n0 \n1 ɹ\n2 d\n3 u\n4 ɹɹ\n5 ɹd\n6 ɹu\n7 dɹ\n8 dd\n9 du\n10 uɹ\n11 ud\n12 uu\n13 ɹɹɹ\n14 ɹɹd\n15 ɹɹu\n16 ɹdɹ\n17 ɹdd\n18 ɹdu\n19 ɹuɹ\n20 ɹud\n21 ɹuu\n22 dɹɹ\n23 dɹd\n24 dɹu\n25 ddɹ\n26 ddd\n27 ddu\n28 duɹ\n29 dud\n30 duu\n31 uɹɹ\n32 uɹd\n33 uɹu\n34 udɹ\n35 udd\n36 udu\n37 uuɹ\n38 uud\n39 uuu\n40 ɹɹɹɹ\n41 ɹɹɹd\n42 ɹɹɹu\n43 ɹɹdɹ\n44 ɹɹdd\n45 ɹɹdu\n46 ɹɹuɹ\n47 ɹɹud\n48 ɹɹuu\n49 ɹdɹɹ\n50 ɹdɹd\n51 ɹdɹu\n52 ɹddɹ\n53 ɹddd\n54 ɹddu\n55 ɹduɹ\n56 ɹdud\n57 ɹduu\n58 ɹuɹɹ\n59 ɹuɹd\n60 ɹuɹu\n61 ɹudɹ\n62 ɹudd\n63 ɹudu\n64 ɹuuɹ\n65 ɹuud\n66 ɹuuu\n67 dɹɹɹ\n68 dɹɹd\n69 dɹɹu\n70 dɹdɹ\n71 dɹdd\n72 dɹdu\n73 dɹuɹ\n74 dɹud\n75 dɹuu\n76 ddɹɹ\n77 ddɹd\n78 ddɹu\n79 dddɹ\n80 dddd\n81 dddu\n82 dduɹ\n83 ddud\n84 dduu\n85 duɹɹ\n86 duɹd\n87 duɹu\n88 dudɹ\n89 dudd\n90 dudu\n91 duuɹ\n92 duud\n93 duuu\n94 uɹɹɹ\n95 uɹɹd\n96 uɹɹu\n97 uɹdɹ\n98 uɹdd\n99 uɹdu\n100 uɹuɹ\n101 uɹud\n102 uɹuu\n103 udɹɹ\n104 udɹd\n105 udɹu\n106 uddɹ\n107 uddd\n108 uddu\n109 uduɹ\n110 udud\n111 uduu\n112 uuɹɹ\n113 uuɹd\n114 uuɹu\n115 uudɹ\n116 uudd\n117 uudu\n118 uuuɹ\n119 uuud\n120 uuuu\n\n\nSo for instance, in the above, you can think of the enumeration as setting aside \\(\\{1, 2, 3\\}\\) for the strings of length 1, \\(\\{4, \\ldots, 12\\}\\) for the strings of length 2, and so on.\nMore formally, we’ll set aside a chunk of natural numbers \\(N_i \\equiv \\left\\{\\left[\\max N_{i-1}\\right] + 1, ..., [\\max N_{i-1}] + |\\Sigma^i| + 1\\right\\}\\)–with \\(N_0 \\equiv \\{|\\Sigma^0| - 1\\} = \\{0\\}\\)–for each \\(\\Sigma^i\\). Note that this definition ensures that \\(N_i \\cap N_j = \\emptyset\\) for all \\(i \\neq j\\). That is, the chunk of natural numbers \\(N_i\\) we set aside for \\(\\Sigma^i\\) is disjoint from the chunk of natural numbers \\(N_j\\) we set aside for \\(\\Sigma^j\\) when \\(i \\neq j\\), and \\(|N_i| = |\\Sigma^i|\\).\nWe then use \\(N_i\\) to construct a sequence \\(S_i: N_i \\rightarrow \\Sigma ^i\\). Finally, we can stitch those sequences together by defining \\(S_*: \\mathbb{N} \\rightarrow \\Sigma^*\\) in terms of those \\(S_i\\) along with a function \\(k: \\mathbb{N} \\rightarrow \\mathbb{N}\\) that satisfies \\(k^{-1}(\\{i\\}) = N_i\\) for all \\(i \\in \\mathbb{N}\\)–i.e. the preimage of \\(\\{i\\}\\) under \\(k\\) is \\(N_i\\).\n\\[S_*(n) = S_{k(n)}(n)\\]\nOne question you might have is how do we know how to build the sequence \\(S_i\\) for an arbitrary set of strings \\(\\Sigma^i\\) of length \\(i\\). As long as the alphabet/lexicon \\(\\Sigma\\) is finite, we can do this by imposing an order on \\(\\Sigma\\) itself, then use that to order sort the elements of \\(\\Sigma^i\\) in lexicographic order."
  },
  {
    "objectID": "formal-and-practical-preliminaries/elementary-mathematical-concepts/languages.html",
    "href": "formal-and-practical-preliminaries/elementary-mathematical-concepts/languages.html",
    "title": "Languages",
    "section": "",
    "text": "We build languages from some alphabet/lexicon \\(\\Sigma\\) by defining a language \\(L\\) on \\(\\Sigma\\) as a subset of the strings \\(\\Sigma^*\\) that can be built from \\(\\Sigma\\).\n\\[L \\subseteq \\Sigma^*\\]\nThis definition implies that the set of all languages on \\(\\Sigma\\) is the power set of \\(\\Sigma^*\\)\n\\[L \\in 2^{\\Sigma^*}\\]\nThis terminology arises from the fact that, if \\(\\Sigma\\) were, say, all of the phonemes of English, at least one element of \\(2^{\\Sigma^*}\\) would be all and only the words of English (or at least one persons English idiolect). If \\(\\Sigma\\) were all English words (and assuming that grammaticality is a coherent binary concept), at least one element of \\(2^{\\Sigma^*}\\) would be all the grammatical sentences of English (or at least one persons English idiolect). Of course, many of the sets in \\(2^{\\Sigma^*}\\) won’t look anything like English or any other languages, and a big part of this class is going to be figuring out how to find subsets of \\(2^{\\Sigma^*}\\) that look like possible languages.\nTo generate the set of all languages, we can compose our generator for \\(\\Sigma^*\\) with our power set generator.\n\n\nfrom itertools import product\n\n#| code-fold: true\n#| code-summary: Generator for natural numbers\n\ndef natural_numbers() -&gt; int:\n    \"\"\"Initialize a generator for the natural numbers\"\"\"\n    i = 0\n    while True:\n        yield i\n        i += 1\n\n\n\nGenerator for power set\nfrom typing import TypeVar\nfrom collections.abc import Iterable\n\nT = TypeVar(\"T\")\n\nemptyset = frozenset()\n\ndef powerset(iterable: Iterable[T]) -&gt; set[T]:\n    yield emptyset\n\n    seen = {emptyset}\n\n    for r in iterable:\n        new = {s | frozenset({r}) for s in seen}\n        for n in new:\n            yield n\n            seen.add(n)\n\n\n\n\nGenerator for Σ*\ndef sigma_i(sigma: set[str], i: int) -&gt; product:\n    sigma_repeated = [sigma]*i\n    return product(*sigma_repeated)\n\ndef sigma_star(sigma: set[str]) -&gt; str:\n    for i in natural_numbers():\n        for s in sigma_i(sigma, i):\n            yield ''.join(s)\n\n\n\nfrom collections.abc import Generator\n\nenglish_phonemes = {\"ɑ\", \"æ\", \"ə\", \"ʌ\", \"ɔ\", \"aʊ\", \"aɪ\", \"b\", \"tʃ\", \"d\", \"ð\", \"ɛ\", \n                    \"ɝ\", \"eɪ\", \"f\", \"g\", \"h\", \"ɪ\", \"i\", \"dʒ\", \"k\", \"l\", \"m\", \n                    \"n\", \"ŋ\", \"oʊ\", \"ɔɪ\", \"p\", \"ɹ\", \"s\", \"ʃ\", \"t\", \"θ\", \"ʊ\", \n                    \"u\", \"v\", \"w\", \"j\", \"z\", \"ʒ\"}\n\nlanguages: Generator[frozenset[str]] = powerset(sigma_star(english_phonemes))\n\nfor i, l in enumerate(languages):\n    if not i % 100000:\n        print(l)\n    \n    if i &gt; 1000000:\n        break\n\nfrozenset()\nfrozenset({'', 'ʃ', 'ə', 'dʒ', 'k', 'ɹ', 'f', 'ʌ', 'ɔ', 'u'})\nfrozenset({'', 'ʃ', 'dʒ', 'w', 'k', 'b', 'z', 'f', 'ʌ', 'ð'})\nfrozenset({'', 'ʃ', 'dʒ', 'i', 'ɝ', 'z', 'f', 'ɔ', 'ɔɪ', 'tʃ'})\nfrozenset({'', 'i', 'k', 'ɝ', 'ɹ', 'f', 'h', 'ɔ', 'u', 'ɔɪ', 'tʃ'})\nfrozenset({'', 'i', 'ɔɪ', 'ɝ', 'ɹ', 'z', 'f', 'h', 'u', 'ð', 'tʃ'})\nfrozenset({'i', 'ð', 'ɔ', 'ə', 'k', 'eɪ', 'ɹ'})\nfrozenset({'ʃ', 'ə', 'dʒ', 'ɝ', 'f', 'ʌ', 'h', 'ɔ', 'u', 'ð', 'tʃ', 'eɪ'})\nfrozenset({'', 'ʃ', 'dʒ', 'i', 'ɹ', 'b', 'ʌ', 'h', 'ə', 'ɔɪ', 'eɪ'})\nfrozenset({'', 'ʃ', 'ə', 'w', 'ɝ', 'b', 'z', 'ʌ', 'u', 'ð', 'tʃ', 'eɪ'})\nfrozenset({'', 'ʃ', 'dʒ', 'i', 'k', 'ɹ', 'f', 'ʌ', 'ɔ', 'ə', 'ð', 'tʃ', 'eɪ'})\n\n\nNotice that even if we compute a million of these languages using our power set generator, we are still ending up with small languages consisting of only singleton strings.\n\n\n\n\n\n\nQuestion\n\n\n\nHow many languages are there in \\(2^{\\Sigma^*}\\)? That is, what is \\(|2^{\\Sigma^*}|\\)?\n\n\nBecause \\(\\{s\\} \\in 2^{\\Sigma^*}\\) for all \\(s \\in \\Sigma^*\\), we know that \\(2^{\\Sigma^*}\\) must be at least as large as \\(\\mathbb{\\Sigma}^*\\), which is the same size as \\(\\mathbb{N}\\). But unlike \\(\\mathbb{\\Sigma}^*\\) in comparison to \\(\\mathbb{N}\\), it turns out that \\(2^{\\Sigma^*}\\) is larger than either.\nThe trick to seeing this is to try to sequence all languages in \\(2^{\\Sigma^*}\\). Suppose there were such a sequence of languages \\(L_*: \\mathbb{N} \\rightarrow 2^{\\Sigma^*}\\) such that \\(L\\) is bijective: if \\(L_*(i) = L_*(j)\\), then \\(i = j\\) (\\(L_*\\) is injective), and \\(L_*(\\mathbb{N}) = 2^{\\Sigma^*}\\) (\\(L_*\\) is surjective). Given a sequence \\(S_*\\) on the strings, which we know we can construct, define a language \\(\\bar{L}\\) such that \\(S(i) \\in \\bar{L}\\) if \\(S(i) \\not\\in L_*(i)\\), otherwise \\(S(i) \\not\\in \\bar{L}\\) for all \\(i \\in \\mathbb{N}\\).\nBy definition, \\(\\bar{L} \\neq L_*(i)\\) for any \\(i\\), since \\(\\bar{L}\\) and \\(L_*(i)\\) differ on at least \\(S(i)\\): either \\(S(i) \\in \\bar{L}\\) and \\(S(i) \\not\\in L_*(i)\\) or \\(S(i) \\in L_*(i)\\) and \\(S(i) \\not\\in \\bar{L}\\). But if \\(\\bar{L} \\neq L_*(i)\\) for any \\(i\\), that means that \\(\\bar{L} \\not\\in L_*(\\mathbb{N})\\) and therefore \\(L_*\\) is not bijective (because it is not surjective): there will always be some \\(\\bar{L}\\) not in a particular sequence \\(L_*\\). Therefore, \\(|2^{\\Sigma^*}| &gt; |\\Sigma^*| = |\\mathbb{N}|\\)."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/index.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/index.html",
    "title": "Overview",
    "section": "",
    "text": "Reading\n\n\n\nJurafsky and Martin (2023, Ch. 2.1) on regular expressions.\n\n\nIn the last submodule, we defined the set of strings on an alphabet \\(\\Sigma\\) as:\n\\[\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\]\nWe then defined a language \\(L\\) on \\(\\Sigma\\) to be some (possibly improper) subset of the set of strings:\n\\[L \\subseteq \\Sigma^*\\]\nThe set of all languages on \\(\\Sigma\\) is thus the powerset \\(2^{\\Sigma^*}\\) of the set of strings \\(\\Sigma^*\\).\nIn this submodule, we’re going to discuss one way that we can compactly describe (a subset of) the languages in \\(2^{\\Sigma^*}\\): regular expressions.\nRegular expressions are foundational both from an applied and from a scientific perspective. From an applied perspective, they allow us to effectively query and modify text in a compact programmatic way. From a scientific perspective, they provide a way of stating phonological grammars in an efficient way.\n\n\n\n\n\n\nReferences\n\nJurafsky, Daniel, and James H. Martin. 2023. Speech and Language Processing."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/formal-definition.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/formal-definition.html",
    "title": "Formal Definition",
    "section": "",
    "text": "Regular expressions themselves are strings. We formally define these strings recursively, starting with an alphabet \\(\\Sigma\\), a symbol for the empty string \\(\\epsilon\\), and a symbol for the empty set \\(\\emptyset\\). Let’s assume the alphabet is the set of English phonemes:\n\\[\\Sigma \\equiv \\{\\text{ɑ, æ, ʌ, ɔ, aʊ, aɪ, b, tʃ, d, ð,} \\ldots\\}\\]\n\\(\\rho\\) is a regular expression if and only if:\n\n\\(\\rho \\in \\Sigma \\cup \\{\\epsilon, \\emptyset\\}\\)\n\\(\\rho\\) is \\((\\rho_1 \\cup \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\((\\rho_1 \\circ \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\(\\rho_1^*\\) for some regular expression \\(\\rho_1\\)\n\nThus, given the \\(\\Sigma\\) above, the following are all regular expressions.\n\næ\nb\n(æ \\(\\cup\\) b)\n(æ \\(\\circ\\) b\\(^*\\))\n(æ \\(\\circ\\) b)\\(^*\\)\n((æ \\(\\circ\\) b) \\(\\cup\\) b)\n(æ \\(\\circ\\) (b \\(\\cup\\) b))\n\nThe following are not regular expressions.\n\næ \\(\\cup\\) b\næ \\(\\circ\\) b\\(^*\\)\næ \\(\\circ\\) b \\(\\cup\\) b\n\nAnother way of saying this is that the regular expressions on \\(\\Sigma\\) are a language on \\(\\Sigma \\cup\\{\\epsilon, \\emptyset, \\cup, \\circ, (, ), *\\}\\). Note that, based on the recursive definition above, the set of regular expressions is infinite. Thus, the regular expressions on \\(\\Sigma\\) are the first infinite language we’ve described (besides \\(\\Sigma^*\\) itself).\nWe can generate all the regular expressions given an alphabet. Note that because the set of regular expressions is infinite, we need to use a generator.\n\nfrom collections.abc import Iterable\n\ndef regular_expressions(sigma: set[str]) -&gt; Iterable[str]:\n    old_regex_set = frozenset(sigma | {'∅', '𝜖'})\n    \n    for rho in old_regex_set:\n        yield rho\n    \n    while True:\n        new_regex_set = set(old_regex_set)\n            \n        for rho in old_regex_set:\n            elem = rho+'*'\n            new_regex_set |= {elem}\n            yield elem\n            \n        for rho1 in old_regex_set:\n            for rho2 in old_regex_set:\n                elem = '('+rho1+' ∪ '+rho2+')'\n                new_regex_set |= {elem}\n                yield elem\n                \n        for rho1 in old_regex_set:\n            for rho2 in old_regex_set:\n                elem = '('+rho1+' ∘ '+rho2+')'\n                new_regex_set |= {elem}\n                yield elem\n                \n        old_regex_set = frozenset(new_regex_set)\n\n\nenglish_phonemes = {\"ɑ\", \"æ\", \"ə\", \"ʌ\", \"ɔ\", \"aʊ\", \"aɪ\", \"b\", \"tʃ\", \"d\", \"ð\", \"ɛ\", \n                    \"ɝ\", \"eɪ\", \"f\", \"g\", \"h\", \"ɪ\", \"i\", \"dʒ\", \"k\", \"l\", \"m\", \n                    \"n\", \"ŋ\", \"oʊ\", \"ɔɪ\", \"p\", \"ɹ\", \"s\", \"ʃ\", \"t\", \"θ\", \"ʊ\", \n                    \"u\", \"v\", \"w\", \"j\", \"z\", \"ʒ\"}\n\nfor i, r in enumerate(regular_expressions(english_phonemes)):\n    print(r)\n    \n    if i &gt; 100:\n      break\n\nh\ns\nb\nɑ\nf\n∅\nθ\nn\nu\np\ndʒ\nɔɪ\nɛ\n𝜖\neɪ\nŋ\noʊ\nz\ng\nʃ\ntʃ\nl\nɔ\næ\nə\nɝ\nɪ\nʒ\ni\nʊ\nj\nd\nɹ\nð\nt\nʌ\nv\nm\nk\naʊ\nw\naɪ\nh*\ns*\nb*\nɑ*\nf*\n∅*\nθ*\nn*\nu*\np*\ndʒ*\nɔɪ*\nɛ*\n𝜖*\neɪ*\nŋ*\noʊ*\nz*\ng*\nʃ*\ntʃ*\nl*\nɔ*\næ*\nə*\nɝ*\nɪ*\nʒ*\ni*\nʊ*\nj*\nd*\nɹ*\nð*\nt*\nʌ*\nv*\nm*\nk*\naʊ*\nw*\naɪ*\n(h ∪ h)\n(h ∪ s)\n(h ∪ b)\n(h ∪ ɑ)\n(h ∪ f)\n(h ∪ ∅)\n(h ∪ θ)\n(h ∪ n)\n(h ∪ u)\n(h ∪ p)\n(h ∪ dʒ)\n(h ∪ ɔɪ)\n(h ∪ ɛ)\n(h ∪ 𝜖)\n(h ∪ eɪ)\n(h ∪ ŋ)\n(h ∪ oʊ)\n(h ∪ z)\n\n\nRegular expressions (so defined) evaluate to sets of strings on \\(\\Sigma\\)–i.e. languages on \\(\\Sigma\\). Another way of thinking about this is that a regular expression on \\(\\Sigma\\) describes a language on \\(\\Sigma\\).\nWe can define this evaluation procedure formally as a function \\(\\text{eval}: R(\\Sigma) \\rightarrow 2^{\\Sigma^*}\\), where \\(R(\\Sigma)\\) is the set of regular expressions on \\(\\Sigma\\).\n\\(\\text{eval}(\\rho) = \\begin{cases}\\{\\} & \\text{if } \\rho = \\emptyset \\\\\\{\\_\\} & \\text{if } \\rho = \\epsilon \\\\ \\{\\rho\\} & \\text{if } \\rho \\in \\Sigma\\\\ \\text{eval}(\\rho_1) \\times \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\circ \\rho_2) \\\\ \\text{eval}(\\rho_1) \\cup \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\cup \\rho_2)\\\\ \\bigcup_{i = 0}^\\infty \\text{eval}(\\rho_1)^i & \\text{if } \\rho = \\rho_1^*\\\\ \\end{cases}\\)\nSo first, we’re going to want to have access to the regular expressions’ structure for the purposes of evaluating them. We could do this by using pyparsing or some hand-built parser to reconstruct the structure of each expression, but we may as well just build retain the structure while generating the expression.\n\ndef regular_expressions_structured(sigma):\n    old_regex_set = frozenset(sigma | {'∅', '𝜖'})\n    \n    for rho in old_regex_set:\n        yield rho\n    \n    while True:\n        new_regex_set = set(old_regex_set)\n            \n        for rho in old_regex_set:\n            elem = (rho, '*')\n            new_regex_set |= {elem}\n            yield elem\n            \n        for rho1 in old_regex_set:\n            for rho2 in old_regex_set:\n                elem = (rho1, '∪', rho2)\n                new_regex_set |= {elem}\n                yield elem\n                \n        for rho1 in old_regex_set:\n            for rho2 in old_regex_set:\n                elem = (rho1, '∘', rho2)\n                new_regex_set |= {elem}\n                yield elem\n                \n        old_regex_set = frozenset(new_regex_set)\n        \n\n\nfor i, r in enumerate(regular_expressions_structured({'ə', 'm'})):\n    print(r)\n    \n    if i &gt; 100:\n        break\n\nm\n∅\nə\n𝜖\n('m', '*')\n('∅', '*')\n('ə', '*')\n('𝜖', '*')\n('m', '∪', 'm')\n('m', '∪', '∅')\n('m', '∪', 'ə')\n('m', '∪', '𝜖')\n('∅', '∪', 'm')\n('∅', '∪', '∅')\n('∅', '∪', 'ə')\n('∅', '∪', '𝜖')\n('ə', '∪', 'm')\n('ə', '∪', '∅')\n('ə', '∪', 'ə')\n('ə', '∪', '𝜖')\n('𝜖', '∪', 'm')\n('𝜖', '∪', '∅')\n('𝜖', '∪', 'ə')\n('𝜖', '∪', '𝜖')\n('m', '∘', 'm')\n('m', '∘', '∅')\n('m', '∘', 'ə')\n('m', '∘', '𝜖')\n('∅', '∘', 'm')\n('∅', '∘', '∅')\n('∅', '∘', 'ə')\n('∅', '∘', '𝜖')\n('ə', '∘', 'm')\n('ə', '∘', '∅')\n('ə', '∘', 'ə')\n('ə', '∘', '𝜖')\n('𝜖', '∘', 'm')\n('𝜖', '∘', '∅')\n('𝜖', '∘', 'ə')\n('𝜖', '∘', '𝜖')\n(('𝜖', '∘', 'm'), '*')\n(('∅', '∘', 'ə'), '*')\n(('∅', '∪', 'm'), '*')\n(('ə', '∘', 'ə'), '*')\n('∅', '*')\n(('m', '∪', '𝜖'), '*')\n(('m', '∪', '∅'), '*')\n(('m', '∪', 'm'), '*')\n(('𝜖', '∪', 'ə'), '*')\n(('m', '∘', 'ə'), '*')\n(('∅', '∪', '∅'), '*')\n(('∅', '∘', '𝜖'), '*')\n(('∅', '∘', '∅'), '*')\n(('∅', '∘', 'm'), '*')\n('𝜖', '*')\n(('ə', '∘', '∅'), '*')\n(('m', '*'), '*')\n(('ə', '∘', '𝜖'), '*')\n(('ə', '*'), '*')\n(('ə', '∘', 'm'), '*')\n(('𝜖', '∪', '𝜖'), '*')\n(('∅', '∪', 'ə'), '*')\n(('ə', '∪', 'ə'), '*')\n(('𝜖', '∪', '∅'), '*')\n(('𝜖', '∘', 'ə'), '*')\n(('𝜖', '∪', 'm'), '*')\n(('𝜖', '*'), '*')\n('ə', '*')\n(('m', '∘', '𝜖'), '*')\n(('∅', '*'), '*')\n(('m', '∘', '∅'), '*')\n(('m', '∘', 'm'), '*')\n(('m', '∪', 'ə'), '*')\n('m', '*')\n(('ə', '∪', '𝜖'), '*')\n(('ə', '∪', '∅'), '*')\n(('𝜖', '∘', '𝜖'), '*')\n(('𝜖', '∘', '∅'), '*')\n(('ə', '∪', 'm'), '*')\n(('∅', '∪', '𝜖'), '*')\n(('𝜖', '∘', 'm'), '∪', ('𝜖', '∘', 'm'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∘', 'ə'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∪', 'm'))\n(('𝜖', '∘', 'm'), '∪', ('ə', '∘', 'ə'))\n(('𝜖', '∘', 'm'), '∪', '∅')\n(('𝜖', '∘', 'm'), '∪', ('m', '∪', '𝜖'))\n(('𝜖', '∘', 'm'), '∪', ('m', '∪', '∅'))\n(('𝜖', '∘', 'm'), '∪', ('m', '∪', 'm'))\n(('𝜖', '∘', 'm'), '∪', ('𝜖', '∪', 'ə'))\n(('𝜖', '∘', 'm'), '∪', ('m', '∘', 'ə'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∪', '∅'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∘', '𝜖'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∘', '∅'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∘', 'm'))\n(('𝜖', '∘', 'm'), '∪', '𝜖')\n(('𝜖', '∘', 'm'), '∪', ('ə', '∘', '∅'))\n(('𝜖', '∘', 'm'), '∪', ('m', '*'))\n(('𝜖', '∘', 'm'), '∪', ('ə', '∘', '𝜖'))\n(('𝜖', '∘', 'm'), '∪', ('ə', '*'))\n(('𝜖', '∘', 'm'), '∪', ('ə', '∘', 'm'))\n(('𝜖', '∘', 'm'), '∪', ('𝜖', '∪', '𝜖'))\n(('𝜖', '∘', 'm'), '∪', ('∅', '∪', 'ə'))\n\n\nNext, we can write a function that recursively evaluates a regular expression.\n\ndef evaluate_regular_expression(regex):\n    if regex == '∅':\n        return\n    \n    elif regex == '𝜖':\n        yield ''\n    \n    elif isinstance(regex, str):\n        yield regex\n    \n    elif regex[1] == '*':\n        i = 0\n        while True:\n            for s in evaluate_regular_expression(regex[0]):\n                yield s*i\n            \n            i += 1\n            \n    elif regex[1] == '∪':\n        for s1 in evaluate_regular_expression(regex[0]):\n            yield s1\n            \n        for s2 in evaluate_regular_expression(regex[2]):\n            yield s2\n            \n    elif regex[1] == '∘':\n        for s1 in evaluate_regular_expression(regex[0]):\n            for s2 in evaluate_regular_expression(regex[2]):\n                yield s1 + s2\n\n\nfor s in evaluate_regular_expression('ə'):\n    print(s)\n\nə\n\n\n\nfor s in evaluate_regular_expression(('ə', '∪', 'm')):\n    print(s)\n\nə\nm\n\n\n\nfor s in evaluate_regular_expression(('ə', '∘', 'm')):\n    print(s)\n\nəm\n\n\n\nfor s in evaluate_regular_expression(('ə', '∘', ('m', '∪', 'g'))):\n    print(s)\n\nəm\nəg\n\n\n\nfor i, s in enumerate(evaluate_regular_expression(('ə', '*'))):\n    print(s)\n    \n    if i &gt; 10:\n        break\n\n\nə\nəə\nəəə\nəəəə\nəəəəə\nəəəəəə\nəəəəəəə\nəəəəəəəə\nəəəəəəəəə\nəəəəəəəəəə\nəəəəəəəəəəə\n\n\n\nfor i, s in enumerate(evaluate_regular_expression((('ə', '∪', 'm'), '*'))):\n    print(s)\n    \n    if i &gt; 10:\n        break\n\n\n\nə\nm\nəə\nmm\nəəə\nmmm\nəəəə\nmmmm\nəəəəə\nmmmmm"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/loading-a-lexicon.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/loading-a-lexicon.html",
    "title": "Loading a Lexicon",
    "section": "",
    "text": "One use case for regular expressions in the context of computational linguistics is querying a lexicon. To look at this use case, we will use the lexicon of word forms found in the CMU Pronouncing Dictionary.\n\n!wget https://raw.githubusercontent.com/Alexir/CMUdict/master/cmudict-0.7b\n\n--2024-01-31 12:13:47--  https://raw.githubusercontent.com/Alexir/CMUdict/master/cmudict-0.7b\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3865710 (3.7M) [text/plain]\nSaving to: ‘cmudict-0.7b.1’\n\ncmudict-0.7b.1      100%[===================&gt;]   3.69M  14.0MB/s    in 0.3s    \n\n2024-01-31 12:13:48 (14.0 MB/s) - ‘cmudict-0.7b.1’ saved [3865710/3865710]\n\n\n\n\n!head -n 200 cmudict-0.7b\n\n;;; # CMUdict  --  Major Version: 0.07\n;;; \n;;; # $HeadURL: https://svn.code.sf.net/p/cmusphinx/code/branches/cmudict/cmudict-0.7b $\n;;; # $Date:: 2015-07-15 12:34:30 -0400 (Wed, 15 Jul 2015)      $:\n;;; # $Id:: cmudict-0.7b 13083 2015-07-15 16:34:30Z air         $:\n;;; # $Rev:: 13083                                              $: \n;;; # $Author:: air                                             $:\n;;;\n;;; #\n;;; # ========================================================================\n;;; # Copyright (C) 1993-2015 Carnegie Mellon University. All rights reserved.\n;;; #\n;;; # Redistribution and use in source and binary forms, with or without\n;;; # modification, are permitted provided that the following conditions\n;;; # are met:\n;;; #\n;;; # 1. Redistributions of source code must retain the above copyright\n;;; #    notice, this list of conditions and the following disclaimer.\n;;; #    The contents of this file are deemed to be source code.\n;;; #\n;;; # 2. Redistributions in binary form must reproduce the above copyright\n;;; #    notice, this list of conditions and the following disclaimer in\n;;; #    the documentation and/or other materials provided with the\n;;; #    distribution.\n;;; #\n;;; # This work was supported in part by funding from the Defense Advanced\n;;; # Research Projects Agency, the Office of Naval Research and the National\n;;; # Science Foundation of the United States of America, and by member\n;;; # companies of the Carnegie Mellon Sphinx Speech Consortium. We acknowledge\n;;; # the contributions of many volunteers to the expansion and improvement of\n;;; # this dictionary.\n;;; #\n;;; # THIS SOFTWARE IS PROVIDED BY CARNEGIE MELLON UNIVERSITY ``AS IS'' AND\n;;; # ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n;;; # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n;;; # PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY\n;;; # NOR ITS EMPLOYEES BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n;;; # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n;;; # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n;;; # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n;;; # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n;;; # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n;;; # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n;;; #\n;;; # ========================================================================\n;;; #\n;;;\n;;;  NOTES  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n;;; \n;;;  [20080401] (air)  New dict file format introduced \n;;;   - comments (like this section) are allowed\n;;;   - file name is major version; vers/rev information is now in the header\n;;;\n;;; \n;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n;;; \n!EXCLAMATION-POINT  EH2 K S K L AH0 M EY1 SH AH0 N P OY2 N T\n\"CLOSE-QUOTE  K L OW1 Z K W OW1 T\n\"DOUBLE-QUOTE  D AH1 B AH0 L K W OW1 T\n\"END-OF-QUOTE  EH1 N D AH0 V K W OW1 T\n\"END-QUOTE  EH1 N D K W OW1 T\n\"IN-QUOTES  IH1 N K W OW1 T S\n\"QUOTE  K W OW1 T\n\"UNQUOTE  AH1 N K W OW1 T\n#HASH-MARK  HH AE1 M AA2 R K\n#POUND-SIGN  P AW1 N D S AY2 N\n#SHARP-SIGN  SH AA1 R P S AY2 N\n%PERCENT  P ER0 S EH1 N T\n&AMPERSAND  AE1 M P ER0 S AE2 N D\n'ALLO  AA2 L OW1\n'APOSTROPHE  AH0 P AA1 S T R AH0 F IY0\n'BOUT  B AW1 T\n'CAUSE  K AH0 Z\n'COURSE  K AO1 R S\n'CUSE  K Y UW1 Z\n'EM  AH0 M\n'END-INNER-QUOTE  EH1 N D IH1 N ER0 K W OW1 T\n'END-QUOTE  EH1 N D K W OW1 T\n'FRISCO  F R IH1 S K OW0\n'GAIN  G EH1 N\n'INNER-QUOTE  IH1 N ER0 K W OW1 T\n'KAY  K EY1\n'M  AH0 M\n'N  AH0 N\n'QUOTE  K W OW1 T\n'ROUND  R AW1 N D\n'S  EH1 S\n'SINGLE-QUOTE  S IH1 NG G AH0 L K W OW1 T\n'TIL  T IH1 L\n'TIS  T IH1 Z\n'TWAS  T W AH1 Z\n(BEGIN-PARENS  B IH0 G IH1 N P ER0 EH1 N Z\n(IN-PARENTHESES  IH1 N P ER0 EH1 N TH AH0 S IY2 Z\n(LEFT-PAREN  L EH1 F T P ER0 EH1 N\n(OPEN-PARENTHESES  OW1 P AH0 N P ER0 EH1 N TH AH0 S IY2 Z\n(PAREN  P ER0 EH1 N\n(PARENS  P ER0 EH1 N Z\n(PARENTHESES  P ER0 EH1 N TH AH0 S IY2 Z\n)CLOSE-PAREN  K L OW1 Z P ER0 EH1 N\n)CLOSE-PARENTHESES  K L OW1 Z P ER0 EH1 N TH AH0 S IY2 Z\n)END-PAREN  EH1 N D P ER0 EH1 N\n)END-PARENS  EH1 N D P ER0 EH1 N Z\n)END-PARENTHESES  EH1 N D P ER0 EH1 N TH AH0 S IY2 Z\n)END-THE-PAREN  EH1 N D DH AH0 P ER0 EH1 N\n)PAREN  P ER0 EH1 N\n)PARENS  P ER0 EH1 N Z\n)RIGHT-PAREN  R AY1 T P EH2 R AH0 N\n)UN-PARENTHESES  AH1 N P ER0 EH1 N TH AH0 S IY1 Z\n+PLUS  P L UH1 S\n,COMMA  K AA1 M AH0\n--DASH  D AE1 SH\n-DASH  D AE1 SH\n-HYPHEN  HH AY1 F AH0 N\n...ELLIPSIS  IH2 L IH1 P S IH0 S\n.DECIMAL  D EH1 S AH0 M AH0 L\n.DOT  D AA1 T\n.FULL-STOP  F UH1 L S T AA1 P\n.PERIOD  P IH1 R IY0 AH0 D\n.POINT  P OY1 N T\n/SLASH  S L AE1 SH\n3-D  TH R IY1 D IY2\n3D  TH R IY1 D IY2\n:COLON  K OW1 L AH0 N\n;SEMI-COLON  S EH1 M IY0 K OW1 L AH0 N\n;SEMI-COLON(1)  S EH1 M IH0 K OW2 L AH0 N\n?QUESTION-MARK  K W EH1 S CH AH0 N M AA1 R K\nA  AH0\nA(1)  EY1\nA'S  EY1 Z\nA.  EY1\nA.'S  EY1 Z\nA.D.  EY2 D IY1\nA.M.  EY2 EH1 M\nA.S  EY1 Z\nA42128  EY1 F AO1 R T UW1 W AH1 N T UW1 EY1 T\nAA  EY2 EY1\nAAA  T R IH2 P AH0 L EY1\nAAAI  T R IH2 P AH0 L EY2 AY1\nAABERG  AA1 B ER0 G\nAACHEN  AA1 K AH0 N\nAACHENER  AA1 K AH0 N ER0\nAAH  AA1\nAAKER  AA1 K ER0\nAALIYAH  AA2 L IY1 AA2\nAALSETH  AA1 L S EH0 TH\nAAMODT  AA1 M AH0 T\nAANCOR  AA1 N K AO2 R\nAARDEMA  AA0 R D EH1 M AH0\nAARDVARK  AA1 R D V AA2 R K\nAARDVARKS  AA1 R D V AA2 R K S\nAARGH  AA1 R G\nAARHUS  AA2 HH UW1 S\nAARON  EH1 R AH0 N\nAARON'S  EH1 R AH0 N Z\nAARONS  EH1 R AH0 N Z\nAARONSON  EH1 R AH0 N S AH0 N\nAARONSON(1)  AA1 R AH0 N S AH0 N\nAARONSON'S  EH1 R AH0 N S AH0 N Z\nAARONSON'S(1)  AA1 R AH0 N S AH0 N Z\nAARTI  AA1 R T IY2\nAASE  AA1 S\nAASEN  AA1 S AH0 N\nAB  AE1 B\nABA  EY2 B IY2 EY1\nABABA  AH0 B AA1 B AH0\nABABA(1)  AA1 B AH0 B AH0\nABACHA  AE1 B AH0 K AH0\nABACK  AH0 B AE1 K\nABACO  AE1 B AH0 K OW2\nABACUS  AE1 B AH0 K AH0 S\nABAD  AH0 B AA1 D\nABADAKA  AH0 B AE1 D AH0 K AH0\nABADI  AH0 B AE1 D IY0\nABADIE  AH0 B AE1 D IY0\nABAIR  AH0 B EH1 R\nABALKIN  AH0 B AA1 L K IH0 N\nABALONE  AE2 B AH0 L OW1 N IY0\nABALONES  AE2 B AH0 L OW1 N IY0 Z\nABALOS  AA0 B AA1 L OW0 Z\nABANDON  AH0 B AE1 N D AH0 N\nABANDONED  AH0 B AE1 N D AH0 N D\nABANDONING  AH0 B AE1 N D AH0 N IH0 NG\nABANDONMENT  AH0 B AE1 N D AH0 N M AH0 N T\nABANDONMENTS  AH0 B AE1 N D AH0 N M AH0 N T S\nABANDONS  AH0 B AE1 N D AH0 N Z\nABANTO  AH0 B AE1 N T OW0\nABARCA  AH0 B AA1 R K AH0\nABARE  AA0 B AA1 R IY0\nABASCAL  AE1 B AH0 S K AH0 L\nABASH  AH0 B AE1 SH\nABASHED  AH0 B AE1 SH T\nABASIA  AH0 B EY1 ZH Y AH0\nABATE  AH0 B EY1 T\nABATED  AH0 B EY1 T IH0 D\nABATEMENT  AH0 B EY1 T M AH0 N T\nABATEMENTS  AH0 B EY1 T M AH0 N T S\nABATES  AH0 B EY1 T S\nABATING  AH0 B EY1 T IH0 NG\nABATTOIR  AE2 B AH0 T W AA1 R\nABBA  AE1 B AH0\n\n\n\nwith open('cmudict-0.7b', encoding = \"ISO-8859-1\") as f:\n    cmudict = [l.split() for l in f if l[:3] != \";;;\"]\n    cmudict = {w[0].lower(): w[1:] for w in cmudict}\n    \ncmudict[\"abstraction\"]\n\n['AE0', 'B', 'S', 'T', 'R', 'AE1', 'K', 'SH', 'AH0', 'N']\n\n\nThis dictionary uses what’s known as the ARPABET and represents stress using numeric indicators: 0 for no stress, 1 for primary stress, and 2 for secondary stress. The ARPABET maps to more recognizable IPA representations in the following way.\n\narpabet_to_phoneme = {'AA': 'ɑ', \n                      'AE': 'æ', \n                      'AH': 'ə', \n                      'AO': 'ɔ', \n                      'AW': 'aʊ', \n                      'AY': 'aɪ', \n                      'B': 'b', \n                      'CH': 'tʃ', \n                      'D': 'd', \n                      'DH': 'ð', \n                      'EH': 'ɛ',\n                      'ER': 'ɝ', \n                      'EY': 'eɪ', \n                      'F': 'f', \n                      'G': 'g', \n                      'HH': 'h', \n                      'IH': 'ɪ', \n                      'IY': 'i', \n                      'JH': 'dʒ', \n                      'K': 'k', \n                      'L': 'l', \n                      'M': 'm', \n                      'N': 'n',\n                      'NG': 'ŋ', \n                      'OW': 'oʊ', \n                      'OY': 'ɔɪ', \n                      'P': 'p', \n                      'R': 'ɹ', \n                      'S': 's', \n                      'SH': 'ʃ', \n                      'T': 't', \n                      'TH': 'θ', \n                      'UH': 'ʊ', \n                      'UW': 'u', \n                      'V': 'v',\n                      'W': 'w', \n                      'Y': 'j', \n                      'Z': 'z', \n                      'ZH': 'ʒ'}\n\nWe’ll use this mapping to construct the IPA representation from the ARPABET representation.\n\nimport re\n\nentries: dict[str, list[str]] = {\n    w: [arpabet_to_phoneme[''.join(re.findall('[A-z]', phoneme))]\n        for phoneme in transcription]\n        for w, transcription in cmudict.items()\n        if len({c for c in w})&gt;1\n        if len(transcription)&gt;1 \n        if not re.findall('[^A-z]', w[0])\n}\n\nFor instance, the IPA representation for the word abstraction can be accessed in the following way.\n\nentries[\"abstraction\"]\n\n['æ', 'b', 's', 't', 'ɹ', 'æ', 'k', 'ʃ', 'ʌ', 'n']\n\n\n\n\nDump IPA representation to file\nwith open(\"cmudict-ipa\", \"w\") as f:\n    for w, ipa in entries.items():\n        ipa = \" \".join(ipa)\n        f.write(f\"{w},{ipa}\\n\")"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/basic-matching.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/basic-matching.html",
    "title": "Basic Matching",
    "section": "",
    "text": "We mainly use regular expressions for searching, extracting from, and modifying strings. Each such use is underwritten by a string being in the set of strings that a regular expression evaluates to.\nOne core use of regular expressions is basic matching, which checks whether a string falls into the language that a regular expression evaluates to. We can describe matching formally in a couple ways. One is to view it as a function from a regular expression and a string to a boolean.\n\\[\\text{match}: R(\\Sigma)\\;\\times\\;\\Sigma^*  \\rightarrow \\{\\top, \\bot\\}\\]\n\\[\\text{match}(\\rho, \\sigma) = \\begin{cases}\\top & \\text{if } \\sigma \\in \\text{eval}(\\rho) \\\\\n\\bot & \\text{otherwise}\\end{cases}\\]\nWe can compute this function with fullmatch from the re module.\n\nimport re\n\nregex_æ = 'æ'\nstring_æ = 'æ'\n\nre.fullmatch(regex_æ, string_æ)\n\n&lt;re.Match object; span=(0, 1), match='æ'&gt;\n\n\nAnother is to view it as a function from strings to booleans parameterized by a regular expression \\(\\rho\\).\n\\[\\text{match}_\\rho: \\Sigma^*  \\rightarrow \\{\\top, \\bot\\}\\]\n\\[\\text{match}_\\rho(\\sigma) = \\begin{cases}\\top & \\text{if } \\sigma \\in \\text{eval}(\\rho) \\\\\n\\bot & \\text{otherwise}\\end{cases}\\]\nI’m pointing out this seemingly trivial distinction because it will be important when we discuss the concept of a grammar recognizing a string, where we will define a recognition algorithm for a particular class of grammars that we parameterize by grammars in that class.\nWe can compute this function by compiling the regular expression to a Pattern object…\n\nmatch_æ = re.compile(regex_æ)\n\ntype(match_æ)\n\nre.Pattern\n\n\n…then call the fullmatch instance method on this object.\n\nmatch_æ.fullmatch(string_æ)\n\n&lt;re.Match object; span=(0, 1), match='æ'&gt;\n\n\nBoth return a Match object (so its not a perfect implementation of the formal definition).\n\ntype(re.fullmatch(regex_æ, string_æ)), type(match_æ.fullmatch(string_æ))\n\n(re.Match, re.Match)\n\n\nBut if we try to cast this object to a bool we get True.\n\nbool(re.fullmatch(regex_æ, string_æ))\n\nTrue\n\n\nIn contrast, match returns None if there is no match.\n\nstring_æbstɹækʃən = 'æbstɹækʃən'\n\ntype(re.fullmatch(regex_æ, string_æbstɹækʃən))\n\nNoneType\n\n\nThis means that casting a non-match to a bool returns False.\n\nbool(re.fullmatch(regex_æ, string_æbstɹækʃən))\n\nFalse\n\n\nI point this out because you might expect that fullmatch would return a bool, but it doesn’t. This can be a problem if you’re not calling match within some if-else statement. Nonetheless, this is probably the most straightforward way to check whether a particularly pattern is matched by a string.\nYou might have noticed that there is another function match. It might seem like this functon would be simpler, but it has some odd behavior: it matches characters at the beginning of the string and ignores the rest of the string.\n\nre.match(regex_æ, string_æbstɹækʃən)\n\n&lt;re.Match object; span=(0, 1), match='æ'&gt;\n\n\nThe reason this is annoying is that this “beginning of the string” matching behavior is already built into regular expressions, which we’ll see in a second. It also means that re.match is not an implementation of the formal definition above."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html",
    "title": "Implementing the Regular Operations",
    "section": "",
    "text": "So far, we’ve seen only a trivial regular expression: one containing a single character æ, which evaluates to the language {æ} \\(\\in 2^{\\Sigma^*}\\). How do we represent other kinds of regular expressions?"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#concatenation",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#concatenation",
    "title": "Implementing the Regular Operations",
    "section": "Concatenation",
    "text": "Concatenation\nThe operation of concatenation, which we represented using \\(\\circ\\), is implicit in putting two characters next to each other. For instance, to represent the regular expression \\((\\text{æ} \\circ (\\text{b} \\circ (\\text{s} \\circ (\\text{t} \\circ (\\text{ɹ} \\circ (\\text{æ} \\circ (\\text{k} \\circ (\\text{ʃ} \\circ (\\text{ə} \\circ \\text{n})))))))))\\), we can simply write æbstɹækʃən.\n\n\nLoad IPA representation of CMU Pronouncing Dictionary\nwith open(\"cmudict-ipa\") as f:\n    entries: list[tuple[str, str]] = [\n        l.strip().split(\",\") for l in f\n    ]\n    entries: dict[str, list[str]] = {\n        w: ipa.split() for w, ipa in entries\n    }\n\n\n\nimport re\n\nregex_æbstɹækʃən = \"æbstɹækʃən\"\n\nstring_æbstɹækʃən = \"\".join(entries[\"abstraction\"])\n\nre.fullmatch(regex_æbstɹækʃən, string_æbstɹækʃən)\n\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#union",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#union",
    "title": "Implementing the Regular Operations",
    "section": "Union",
    "text": "Union\nIn contrast, to represent the regular expression \\(((\\text{æ} \\cup \\text{ə}) \\circ (\\text{b} \\circ (\\text{s} \\circ (\\text{t} \\circ (\\text{ɹ} \\circ ((\\text{æ} \\cup \\text{ə}) \\circ (\\text{k} \\circ (\\text{ʃ} \\circ (\\text{ə} \\circ \\text{n})))))))))\\), which evaluates to {æbstɹækʃən, əbstɹækʃən, æbstɹəkʃən, əbstɹəkʃən}, we either use []…\n\nregex_æəbstɹæəkʃən = \"[æə]bstɹ[æə]kʃən\"\n\nstring_əbstɹəkʃən = \"\".join(entries[\"obstruction\"])\nstring_æbstɹəkʃən = \"æbstɹəkʃən\"\nstring_əbstɹækʃən = \"əbstɹækʃən\"\n\n(re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹækʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹəkʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹækʃən), \n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹəkʃən))\n\n(&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='æbstɹəkʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='əbstɹækʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='əbstɹəkʃən'&gt;)\n\n\n…or an explicit |.\n\nregex_æəbstɹæəkʃən = \"(æ|ə)bstɹ(æ|ə)kʃən\"\n\n(re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹækʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹəkʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹækʃən), \n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹəkʃən))\n\n(&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='æbstɹəkʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='əbstɹækʃən'&gt;,\n &lt;re.Match object; span=(0, 10), match='əbstɹəkʃən'&gt;)\n\n\nNote that the () are important in the latter case!\n\nregex_æəbstɹæəkʃən = \"æ|əbstɹæ|əkʃən\"\n\n(re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹækʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_æbstɹəkʃən),\n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹækʃən), \n re.fullmatch(regex_æəbstɹæəkʃən, string_əbstɹəkʃən))\n\n(None, None, None, None)"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#kleene-star",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#kleene-star",
    "title": "Implementing the Regular Operations",
    "section": "Kleene star",
    "text": "Kleene star\nFinally, the Kleene star works the way you would expect.\n\nregex_ææææbstɹækʃən = \"æ*bstɹækʃən\"\n\nfor i in range(10):\n    print(re.fullmatch(regex_ææææbstɹækʃən, \"æ\"*i + string_æbstɹækʃən[1:]))\n\n&lt;re.Match object; span=(0, 9), match='bstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 11), match='ææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 12), match='æææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 13), match='ææææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 14), match='æææææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 15), match='ææææææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 16), match='æææææææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 17), match='ææææææææbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 18), match='æææææææææbstɹækʃən'&gt;\n\n\nTo apply the Kleene star to a complex regular expression, we need ().\n\nregex_reæbstɹækʃən = \"(ɹi|di)*æbstɹækʃən\"\n\nfor i in range(3):\n    print(re.fullmatch(regex_reæbstɹækʃən, \"ɹi\"*i + string_æbstɹækʃən))\n    print(re.fullmatch(regex_reæbstɹækʃən, \"di\"*i + string_æbstɹækʃən))\n    print(re.fullmatch(regex_reæbstɹækʃən, \"ɹidi\"*i + string_æbstɹækʃən))\n    print(re.fullmatch(regex_reæbstɹækʃən, \"diɹi\"*i + string_æbstɹækʃən))\n\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 12), match='ɹiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 12), match='diæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 14), match='ɹidiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 14), match='diɹiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 14), match='ɹiɹiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 14), match='didiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 18), match='ɹidiɹidiæbstɹækʃən'&gt;\n&lt;re.Match object; span=(0, 18), match='diɹidiɹiæbstɹækʃən'&gt;"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#groups-and-greediness",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#groups-and-greediness",
    "title": "Implementing the Regular Operations",
    "section": "Groups and greediness",
    "text": "Groups and greediness\nOne of the major uses for regular expressions is for extracting substrings from a string. This can be done with groups. For instance, suppose I want all of the last names of people named Aaron in some text (if such a last name is mentioned). I would put parentheses around what I think is the last name and use the findall function.\n\nregex = 'Aaron ([A-Z][a-z]*)'\nstring = 'Aaron White'\n\n#re.fullmatch(regex, string)\nre.findall(regex, string)\n\n['White']\n\n\nThis particular pattern could be problematic if a middle name is listed.\n\nregex = 'Aaron ([A-Z][a-z]*)'\nstring = 'Aaron Steven White'\n\nre.findall(regex, string)\n\n['Steven']\n\n\nTo handle this, we could explicitly assume that there is always a middle name.\n\nregex = 'Aaron [A-Z][a-z]* ([A-Z][a-z]*)'\nstring = 'Aaron Steven White'\n\nre.findall(regex, string)\n\n['White']\n\n\nBut then what about the cases where there’s not?\n\nstring = 'Aaron Steven White'\n\nre.findall(regex, string)\n\n['White']\n\n\nIn this case, we can use the quantifier ?, which matches zero or one instances of a character before it.\n\nstring = 'Aaron Steven White'\nregex = 'Aaron ([A-Z]?[a-z]+ ?)+'\n\nre.findall(regex, string)\n\n['White']\n\n\nWe don’t always want to have to list out all the possible matches we might find. For instance, suppose we’re parsing a CSV.\n\ncsv = 'c1,c2,c3,c4,c5,c6,c7,c8\\nd1,d2,d3,d4,d5,d6,d7,d8'\n\nprint(csv)\n\nc1,c2,c3,c4,c5,c6,c7,c8\nd1,d2,d3,d4,d5,d6,d7,d8\n\n\nFor this, we want to be able to repeat a pattern an arbitrary number of times.\nYou’d think you might be able to use the following:\n\nregex = '((.*),)*'\n\nre.findall(regex, csv)\n\n[('c1,c2,c3,c4,c5,c6,c7,', 'c1,c2,c3,c4,c5,c6,c7'),\n ('', ''),\n ('', ''),\n ('', ''),\n ('d1,d2,d3,d4,d5,d6,d7,', 'd1,d2,d3,d4,d5,d6,d7'),\n ('', ''),\n ('', ''),\n ('', '')]\n\n\nWhy doesn’t this work? The problem is that quantifiers are greedy by default: thy will match as much as they can before stopping. We can use ? to ensure that the quantifier is not intepreted greedily.\n\nregex = '((.*?),)+?'\n\nre.findall(regex, csv)\n\n[('c1,', 'c1'),\n ('c2,', 'c2'),\n ('c3,', 'c3'),\n ('c4,', 'c4'),\n ('c5,', 'c5'),\n ('c6,', 'c6'),\n ('c7,', 'c7'),\n ('d1,', 'd1'),\n ('d2,', 'd2'),\n ('d3,', 'd3'),\n ('d4,', 'd4'),\n ('d5,', 'd5'),\n ('d6,', 'd6'),\n ('d7,', 'd7')]\n\n\nThat’s almost right, but what’s the problem? We’re capturing more than we want, and we’re missing the last column. To partially handle the first problem, we can use non-capturing groups, which place a ?: right after the left (.\n\nregex = r'(?:(.+?)(?:,|\\n))+?'\n\nre.findall(regex, csv)\n\n['c1',\n 'c2',\n 'c3',\n 'c4',\n 'c5',\n 'c6',\n 'c7',\n 'c8',\n 'd1',\n 'd2',\n 'd3',\n 'd4',\n 'd5',\n 'd6',\n 'd7']\n\n\nHow do we make sure we capture the last column?\n\nregex = r'(?:(.+?)(?:,|(?=\\n)|(?=$)))+?'\n\nre.findall(regex, csv)\n\n['c1',\n 'c2',\n 'c3',\n 'c4',\n 'c5',\n 'c6',\n 'c7',\n 'c8',\n 'd1',\n 'd2',\n 'd3',\n 'd4',\n 'd5',\n 'd6',\n 'd7',\n 'd8']\n\n\nTo get a format that represents each row in the CSV as its own list, we might then restructure this result into a list of lists. In actual practice, you could do this in an number of ways—e.g. first matching each row up to a newline, then matching the above regex with (?=\\n) part against the list or eschewing regexes and using Python’s str.split() methods or the csv library (which, by the way, would be very useful for Assignment 1).\n\nA note on raw (r) strings\nOne thing I haven’t addressed here is that you’ll often seen an odd-looking r directly before the first quote in the string in a lot of Python regular expressions. This is because, as explained here…\n\nRegular expressions use the backslash character (‘\\’) to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals; for example, to match a literal backslash, one might have to write ‘\\\\\\\\’ as the pattern string, because the regular expression must be \\\\, and each backslash must be expressed as \\\\ inside a regular Python string literal.\n\n\nThe solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with ‘r’. So r”” is a two-character string containing ‘\\’ and ‘n’, while “” is a one-character string containing a newline. Usually patterns will be expressed in Python code using this raw string notation.\n\nSo it only really matters when you’re using special escape characters, but you should just get in the habit of using it more generally."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#querying-a-lexicon",
    "href": "formal-and-practical-preliminaries/regular-expressions/implementing-the-regular-operations.html#querying-a-lexicon",
    "title": "Implementing the Regular Operations",
    "section": "Querying a lexicon",
    "text": "Querying a lexicon\nOne use case for regular expressions in the context of computational linguistics is querying a lexicon. To look at this use case, we will use the lexicon of word forms found in the CMU Pronouncing Dictionary.\n\n!wget https://raw.githubusercontent.com/Alexir/CMUdict/master/cmudict-0.7b\n\n--2024-01-29 12:13:08--  https://raw.githubusercontent.com/Alexir/CMUdict/master/cmudict-0.7b\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3865710 (3.7M) [text/plain]\nSaving to: ‘cmudict-0.7b.6’\n\ncmudict-0.7b.6      100%[===================&gt;]   3.69M  12.2MB/s    in 0.3s    \n\n2024-01-29 12:13:09 (12.2 MB/s) - ‘cmudict-0.7b.6’ saved [3865710/3865710]\n\n\n\n\n!head -n 200 cmudict-0.7b\n\n;;; # CMUdict  --  Major Version: 0.07\n;;; \n;;; # $HeadURL: https://svn.code.sf.net/p/cmusphinx/code/branches/cmudict/cmudict-0.7b $\n;;; # $Date:: 2015-07-15 12:34:30 -0400 (Wed, 15 Jul 2015)      $:\n;;; # $Id:: cmudict-0.7b 13083 2015-07-15 16:34:30Z air         $:\n;;; # $Rev:: 13083                                              $: \n;;; # $Author:: air                                             $:\n;;;\n;;; #\n;;; # ========================================================================\n;;; # Copyright (C) 1993-2015 Carnegie Mellon University. All rights reserved.\n;;; #\n;;; # Redistribution and use in source and binary forms, with or without\n;;; # modification, are permitted provided that the following conditions\n;;; # are met:\n;;; #\n;;; # 1. Redistributions of source code must retain the above copyright\n;;; #    notice, this list of conditions and the following disclaimer.\n;;; #    The contents of this file are deemed to be source code.\n;;; #\n;;; # 2. Redistributions in binary form must reproduce the above copyright\n;;; #    notice, this list of conditions and the following disclaimer in\n;;; #    the documentation and/or other materials provided with the\n;;; #    distribution.\n;;; #\n;;; # This work was supported in part by funding from the Defense Advanced\n;;; # Research Projects Agency, the Office of Naval Research and the National\n;;; # Science Foundation of the United States of America, and by member\n;;; # companies of the Carnegie Mellon Sphinx Speech Consortium. We acknowledge\n;;; # the contributions of many volunteers to the expansion and improvement of\n;;; # this dictionary.\n;;; #\n;;; # THIS SOFTWARE IS PROVIDED BY CARNEGIE MELLON UNIVERSITY ``AS IS'' AND\n;;; # ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\n;;; # THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\n;;; # PURPOSE ARE DISCLAIMED.  IN NO EVENT SHALL CARNEGIE MELLON UNIVERSITY\n;;; # NOR ITS EMPLOYEES BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n;;; # SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n;;; # LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n;;; # DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n;;; # THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n;;; # (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n;;; # OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n;;; #\n;;; # ========================================================================\n;;; #\n;;;\n;;;  NOTES  ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n;;; \n;;;  [20080401] (air)  New dict file format introduced \n;;;   - comments (like this section) are allowed\n;;;   - file name is major version; vers/rev information is now in the header\n;;;\n;;; \n;;; ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;\n;;; \n!EXCLAMATION-POINT  EH2 K S K L AH0 M EY1 SH AH0 N P OY2 N T\n\"CLOSE-QUOTE  K L OW1 Z K W OW1 T\n\"DOUBLE-QUOTE  D AH1 B AH0 L K W OW1 T\n\"END-OF-QUOTE  EH1 N D AH0 V K W OW1 T\n\"END-QUOTE  EH1 N D K W OW1 T\n\"IN-QUOTES  IH1 N K W OW1 T S\n\"QUOTE  K W OW1 T\n\"UNQUOTE  AH1 N K W OW1 T\n#HASH-MARK  HH AE1 M AA2 R K\n#POUND-SIGN  P AW1 N D S AY2 N\n#SHARP-SIGN  SH AA1 R P S AY2 N\n%PERCENT  P ER0 S EH1 N T\n&AMPERSAND  AE1 M P ER0 S AE2 N D\n'ALLO  AA2 L OW1\n'APOSTROPHE  AH0 P AA1 S T R AH0 F IY0\n'BOUT  B AW1 T\n'CAUSE  K AH0 Z\n'COURSE  K AO1 R S\n'CUSE  K Y UW1 Z\n'EM  AH0 M\n'END-INNER-QUOTE  EH1 N D IH1 N ER0 K W OW1 T\n'END-QUOTE  EH1 N D K W OW1 T\n'FRISCO  F R IH1 S K OW0\n'GAIN  G EH1 N\n'INNER-QUOTE  IH1 N ER0 K W OW1 T\n'KAY  K EY1\n'M  AH0 M\n'N  AH0 N\n'QUOTE  K W OW1 T\n'ROUND  R AW1 N D\n'S  EH1 S\n'SINGLE-QUOTE  S IH1 NG G AH0 L K W OW1 T\n'TIL  T IH1 L\n'TIS  T IH1 Z\n'TWAS  T W AH1 Z\n(BEGIN-PARENS  B IH0 G IH1 N P ER0 EH1 N Z\n(IN-PARENTHESES  IH1 N P ER0 EH1 N TH AH0 S IY2 Z\n(LEFT-PAREN  L EH1 F T P ER0 EH1 N\n(OPEN-PARENTHESES  OW1 P AH0 N P ER0 EH1 N TH AH0 S IY2 Z\n(PAREN  P ER0 EH1 N\n(PARENS  P ER0 EH1 N Z\n(PARENTHESES  P ER0 EH1 N TH AH0 S IY2 Z\n)CLOSE-PAREN  K L OW1 Z P ER0 EH1 N\n)CLOSE-PARENTHESES  K L OW1 Z P ER0 EH1 N TH AH0 S IY2 Z\n)END-PAREN  EH1 N D P ER0 EH1 N\n)END-PARENS  EH1 N D P ER0 EH1 N Z\n)END-PARENTHESES  EH1 N D P ER0 EH1 N TH AH0 S IY2 Z\n)END-THE-PAREN  EH1 N D DH AH0 P ER0 EH1 N\n)PAREN  P ER0 EH1 N\n)PARENS  P ER0 EH1 N Z\n)RIGHT-PAREN  R AY1 T P EH2 R AH0 N\n)UN-PARENTHESES  AH1 N P ER0 EH1 N TH AH0 S IY1 Z\n+PLUS  P L UH1 S\n,COMMA  K AA1 M AH0\n--DASH  D AE1 SH\n-DASH  D AE1 SH\n-HYPHEN  HH AY1 F AH0 N\n...ELLIPSIS  IH2 L IH1 P S IH0 S\n.DECIMAL  D EH1 S AH0 M AH0 L\n.DOT  D AA1 T\n.FULL-STOP  F UH1 L S T AA1 P\n.PERIOD  P IH1 R IY0 AH0 D\n.POINT  P OY1 N T\n/SLASH  S L AE1 SH\n3-D  TH R IY1 D IY2\n3D  TH R IY1 D IY2\n:COLON  K OW1 L AH0 N\n;SEMI-COLON  S EH1 M IY0 K OW1 L AH0 N\n;SEMI-COLON(1)  S EH1 M IH0 K OW2 L AH0 N\n?QUESTION-MARK  K W EH1 S CH AH0 N M AA1 R K\nA  AH0\nA(1)  EY1\nA'S  EY1 Z\nA.  EY1\nA.'S  EY1 Z\nA.D.  EY2 D IY1\nA.M.  EY2 EH1 M\nA.S  EY1 Z\nA42128  EY1 F AO1 R T UW1 W AH1 N T UW1 EY1 T\nAA  EY2 EY1\nAAA  T R IH2 P AH0 L EY1\nAAAI  T R IH2 P AH0 L EY2 AY1\nAABERG  AA1 B ER0 G\nAACHEN  AA1 K AH0 N\nAACHENER  AA1 K AH0 N ER0\nAAH  AA1\nAAKER  AA1 K ER0\nAALIYAH  AA2 L IY1 AA2\nAALSETH  AA1 L S EH0 TH\nAAMODT  AA1 M AH0 T\nAANCOR  AA1 N K AO2 R\nAARDEMA  AA0 R D EH1 M AH0\nAARDVARK  AA1 R D V AA2 R K\nAARDVARKS  AA1 R D V AA2 R K S\nAARGH  AA1 R G\nAARHUS  AA2 HH UW1 S\nAARON  EH1 R AH0 N\nAARON'S  EH1 R AH0 N Z\nAARONS  EH1 R AH0 N Z\nAARONSON  EH1 R AH0 N S AH0 N\nAARONSON(1)  AA1 R AH0 N S AH0 N\nAARONSON'S  EH1 R AH0 N S AH0 N Z\nAARONSON'S(1)  AA1 R AH0 N S AH0 N Z\nAARTI  AA1 R T IY2\nAASE  AA1 S\nAASEN  AA1 S AH0 N\nAB  AE1 B\nABA  EY2 B IY2 EY1\nABABA  AH0 B AA1 B AH0\nABABA(1)  AA1 B AH0 B AH0\nABACHA  AE1 B AH0 K AH0\nABACK  AH0 B AE1 K\nABACO  AE1 B AH0 K OW2\nABACUS  AE1 B AH0 K AH0 S\nABAD  AH0 B AA1 D\nABADAKA  AH0 B AE1 D AH0 K AH0\nABADI  AH0 B AE1 D IY0\nABADIE  AH0 B AE1 D IY0\nABAIR  AH0 B EH1 R\nABALKIN  AH0 B AA1 L K IH0 N\nABALONE  AE2 B AH0 L OW1 N IY0\nABALONES  AE2 B AH0 L OW1 N IY0 Z\nABALOS  AA0 B AA1 L OW0 Z\nABANDON  AH0 B AE1 N D AH0 N\nABANDONED  AH0 B AE1 N D AH0 N D\nABANDONING  AH0 B AE1 N D AH0 N IH0 NG\nABANDONMENT  AH0 B AE1 N D AH0 N M AH0 N T\nABANDONMENTS  AH0 B AE1 N D AH0 N M AH0 N T S\nABANDONS  AH0 B AE1 N D AH0 N Z\nABANTO  AH0 B AE1 N T OW0\nABARCA  AH0 B AA1 R K AH0\nABARE  AA0 B AA1 R IY0\nABASCAL  AE1 B AH0 S K AH0 L\nABASH  AH0 B AE1 SH\nABASHED  AH0 B AE1 SH T\nABASIA  AH0 B EY1 ZH Y AH0\nABATE  AH0 B EY1 T\nABATED  AH0 B EY1 T IH0 D\nABATEMENT  AH0 B EY1 T M AH0 N T\nABATEMENTS  AH0 B EY1 T M AH0 N T S\nABATES  AH0 B EY1 T S\nABATING  AH0 B EY1 T IH0 NG\nABATTOIR  AE2 B AH0 T W AA1 R\nABBA  AE1 B AH0\n\n\n\nwith open('cmudict-0.7b', encoding = \"ISO-8859-1\") as f:\n    cmudict = [l.split() for l in f if l[:3] != \";;;\"]\n    cmudict = {w[0].lower(): w[1:] for w in cmudict}\n    \ncmudict[\"abstraction\"]\n\n['AE0', 'B', 'S', 'T', 'R', 'AE1', 'K', 'SH', 'AH0', 'N']\n\n\nThis dictionary uses what’s known as the ARPABET and represents stress using numeric indicators: 0 for no stress, 1 for primary stress, and 2 for secondary stress. The ARPABET maps to more recognizable phonemes.\n\narpabet_to_phoneme = {'AA': 'ɑ', \n                      'AE': 'æ', \n                      'AH': 'ʌ', \n                      'AO': 'ɔ', \n                      'AW': 'aʊ', \n                      'AY': 'aɪ', \n                      'B': 'b', \n                      'CH': 'tʃ', \n                      'D': 'd', \n                      'DH': 'ð', \n                      'EH': 'ɛ',\n                      'ER': 'ɝ', \n                      'EY': 'eɪ', \n                      'F': 'f', \n                      'G': 'g', \n                      'HH': 'h', \n                      'IH': 'ɪ', \n                      'IY': 'i', \n                      'JH': 'dʒ', \n                      'K': 'k', \n                      'L': 'l', \n                      'M': 'm', \n                      'N': 'n',\n                      'NG': 'ŋ', \n                      'OW': 'oʊ', \n                      'OY': 'ɔɪ', \n                      'P': 'p', \n                      'R': 'ɹ', \n                      'S': 's', \n                      'SH': 'ʃ', \n                      'T': 't', \n                      'TH': 'θ', \n                      'UH': 'ʊ', \n                      'UW': 'u', \n                      'V': 'v',\n                      'W': 'w', \n                      'Y': 'j', \n                      'Z': 'z', \n                      'ZH': 'ʒ'}\n\n\nimport re\n\nentries: dict[str, list[str]] = {\n    w: [arpabet_to_phoneme[''.join(re.findall('[A-z]', phoneme))]\n        for phoneme in transcription]\n        for w, transcription in cmudict.items()\n        if len({c for c in w})&gt;1\n        if len(transcription)&gt;1 \n        if not re.findall('[^A-z]', w[0])\n}\n\nentries[\"abstraction\"]\n\n['æ', 'b', 's', 't', 'ɹ', 'æ', 'k', 'ʃ', 'ʌ', 'n']\n\n\nFor instance, one thing we might be interested in is investigating ablaut patterns—e.g. cases where the verb inflects for some tense or aspect by a vowel change rather than a regular “-ed” ending. For instance, the past tense of sing is sang, rather than singed.\nExercise: how would we find all cases of this sort of ablaut using English UniMorph? Does your solution handle cases like fight to fought.\n\n# implement regex-based solution here\n\nExercise: how might we discover cases that are similar to think, whose past is thought? Note that this case is intuitively different in nature from verbs that are fully irregular in their inflection, like be.\n\n# implement regex-based solution here"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/wild-cards-and-character-ranges.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/wild-cards-and-character-ranges.html",
    "title": "Wild cards and character ranges",
    "section": "",
    "text": "We can represent all simple regular expressions according to our formal definition, but in certain cases, doing so would be tedious. For instance, suppose I want to represent the set of all English phonemes \\(\\Sigma\\). Using our formal definition, we would need to list out all of the phonemes joined by \\(\\cup\\): \\((\\text{i} \\cup (\\text{ɝ} \\cup (\\text{a} \\cup (\\text{ɪ} \\cup \\ldots))))\\).\nTo make this less tedious, Python (and many other languages) introduce additional special characters into the definition of \\(R(\\Sigma)\\). The most basic is the wildcard ., which matches any single character (alphanumeric or otherwise) except the newline \\n.\n\n\nLoad IPA representation of CMU Pronouncing Dictionary\nwith open(\"cmudict-ipa\") as f:\n    entries: list[tuple[str, str]] = [\n        l.strip().split(\",\") for l in f\n    ]\n    entries: dict[str, list[str]] = {\n        w: ipa.split() for w, ipa in entries\n    }\n\n\n\nimport re\n\nregex_dot_bstɹ_dot_kʃən = '.bstɹ.kʃən'\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot_bstɹ_dot_kʃən, \"\".join(ipa)):\n        print(\"\".join(ipa), f\"({w})\")\n\nabstraction æbstɹækʃən\nobstruction əbstɹəkʃən\n\n\nIf you want to match the . itself (or any special character we introduce below), you need to escape it.\n\nregex_period_bstɹækʃən = '\\.bstɹækʃən'\n\nre.fullmatch(regex_period_bstɹækʃən, '.' + string_æbstɹækʃən[1:])\n\n&lt;re.Match object; span=(0, 10), match='.bstɹækʃən'&gt;\n\n\nBesides ., we can also use character ranges to target more specific sets, like upper- and lower-case alphabetic characters ([A-z]), lower-case alphabetic character ([a-z]), numerals [0-9].\n\nregex_numeric_bstɹækʃən = '[0-9]bstɹækʃən'\nstring_4bstɹækʃən = '4bstɹækʃən'\n\nre.fullmatch(regex_numeric_bstɹækʃən, string_4bstɹækʃən)\n\n&lt;re.Match object; span=(0, 10), match='4bstɹækʃən'&gt;\n\n\nIn addition to ranges, there are even more compact escape characters. For instance, alphanumeric characters plus _ can be gotten with \\w (the same as the range [A-z0-9_]).\n\nregex_alphanumeric_bstɹækʃən = '\\wbstɹækʃən'\n\nre.fullmatch(regex_alphanumeric_bstɹækʃən, string_æbstɹækʃən)\n\n&lt;re.Match object; span=(0, 10), match='æbstɹækʃən'&gt;"
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/quantifiers.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/quantifiers.html",
    "title": "Quantifiers",
    "section": "",
    "text": "Note that ., the character ranges, and the escape characters match only a single character, and so to match more than one, we need more than one of whichever we are interested in matching.\n\n\nLoad IPA representation of CMU Pronouncing Dictionary\nwith open(\"cmudict-ipa\") as f:\n    entries: list[tuple[str, str]] = [\n        l.strip().split(\",\") for l in f\n    ]\n    entries: dict[str, list[str]] = {\n        w: ipa.split() for w, ipa in entries\n    }\n\n\n\nimport re\n\nregex_dot_stɹ_dot_kʃən = '.bstɹ.kʃən'\nregex_dot_dot_tɹ_dot_kʃən = '..stɹ.kʃən'\n\nprint(regex_dot_stɹ_dot_kʃən, \"matches:\")\nprint()\n\nfor w, ipa in entries.items():    \n    if re.fullmatch(regex_dot_stɹ_dot_kʃən, \"\".join(ipa)):\n        print(regex_dot_stɹ_dot_kʃən, \"matches\", \"\".join(ipa), f\"({w})\")\n\nprint()\n\nprint(regex_dot_dot_tɹ_dot_kʃən, \"matches:\")\nprint()\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot_dot_tɹ_dot_kʃən, \"\".join(ipa)):\n        print(\"\".join(ipa), f\"({w})\")\n\n.bstɹ.kʃən matches:\n\n.bstɹ.kʃən matches æbstɹækʃən (abstraction)\n.bstɹ.kʃən matches əbstɹəkʃən (obstruction)\n\n..stɹ.kʃən matches:\n\næbstɹækʃən (abstraction)\ndɪstɹəkʃən (destruction)\ndɪstɹækʃən (distraction)\nɛkstɹækʃən (extraction)\nɪnstɹəkʃən (instruction)\nəbstɹəkʃən (obstruction)\nɹistɹɪkʃən (restriction)\n\n\nWe can avoid writing out multiple by using a quantifier. There are a few different quantifiers. For instance, if you have an exact number in mind:\n\nregex_dot2_tɹ_dot_kʃən = '.{2}stɹ.kʃən'\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot2_tɹ_dot_kʃən, \"\".join(ipa)):\n        print(\"\".join(ipa), f\"({w})\")\n\næbstɹækʃən (abstraction)\ndɪstɹəkʃən (destruction)\ndɪstɹækʃən (distraction)\nɛkstɹækʃən (extraction)\nɪnstɹəkʃən (instruction)\nəbstɹəkʃən (obstruction)\nɹistɹɪkʃən (restriction)\n\n\nOr if you had a range of numbers in mind:\n\nregex_dot2_tɹæk_dot13 = '.{2}stɹək.{1,3}'\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot2_tɹæk_dot13, \"\".join(ipa)):\n        print(\"\".join(ipa), f\"({w})\")\n\ndɪstɹəkt (destruct)\ndɪstɹəktɪd (destructed)\ndɪstɹəktɪŋ (destructing)\ndɪstɹəkʃən (destruction)\ndɪstɹəktɪv (destructive)\ndɪstɹəkts (destructs)\nɛkstɹəkeɪt (extricate)\nɪnstɹəkt (instruct)\nɪnstɹəktəd (instructed)\nɪnstɹəktɪd (instructed(1))\nɪnstɹəktɪŋ (instructing)\nɪnstɹəkʃən (instruction)\nɪnstɹəktɪv (instructive)\nɪnstɹəktɝ (instructor)\nɪnstɹəktɝz (instructors)\nɪnstɹəkts (instructs)\nəbstɹəkt (obstruct)\nəbstɹəktɪd (obstructed)\nəbstɹəktɪŋ (obstructing)\nəbstɹəkʃən (obstruction)\nəbstɹəktɪv (obstructive)\nəbstɹəkts (obstructs)\nɹistɹəktʃɝ (restructure)\nənstɹəkʃɝd (unstructured)\n\n\nYou can also leave off one bound:\n\nregex_dot2_tɹæk_dot03 = '.{2}stɹək.{,3}'\nregex_dot2_tɹæk_dot1inf = '.{2}stɹək.{1,}'\n\nprint(regex_dot2_tɹæk_dot03, \"matches:\")\nprint()\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot2_tɹæk_dot03, \"\".join(ipa)):\n        if n_matches &lt; 10:\n            n_matches += 1\n            print(\"\".join(ipa), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\nprint()\nprint(regex_dot2_tɹæk_dot1inf, \"matches:\")\nprint()\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot2_tɹæk_dot1inf, \"\".join(ipa)):\n        if n_matches &lt; 10:\n            n_matches += 1\n            print(\"\".join(ipa), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\n.{2}stɹək.{,3} matches:\n\ndɪstɹəkt (destruct)\ndɪstɹəktɪd (destructed)\ndɪstɹəktɪŋ (destructing)\ndɪstɹəkʃən (destruction)\ndɪstɹəktɪv (destructive)\ndɪstɹəkts (destructs)\nɛkstɹəkeɪt (extricate)\nɪnstɹəkt (instruct)\nɪnstɹəktəd (instructed)\nɪnstɹəktɪd (instructed(1))\n...\n\n.{2}stɹək.{1,} matches:\n\ndɪstɹəkt (destruct)\ndɪstɹəktəbəl (destructable)\ndɪstɹəktɪd (destructed)\ndɪstɹəktɪŋ (destructing)\ndɪstɹəkʃən (destruction)\ndɪstɹəktɪv (destructive)\ndɪstɹəktɪvnɪs (destructiveness)\ndɪstɹəkts (destructs)\nɛkstɹəkɝɪkjəlɝ (extracurricular)\nɛkstɹəkeɪt (extricate)\n...\n\n\nNote that {,} is equivalent to *. There is also a special quantifier symbol for {1,}: +\nAnd if you wanted at least one character ot come after Aaron, but didn’t care after that you could use +.\n\nregex_dot2_tɹæk_dotplus = '.{2}stɹək.+'\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_dot2_tɹæk_dotplus, \"\".join(ipa)):\n        if n_matches &lt; 10:\n            n_matches += 1\n            print(\"\".join(ipa), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\ndɪstɹəkt (destruct)\ndɪstɹəktəbəl (destructable)\ndɪstɹəktɪd (destructed)\ndɪstɹəktɪŋ (destructing)\ndɪstɹəkʃən (destruction)\ndɪstɹəktɪv (destructive)\ndɪstɹəktɪvnɪs (destructiveness)\ndɪstɹəkts (destructs)\nɛkstɹəkɝɪkjəlɝ (extracurricular)\nɛkstɹəkeɪt (extricate)\n...\n\n\nNote that none of these quantifiers increase the expressive power of the regular expressions. We can always write their equivalents as a vanilla regular expression (in the sense of the formal definition we gave above); it would just be tedious in many cases.\n\nSet complement\nFor any of these cases where we escape a lowercase alphabetic character to get a character set, the set complement can generally be gotten with by the uppercase version—e.g. \\w goes to \\W.\n\nregex_notw_bstɹækt = '\\Wbstɹəkt'\n\n(re.fullmatch(regex_notw_bstɹækt, \"\".join(entries[\"obstruct\"])),\n re.fullmatch(regex_notw_bstɹækt, '\\n'+\"\".join(entries[\"obstruct\"])[1:]))\n\n(None, &lt;re.Match object; span=(0, 8), match='\\nbstɹəkt'&gt;)\n\n\nSometimes you want the complement of a set that doesn’t have an associated escaped alphabetic character. For that you can use the same square bracket set notation but put a ^ after the first bracket.\n\nregex_notæ_bstɹ_notæ_kt = '[^æ][^b]stɹ[^æ]kt'\n\nfor w, ipa in entries.items():    \n    if re.fullmatch(regex_notæ_bstɹ_notæ_kt, \"\".join(ipa)):\n        print(\"\".join(ipa), f\"({w})\")\n\ndɪstɹəkt (destruct)\ndɪstɹɪkt (district)\nɪnstɹəkt (instruct)\nmɑstɹɪkt (maastricht)\nɹistɹɪkt (restrict)\n\n\nThe placement of this ^ is really important, since it only has the negation interpretation directly after [."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/groups-and-greediness.html",
    "href": "formal-and-practical-preliminaries/regular-expressions/groups-and-greediness.html",
    "title": "Groups and Greediness",
    "section": "",
    "text": "One of the major uses for regular expressions is for extracting substrings from a string. This can be done with groups. For instance, suppose I want all of stems that have the morpheme with the form /ʃən/.\nLoad IPA representation of CMU Pronouncing Dictionary\nwith open(\"cmudict-ipa\") as f:\n    entries: list[tuple[str, str]] = [\n        l.strip().split(\",\") for l in f\n    ]\n    entries: dict[str, list[str]] = {\n        w: ipa.split() for w, ipa in entries\n    }\nimport re\n\nregex_ʃən = '(.+)ʃən'\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_ʃən, \"\".join(ipa)):\n        if n_matches &lt; 30:\n            n_matches += 1\n            print(\"\".join(ipa), re.findall(regex_ʃən, \"\".join(ipa)), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\nəbɹivieɪʃən ['əbɹivieɪ'] (abbreviation)\næbdɪkeɪʃən ['æbdɪkeɪ'] (abdication)\næbdəkʃən ['æbdək'] (abduction)\nəbdəkʃən ['əbdək'] (abduction(1))\næbɝeɪʃən ['æbɝeɪ'] (aberration)\nəbleɪʃən ['əbleɪ'] (ablation)\nəbluʃən ['əblu'] (ablution)\næbnɛgeɪʃən ['æbnɛgeɪ'] (abnegation)\næbəlɪʃən ['æbəlɪ'] (abolition)\nəbɑməneɪʃən ['əbɑməneɪ'] (abomination)\nəbɔɹʃən ['əbɔɹ'] (abortion)\næbɹəgeɪʃən ['æbɹəgeɪ'] (abrogation)\næbsəluʃən ['æbsəlu'] (absolution)\nəbzɔɹpʃən ['əbzɔɹp'] (absorption)\nəbsɔɹpʃən ['əbsɔɹp'] (absorption(1))\nəbstɛntʃən ['əbstɛnt'] (abstention)\næbstɛntʃən ['æbstɛnt'] (abstention(1))\næbstɹækʃən ['æbstɹæk'] (abstraction)\nækədəmɪʃən ['ækədəmɪ'] (academician)\næksɛlɝeɪʃən ['æksɛlɝeɪ'] (acceleration)\nəksɛʃən ['əksɛ'] (accession)\nækləmeɪʃən ['ækləmeɪ'] (acclamation)\nækləmeɪʃən ['ækləmeɪ'] (acclimation)\nəkɑmədeɪʃən ['əkɑmədeɪ'] (accommodation)\nəkɹɛdəteɪʃən ['əkɹɛdəteɪ'] (accreditation)\nəkɹiʃən ['əkɹi'] (accretion)\nəkjumjəleɪʃən ['əkjumjəleɪ'] (accumulation)\nækjəzeɪʃən ['ækjəzeɪ'] (accusation)\nækjuzeɪʃən ['ækjuzeɪ'] (accusation(1))\nəsɪdəfəkeɪʃən ['əsɪdəfəkeɪ'] (acidification)\n...\nThis works to some extent, but notice that it will capture cases where /ʃən/ is not a morpheme. For instance, the word passion will get matched. It will also return the wrong stem when the morpheme is realized as /eɪʃən/, such as accreditation.\nre.findall(regex_ʃən, \"\".join(entries[\"passion\"])), re.findall(regex_ʃən, \"\".join(entries[\"accreditation\"]))\n\n(['pæ'], ['əkɹɛdəteɪ'])\nTo handle the second, we might look for /eɪʃən/ and /ʃən/. We can use the quantifier ? to say that /eɪ/ is optional. Because it is a digraph, we need to surround it with parentheses.\nregex_ʃən = '(.+)(eɪ)?ʃən'\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_ʃən, \"\".join(ipa)):\n        if n_matches &lt; 30:\n            n_matches += 1\n            print(\"\".join(ipa), re.findall(regex_ʃən, \"\".join(ipa)), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\nəbɹivieɪʃən [('əbɹivieɪ', '')] (abbreviation)\næbdɪkeɪʃən [('æbdɪkeɪ', '')] (abdication)\næbdəkʃən [('æbdək', '')] (abduction)\nəbdəkʃən [('əbdək', '')] (abduction(1))\næbɝeɪʃən [('æbɝeɪ', '')] (aberration)\nəbleɪʃən [('əbleɪ', '')] (ablation)\nəbluʃən [('əblu', '')] (ablution)\næbnɛgeɪʃən [('æbnɛgeɪ', '')] (abnegation)\næbəlɪʃən [('æbəlɪ', '')] (abolition)\nəbɑməneɪʃən [('əbɑməneɪ', '')] (abomination)\nəbɔɹʃən [('əbɔɹ', '')] (abortion)\næbɹəgeɪʃən [('æbɹəgeɪ', '')] (abrogation)\næbsəluʃən [('æbsəlu', '')] (absolution)\nəbzɔɹpʃən [('əbzɔɹp', '')] (absorption)\nəbsɔɹpʃən [('əbsɔɹp', '')] (absorption(1))\nəbstɛntʃən [('əbstɛnt', '')] (abstention)\næbstɛntʃən [('æbstɛnt', '')] (abstention(1))\næbstɹækʃən [('æbstɹæk', '')] (abstraction)\nækədəmɪʃən [('ækədəmɪ', '')] (academician)\næksɛlɝeɪʃən [('æksɛlɝeɪ', '')] (acceleration)\nəksɛʃən [('əksɛ', '')] (accession)\nækləmeɪʃən [('ækləmeɪ', '')] (acclamation)\nækləmeɪʃən [('ækləmeɪ', '')] (acclimation)\nəkɑmədeɪʃən [('əkɑmədeɪ', '')] (accommodation)\nəkɹɛdəteɪʃən [('əkɹɛdəteɪ', '')] (accreditation)\nəkɹiʃən [('əkɹi', '')] (accretion)\nəkjumjəleɪʃən [('əkjumjəleɪ', '')] (accumulation)\nækjəzeɪʃən [('ækjəzeɪ', '')] (accusation)\nækjuzeɪʃən [('ækjuzeɪ', '')] (accusation(1))\nəsɪdəfəkeɪʃən [('əsɪdəfəkeɪ', '')] (acidification)\n...\nThe problem is that this makes Python think we want to capture it. So what we need is a non-capturing group, which we get by putting ?: after the open parenthesis.\nregex_ʃən = '(.+)(?:eɪ)?ʃən'\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_ʃən, \"\".join(ipa)):\n        if n_matches &lt; 30:\n            n_matches += 1\n            print(\"\".join(ipa), re.findall(regex_ʃən, \"\".join(ipa)), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\nəbɹivieɪʃən ['əbɹivieɪ'] (abbreviation)\næbdɪkeɪʃən ['æbdɪkeɪ'] (abdication)\næbdəkʃən ['æbdək'] (abduction)\nəbdəkʃən ['əbdək'] (abduction(1))\næbɝeɪʃən ['æbɝeɪ'] (aberration)\nəbleɪʃən ['əbleɪ'] (ablation)\nəbluʃən ['əblu'] (ablution)\næbnɛgeɪʃən ['æbnɛgeɪ'] (abnegation)\næbəlɪʃən ['æbəlɪ'] (abolition)\nəbɑməneɪʃən ['əbɑməneɪ'] (abomination)\nəbɔɹʃən ['əbɔɹ'] (abortion)\næbɹəgeɪʃən ['æbɹəgeɪ'] (abrogation)\næbsəluʃən ['æbsəlu'] (absolution)\nəbzɔɹpʃən ['əbzɔɹp'] (absorption)\nəbsɔɹpʃən ['əbsɔɹp'] (absorption(1))\nəbstɛntʃən ['əbstɛnt'] (abstention)\næbstɛntʃən ['æbstɛnt'] (abstention(1))\næbstɹækʃən ['æbstɹæk'] (abstraction)\nækədəmɪʃən ['ækədəmɪ'] (academician)\næksɛlɝeɪʃən ['æksɛlɝeɪ'] (acceleration)\nəksɛʃən ['əksɛ'] (accession)\nækləmeɪʃən ['ækləmeɪ'] (acclamation)\nækləmeɪʃən ['ækləmeɪ'] (acclimation)\nəkɑmədeɪʃən ['əkɑmədeɪ'] (accommodation)\nəkɹɛdəteɪʃən ['əkɹɛdəteɪ'] (accreditation)\nəkɹiʃən ['əkɹi'] (accretion)\nəkjumjəleɪʃən ['əkjumjəleɪ'] (accumulation)\nækjəzeɪʃən ['ækjəzeɪ'] (accusation)\nækjuzeɪʃən ['ækjuzeɪ'] (accusation(1))\nəsɪdəfəkeɪʃən ['əsɪdəfəkeɪ'] (acidification)\n...\nIt still seems to be capturing /eɪ/ in accreditation. What gives? The reason this is happening is that quantifiers like + are greedy by default. That means they will match as much as they can. And because /eɪ/ is optional, (.+) can match it.\nTo make sure it doesn’t match it if it doesn’t need to, we can make the quantifier non-greedy by appending a ?.\nregex_ʃən = '(.+?)(?:eɪ)?ʃən'\n\nn_matches = 0\n\nfor w, ipa in entries.items():\n    if re.fullmatch(regex_ʃən, \"\".join(ipa)):\n        if n_matches &lt; 30:\n            n_matches += 1\n            print(\"\".join(ipa), re.findall(regex_ʃən, \"\".join(ipa)), f\"({w})\")\n        else:\n            print(\"...\")\n            break\n\nəbɹivieɪʃən ['əbɹivi'] (abbreviation)\næbdɪkeɪʃən ['æbdɪk'] (abdication)\næbdəkʃən ['æbdək'] (abduction)\nəbdəkʃən ['əbdək'] (abduction(1))\næbɝeɪʃən ['æbɝ'] (aberration)\nəbleɪʃən ['əbl'] (ablation)\nəbluʃən ['əblu'] (ablution)\næbnɛgeɪʃən ['æbnɛg'] (abnegation)\næbəlɪʃən ['æbəlɪ'] (abolition)\nəbɑməneɪʃən ['əbɑmən'] (abomination)\nəbɔɹʃən ['əbɔɹ'] (abortion)\næbɹəgeɪʃən ['æbɹəg'] (abrogation)\næbsəluʃən ['æbsəlu'] (absolution)\nəbzɔɹpʃən ['əbzɔɹp'] (absorption)\nəbsɔɹpʃən ['əbsɔɹp'] (absorption(1))\nəbstɛntʃən ['əbstɛnt'] (abstention)\næbstɛntʃən ['æbstɛnt'] (abstention(1))\næbstɹækʃən ['æbstɹæk'] (abstraction)\nækədəmɪʃən ['ækədəmɪ'] (academician)\næksɛlɝeɪʃən ['æksɛlɝ'] (acceleration)\nəksɛʃən ['əksɛ'] (accession)\nækləmeɪʃən ['ækləm'] (acclamation)\nækləmeɪʃən ['ækləm'] (acclimation)\nəkɑmədeɪʃən ['əkɑməd'] (accommodation)\nəkɹɛdəteɪʃən ['əkɹɛdət'] (accreditation)\nəkɹiʃən ['əkɹi'] (accretion)\nəkjumjəleɪʃən ['əkjumjəl'] (accumulation)\nækjəzeɪʃən ['ækjəz'] (accusation)\nækjuzeɪʃən ['ækjuz'] (accusation(1))\nəsɪdəfəkeɪʃən ['əsɪdəfək'] (acidification)\n...\nOkay. So how do we deal with cases where /ʃən/ is not a morpheme? One thing we can do is to look for stems that show up without /ʃən/. This will exclude passion, since /pæ/ is not a word.\nregex_ʃən = '(.+?)(?:eɪ)?ʃən'\n\nn_matches = 0\nseen = set()\n\nfor w1, ipa1 in entries.items():\n    possible_morpheme = re.findall(regex_ʃən, \"\".join(ipa1))\n    if possible_morpheme:\n        for w2, ipa2 in entries.items():\n            if re.fullmatch(possible_morpheme[0], \"\".join(ipa2)):\n                if n_matches &lt; 20 and \"\".join(ipa2) not in seen:\n                    n_matches += 1\n                    seen |= {\"\".join(ipa2)}\n                    print(\"\".join(ipa2), f\"({w2})\", \"+\", \"ʃən\", \"=\", \"\".join(ipa1), f\"({w1})\")\n                else:\n                    break\n\n    if n_matches &gt;= 20:   \n        print(\"...\")\n        break\n\nəbɔɹ (abor) + ʃən = əbɔɹʃən (abortion)\nəkɹi (acree) + ʃən = əkɹiʃən (accretion)\næk (ack) + ʃən = ækʃən (action)\nædɝ (adder) + ʃən = ædɝeɪʃən (adoration)\nædʒəl (agile) + ʃən = ædʒəleɪʃən (adulation)\neɪliən (alien) + ʃən = eɪliəneɪʃən (alienation)\nɔltɝ (altar) + ʃən = ɔltɝeɪʃən (alteration)\nəmælgəm (amalgam) + ʃən = əmælgəmeɪʃən (amalgamation)\neɪnt (ain't) + ʃən = eɪntʃənt (ancient)\neɪn (aine) + ʃən = eɪnʃənt (ancient(1))\nænəm (annum) + ʃən = ænəmeɪʃən (animation)\nænɛks (annex) + ʃən = ænɛkseɪʃən (annexation)\næn (ahn) + ʃən = ænʃən (anshan)\næpəl (appel) + ʃən = æpəleɪʃən (appalachian(1))\næspɝ (asper) + ʃən = æspɝeɪʃən (aspiration)\nəsæsən (assassin) + ʃən = əsæsəneɪʃən (assassination)\nɑk (och) + ʃən = ɑkʃən (auction)\nɔtəm (autumn) + ʃən = ɔtəmeɪʃən (automation)\neɪvi (av) + ʃən = eɪvieɪʃən (aviacion)\nbæt (bat) + ʃən = bætʃənd (bachand)\n...\nAn issue here is that /ʃən/ doesn’t simply get appended to a stem. There is an additional phonological process that deletes a portion of that stem–e.g. /æbstɹækt/ + /ʃən/ is /æbstɹækʃən/, not /æbstɹæktʃən/. So we need to consider cases where a final consonant–usually a t–was deleted. But we need to make sure we do so only when the morpheme wasn’t realized as /eɪʃən/, so we need to go back to capturing it.\nregex_ʃən = '(.+?)(eɪ)?ʃən'\n\nn_matches = 0\n\nseen = set()\n\nfor w1, ipa1 in entries.items():\n    possible_morpheme = re.findall(regex_ʃən, \"\".join(ipa1))\n    if possible_morpheme:\n        for w2, ipa2 in entries.items():\n            if possible_morpheme[0][1]:\n                regex_stem = possible_morpheme[0][0]\n            else:\n                regex_stem = possible_morpheme[0][0] + \"t\"\n            \n            if re.fullmatch(regex_stem, \"\".join(ipa2)):\n                if n_matches &lt; 20 and \"\".join(ipa2) not in seen:\n                    n_matches += 1\n                    seen |= {\"\".join(ipa2)}\n                    print(\"\".join(ipa2), f\"({w2})\", \"+\", \"ʃən\", \"=\", \"\".join(ipa1), f\"({w1})\")\n                else:\n                    break\n\n    if n_matches &gt;= 20:   \n        print(\"...\")\n        break\n\næbdəkt (abduct) + ʃən = æbdəkʃən (abduction)\nəbɔɹt (abort) + ʃən = əbɔɹʃən (abortion)\næbsəlut (absolut) + ʃən = æbsəluʃən (absolution)\næbstɹækt (abstract) + ʃən = æbstɹækʃən (abstraction)\nækt (act) + ʃən = ækʃən (action)\nədɪkt (addict) + ʃən = ədɪkʃən (addiction)\nədmɪt (admit) + ʃən = ədmɪʃən (admission(1))\nədɑpt (adopt) + ʃən = ədɑpʃən (adoption)\nædɝ (adder) + ʃən = ædɝeɪʃən (adoration)\nædʒəl (agile) + ʃən = ædʒəleɪʃən (adulation)\nəfɛkt (affect) + ʃən = əfɛkʃən (affection)\nəflɪkt (afflict) + ʃən = əflɪkʃən (affliction)\neɪliən (alien) + ʃən = eɪliəneɪʃən (alienation)\nɔltɝ (altar) + ʃən = ɔltɝeɪʃən (alteration)\nəmælgəm (amalgam) + ʃən = əmælgəmeɪʃən (amalgamation)\neɪnt (ain't) + ʃən = eɪnʃənt (ancient(1))\nænəm (annum) + ʃən = ænəmeɪʃən (animation)\nænɛks (annex) + ʃən = ænɛkseɪʃən (annexation)\nænt (ant) + ʃən = ænʃən (anshan)\næpəl (appel) + ʃən = æpəleɪʃən (appalachian(1))\n...\nThere’s still some wonky stuff in here–e.g. ancient coming from ain’t and ashen coming from at–but we’re getting closer. We can’t really deal with cases like ashen coming from at, but we can deal with ancient coming from ain’t, which reveals a behavior of re.findall: it functions like re.match, rather than re.fullmatch, in that it matches the beginning of a string. If we want it to match the entire string, we have to explicitly specify that in the regular expression. To do this, we can use a $, which means “end of string”.1\nregex_ʃən = '(.+?)(eɪ)?ʃən$'\n\nn_matches = 0\n\nseen = set()\n\nfor w1, ipa1 in entries.items():\n    possible_morpheme = re.findall(regex_ʃən, \"\".join(ipa1))\n    if possible_morpheme:\n        for w2, ipa2 in entries.items():\n            if possible_morpheme[0][1]:\n                regex_stem = possible_morpheme[0][0]\n            else:\n                regex_stem = possible_morpheme[0][0] + \"t\"\n            \n            if re.fullmatch(regex_stem, \"\".join(ipa2)):\n                if n_matches &lt; 20 and \"\".join(ipa2) not in seen:\n                    n_matches += 1\n                    seen |= {\"\".join(ipa2)}\n                    print(\"\".join(ipa2), f\"({w2})\", \"+\", \"ʃən\", \"=\", \"\".join(ipa1), f\"({w1})\")\n                else:\n                    break\n\n    if n_matches &gt;= 20:   \n        print(\"...\")\n        break\n\næbdəkt (abduct) + ʃən = æbdəkʃən (abduction)\nəbɔɹt (abort) + ʃən = əbɔɹʃən (abortion)\næbsəlut (absolut) + ʃən = æbsəluʃən (absolution)\næbstɹækt (abstract) + ʃən = æbstɹækʃən (abstraction)\nækt (act) + ʃən = ækʃən (action)\nədɪkt (addict) + ʃən = ədɪkʃən (addiction)\nədmɪt (admit) + ʃən = ədmɪʃən (admission(1))\nədɑpt (adopt) + ʃən = ədɑpʃən (adoption)\nædɝ (adder) + ʃən = ædɝeɪʃən (adoration)\nædʒəl (agile) + ʃən = ædʒəleɪʃən (adulation)\nəfɛkt (affect) + ʃən = əfɛkʃən (affection)\nəflɪkt (afflict) + ʃən = əflɪkʃən (affliction)\neɪliən (alien) + ʃən = eɪliəneɪʃən (alienation)\nɔltɝ (altar) + ʃən = ɔltɝeɪʃən (alteration)\nəmælgəm (amalgam) + ʃən = əmælgəmeɪʃən (amalgamation)\nænəm (annum) + ʃən = ænəmeɪʃən (animation)\nænɛks (annex) + ʃən = ænɛkseɪʃən (annexation)\nænt (ant) + ʃən = ænʃən (anshan)\næpəl (appel) + ʃən = æpəleɪʃən (appalachian(1))\nəsɛnt (ascent) + ʃən = əsɛnʃən (ascension)\n...\nTo get much better than this, we’d need to start matching on the orthographic representation as well–e.g. matching the ion at the end of the orthographic representation of the word, thus filtering things like /æt/ + /ʃən/ = /æʃən/. One thing we’ll still miss are cases like adoration, where there is a vowel quality change (which is consequently why we get adder + ion = adoration currently). To handle those cases, we would need to account for the conditions under which vowel quality changes, which we could do in principle using regular expressions but which I won’t do here."
  },
  {
    "objectID": "formal-and-practical-preliminaries/regular-expressions/groups-and-greediness.html#footnotes",
    "href": "formal-and-practical-preliminaries/regular-expressions/groups-and-greediness.html#footnotes",
    "title": "Groups and Greediness",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe dual of $ for the beginning of the string is ^. Note that this looks like the negation symbol we saw earlier. It is different in that that symbol must be preceded by [ to be interpreted as negation. Anywhere else ^ means the beginning of a string. That in turn means that putting a bare ^ anywhere besides the beginning of a regular expression is going to result in a regular expression that evaluates to the empty set.↩︎"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/index.html",
    "href": "formal-and-practical-preliminaries/text-normalization/index.html",
    "title": "Overview",
    "section": "",
    "text": "Reading\n\n\n\nJurafsky and Martin (2023, Ch. 2.2-2.4) on text normalization.\n\n\nIn this submodule, we’re going to be looking at a few key text normalization methods. These methods are crucial to know from a practical standpoint, since most language data you will work with won’t come nicely formatted: it will be a big blob of text that you’ll need to transform in various ways to make it usable.\nWe’ll specifically focus on two basic methods: tokenization and lemmatization. Both turn out to be surprisingly nontrivial to implement–even for a very well-studied like English–and so we won’t be implementing full solutions here. I mainly want to tell you what they are and show you good off-the-shelf tools for implementing them.\n\n\n\n\n\n\nReferences\n\nJurafsky, Daniel, and James H. Martin. 2023. Speech and Language Processing."
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/tokenization.html",
    "href": "formal-and-practical-preliminaries/text-normalization/tokenization.html",
    "title": "Tokenization",
    "section": "",
    "text": "Tokenization is the task of segmenting a string into tokens. A token often corresponds to a word (in the context of computational linguistics), though it may correspond to other kinds of strings, such as punctuation.\nFormally, we say that a tokenizer maps a string (in \\(\\Sigma^*\\)) into a string of strings (in \\(\\Sigma^{**}\\)).\n\\[\\text{tokenize}: \\Sigma^* \\rightarrow \\Sigma^{**}\\]\nWe can generally do tokenization reasonably well in English by finding substrings that have white space on either side. We could do this in Python using str.split(), though a regular expression will turn out to be more useful.\nimport re\n\nnews = (\"Lin Xingzhi said that the trade war prompted the relocation of \"\n        \"factories in China to Vietnam, Laos, India, Taiwan and even Malaysia.\")\n\nre.findall(\"(?:(.*?)(?:\\s|$))+?\", news)\n\n['Lin',\n 'Xingzhi',\n 'said',\n 'that',\n 'the',\n 'trade',\n 'war',\n 'prompted',\n 'the',\n 'relocation',\n 'of',\n 'factories',\n 'in',\n 'China',\n 'to',\n 'Vietnam,',\n 'Laos,',\n 'India,',\n 'Taiwan',\n 'and',\n 'even',\n 'Malaysia.',\n '']\nNote that this implementaton of \\(\\text{tokenize}\\) is furthermore invertible:\n\\[\\text{tokenize}^{-1}: \\Sigma^{**} \\rightarrow \\Sigma^*\\]\nThat is, can recover the original string from the tokenized variant, in this case by simply joining all tokens by whitespace. Not all implementations of \\(\\text{tokenize}\\), viewed as functions from strings to strings of strings, are invertible.\nFor instance, note that, even for a short piece of text like this, we’re missing certain things we might want to capture:\nTo handle these, we might try to introduce additional tokenization rules by augmenting our regular expression.\n# this regular expression won't work for general comma-separated lists\nre.findall(\"(?:([^A-Z].*?|(?:[A-Z].*?(?:\\s?))+)(?:,?\\s|\\.$))+?\", news)\n\n['Lin Xingzhi',\n 'said',\n 'that',\n 'the',\n 'trade',\n 'war',\n 'prompted',\n 'the',\n 'relocation',\n 'of',\n 'factories',\n 'in',\n 'China',\n 'to',\n 'Vietnam',\n 'Laos',\n 'India',\n 'Taiwan',\n 'and',\n 'even',\n 'Malaysia']\nThis works reasonably well in this case, but we are actually throwing out punctuation, where generally a tokenizer will retain it as its own token. This fact implies that we cannot reconstruct the original string from it’s tokenization. And even if we were to augment our tokenizer to retain the punctuation as separate tokens…\n# this regular expression won't work for general comma-separated lists\nre.findall(\"(?:((?&lt;=\\s)[^A-Z,\\.][^,\\.]*?|(?:[A-Z][^,\\.]*?(?:\\s?))+|[,\\.])(?=,?\\s|\\.$))+?\", news)\n\n['Lin Xingzhi',\n 'said',\n 'that',\n 'the',\n 'trade',\n 'war',\n 'prompted',\n 'the',\n 'relocation',\n 'of',\n 'factories',\n 'in',\n 'China',\n 'to',\n 'Vietnam',\n ',',\n 'Laos',\n ',',\n 'India',\n ',',\n 'Taiwan',\n 'and',\n 'even',\n 'Malaysia']\n…we wouldn’t be able to unambiguously reconstruct the string, because we couldn’t be sure whether a whitespace should come before or after a punctuation mark.1 (We could devise heuristics based on our knowledge of English orthographic conventions to get close, but we couldn’t know for sure.)\nThus, if we want to be able to reconstruct the original string (and we don’t necessarily always do), we need to have \\(\\text{tokenize}\\) produce additional information in order to reconstruct the original string: usually some form of character offset, which tells us the beginning and ending points of a token relative to the beginning of the original string.\n\\[\\text{tokenization-offset}: \\Sigma^* \\rightarrow (\\mathbb{N} \\times \\mathbb{N})^{*}\\]\nWe could then use \\(\\text{tokenization-offset}\\) to produce a tokenization by indexing into the original string. (These offsets either need to cover the entire input string, or we need to know what characters to default to for any spans of the original string, such as white-space characters, we didn’t include in the tokenization.)\nIgnoring the reconstruction issue, though, we will see that the regular expression above won’t work in general. For instance, our current regular expression doesn’t handle contractions ('ll, 'd, n't, etc.), which we generally want to treat as separate tokens (since they are semantically equivalent to full words: will, would, not, etc.), and it won’t handle other kinds of punctuation (e.g. -), which could be word-internal or its own token. It also doesn’t handle names (and more generally, multiword expressions) that don’t fit a very specific format involving title-casing.\nThese issues get much more difficult when we move outside of English. For instance, consider the following Chinese text from this article.\nExample from this post.\nnews = \"林行止表示貿易戰促使在中國的工廠搬遷到越南、寮國、印度、台灣甚至是馬來西亞\"\nGoogle Translate translates the article as follows:\nLin Xingzhi said that the trade war prompted the relocation of factories in China to Vietnam, Laos, India, Taiwan and even Malaysia.\nIf you didn’t know anything about Chinese, the lack of white space here might make you think that it is highly synthetic; but it is the opposite: Chinese is highly analytic. Either way, even the a really fancy regular expression won’t work here for.\nWe could of course try splitting every character apart.\nlist(news)\n\n['林',\n '行',\n '止',\n '表',\n '示',\n '貿',\n '易',\n '戰',\n '促',\n '使',\n '在',\n '中',\n '國',\n '的',\n '工',\n '廠',\n '搬',\n '遷',\n '到',\n '越',\n '南',\n '、',\n '寮',\n '國',\n '、',\n '印',\n '度',\n '、',\n '台',\n '灣',\n '甚',\n '至',\n '是',\n '馬',\n '來',\n '西',\n '亞']\nThis doesn’t work for reasons similar to English. Just like in English, words in Chinese can be multiple characters long. For instance, 林行止 (Lin Xingzhi) and 馬來西亞 are names (Malaysia). But even worse, most characters can stand on there own as words as well. For instance, 林行止 separated by character would be the nonsense phrase “forest walk halt” and 馬來西亞 separated by character would be “horse come to Western Asia”.\nThe more general point is that good tokenization requires knowing the context of a token. This is true even in English: consider the string New York. In the context of the sentence I live in Rochester, New York, we probably want New York to be a token. But in the context of I read the New York Times this morning, we probably don’t: we want New York Times instead.\nAgain, we might be able to devise heuristics for handling this case–like take the maximal string, but it’s still not going to handle multiword expressions like run the gamut, where we’re generally not going to have orthographic conventions like title-casing to help us out: should we tokenize this as run the gamut or run, the, gamut?\nSo what should we do? For the purposes of this course, our answer is going to be “use existing packages”–largely because actually solving the tokenization problem, even in English, turns out to be really hard; and our best systems are ones that require fairly heavy machine learning-based approaches (covered in LING281/481 and LING282/482).\nI’ll discuss two of the most popular state-of-the-art packages.\nYou may have also run across the nltk. I strongly advise aginst using nltk for any of the text normalization we’re going to cover today. The packages I’ll discuss are so much better that it’s not even worth considering. (nltk can still be useful for loading particular kinds of resources like wordnet, and we will use it in Assignmnt 6.)"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/tokenization.html#stanza",
    "href": "formal-and-practical-preliminaries/text-normalization/tokenization.html#stanza",
    "title": "Tokenization",
    "section": "Stanza",
    "text": "Stanza\nTo use stanza for a particular language, we then need to download the relevant language-specific models.\n\nimport stanza\n\n# Download an English model\nstanza.download('en')\n  \n# Similarly, download a (traditional) Chinese model\nstanza.download('zh')\n\n\n\n\nINFO:stanza:Downloading default packages for language: en (English) ...\nINFO:stanza:Finished downloading models and saved to /root/stanza_resources.\nINFO:stanza:\"zh\" is an alias for \"zh-hans\"\nINFO:stanza:Downloading default packages for language: zh-hans (Simplified_Chinese) ...\nINFO:stanza:Finished downloading models and saved to /root/stanza_resources.\n\n\n\n\n\n\n\n\n\n\n\n(Source: https://bit.ly/2kb8eJU)\nWe process text using stanza.Pipelines, which contain different Processor units. The pipeline is language-specific, so again you’ll need to first specify the language. By default, Pipelines run a bunch of processors, including ones we’re not going to be looking at today. For now, we’ll specify that we just want the tokenize processor.2\n\n# Build an English pipeline, with all processors by default\nstanza_en_nlp = stanza.Pipeline(lang='en', processors = 'tokenize', use_gpu=False)\n\n# Build a Chinese pipeline, with customized processor list, and force it to use CPU\nstanza_zh_nlp = stanza.Pipeline(lang='zh', processors='tokenize', use_gpu=False)\n\nINFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\nINFO:stanza:Loading these models for language: en (English):\n========================\n| Processor | Package  |\n------------------------\n| tokenize  | combined |\n========================\n\nINFO:stanza:Use device: cpu\nINFO:stanza:Loading: tokenize\nINFO:stanza:Done loading processors!\nINFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\nINFO:stanza:\"zh\" is an alias for \"zh-hans\"\nINFO:stanza:Loading these models for language: zh-hans (Simplified_Chinese):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | gsdsimp |\n=======================\n\nINFO:stanza:Use device: cpu\nINFO:stanza:Loading: tokenize\nINFO:stanza:Done loading processors!\n\n\n\n\n\n\n\n\n\nCreating document objects\n\nen_str = \"Homelessness is to be found in the large numbers of mentally ill and substance-abusing people in the homeless population, don't you think?\"\nzh_str = \"林行止表示貿易戰促使在中國的工廠搬遷到越南、寮國、印度、台灣甚至是馬來西亞\"\n\n\n## Our earlier algo using regex function\nimport re\nprint(re.findall('[^ ]+', en_str))  ## based on whitespace seperation\n\n['Homelessness', 'is', 'to', 'be', 'found', 'in', 'the', 'large', 'numbers', 'of', 'mentally', 'ill', 'and', 'substance-abusing', 'people', 'in', 'the', 'homeless', 'population,', \"don't\", 'you', 'think?']\n\n\n\n# Processing English text\nen_doc = stanza_en_nlp(en_str)\nprint(type(en_doc))\n\n# Processing Chinese text\nzh_doc = stanza_zh_nlp(zh_str)\nprint(type(zh_doc))\n\n&lt;class 'stanza.models.common.doc.Document'&gt;\n&lt;class 'stanza.models.common.doc.Document'&gt;\n\n\n\nfor sent in en_doc.sentences:\n    stanza_en_sent = [word.text for word in sent.words]\n    print(stanza_en_sent)\n\n['Homelessness', 'is', 'to', 'be', 'found', 'in', 'the', 'large', 'numbers', 'of', 'mentally', 'ill', 'and', 'substance', '-', 'abusing', 'people', 'in', 'the', 'homeless', 'population', ',', 'do', \"n't\", 'you', 'think', '?']\n\n\n\nfor i, sent in enumerate(zh_doc.sentences):\n    stanza_zh_sent = [word.text for word in sent.words]\n    print(stanza_zh_sent)\n\n['林', '行止', '表示', '貿易', '戰', '促使', '在', '中國', '的', '工廠', '搬遷', '到', '越南', '、', '寮國', '、', '印度', '、', '台灣', '甚至是', '馬', '來西', '亞']"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/tokenization.html#spacy",
    "href": "formal-and-practical-preliminaries/text-normalization/tokenization.html#spacy",
    "title": "Tokenization",
    "section": "Spacy",
    "text": "Spacy\nAs with stanza, when using spacy, we need to download language-specific models.\n\nimport spacy\n\n!python -m spacy download en_core_web_sm\n!python -m spacy download zh_core_web_sm\n!python -m spacy download it_core_news_sm\n\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n\n\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n2023-02-06 17:17:12.352179: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting en-core-web-sm==3.4.1\n  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 12.8/12.8 MB 40.8 MB/s eta 0:00:00\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.4)\nRequirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.25.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (57.4.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.8)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.4.5)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.10.4)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.0.12)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.4)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.8)\nRequirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.21.6)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.11.3)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.10.1)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.7.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.64.1)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.0.9)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (6.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (23.0)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (3.3.0)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (8.1.7)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.7)\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.4.0)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (4.0.0)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (1.24.3)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2022.12.7)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.10)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.0.4)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (7.1.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;en-core-web-sm==3.4.1) (2.0.1)\n✔ Download and installation successful\nYou can now load the package via spacy.load('en_core_web_sm')\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n2023-02-06 17:17:29.485403: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting zh-core-web-sm==3.4.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.4.0/zh_core_web_sm-3.4.0-py3-none-any.whl (48.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 48.4/48.4 MB 16.3 MB/s eta 0:00:00\nCollecting spacy-pkuseg&lt;0.1.0,&gt;=0.0.27\n  Downloading spacy_pkuseg-0.0.32-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 21.7 MB/s eta 0:00:00\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /usr/local/lib/python3.8/dist-packages (from zh-core-web-sm==3.4.0) (3.4.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (57.4.0)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.4.5)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (8.1.7)\nRequirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (1.21.6)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (4.64.1)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (6.3.0)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (3.0.8)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (0.7.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.11.3)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.25.1)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (1.0.9)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (1.10.4)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.0.8)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (1.0.4)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (3.3.0)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (0.10.1)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (3.0.12)\nRequirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (0.10.1)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (23.0)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.0.7)\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (4.4.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2022.12.7)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (1.24.3)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (4.0.0)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.10)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (0.0.4)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (7.1.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;zh-core-web-sm==3.4.0) (2.0.1)\nInstalling collected packages: spacy-pkuseg, zh-core-web-sm\nSuccessfully installed spacy-pkuseg-0.0.32 zh-core-web-sm-3.4.0\n✔ Download and installation successful\nYou can now load the package via spacy.load('zh_core_web_sm')\n/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n2023-02-06 17:17:43.205211: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\nLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\nCollecting it-core-news-sm==3.4.0\n  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.4.0/it_core_news_sm-3.4.0-py3-none-any.whl (13.0 MB)\n     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 72.1 MB/s eta 0:00:00\nRequirement already satisfied: spacy&lt;3.5.0,&gt;=3.4.0 in /usr/local/lib/python3.8/dist-packages (from it-core-news-sm==3.4.0) (3.4.4)\nRequirement already satisfied: spacy-legacy&lt;3.1.0,&gt;=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (3.0.12)\nRequirement already satisfied: requests&lt;3.0.0,&gt;=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.25.1)\nRequirement already satisfied: preshed&lt;3.1.0,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (3.0.8)\nRequirement already satisfied: murmurhash&lt;1.1.0,&gt;=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (1.0.9)\nRequirement already satisfied: pathy&gt;=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (0.10.1)\nRequirement already satisfied: langcodes&lt;4.0.0,&gt;=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (3.3.0)\nRequirement already satisfied: typer&lt;0.8.0,&gt;=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (0.7.0)\nRequirement already satisfied: spacy-loggers&lt;2.0.0,&gt;=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (1.0.4)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (1.10.4)\nRequirement already satisfied: thinc&lt;8.2.0,&gt;=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (8.1.7)\nRequirement already satisfied: catalogue&lt;2.1.0,&gt;=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.0.8)\nRequirement already satisfied: srsly&lt;3.0.0,&gt;=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.4.5)\nRequirement already satisfied: smart-open&lt;7.0.0,&gt;=5.2.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (6.3.0)\nRequirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (23.0)\nRequirement already satisfied: tqdm&lt;5.0.0,&gt;=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (4.64.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (57.4.0)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.11.3)\nRequirement already satisfied: wasabi&lt;1.1.0,&gt;=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (0.10.1)\nRequirement already satisfied: cymem&lt;2.1.0,&gt;=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.0.7)\nRequirement already satisfied: numpy&gt;=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (1.21.6)\nRequirement already satisfied: typing-extensions&gt;=4.2.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,&lt;1.11.0,&gt;=1.7.4-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (4.4.0)\nRequirement already satisfied: chardet&lt;5,&gt;=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (4.0.0)\nRequirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2022.12.7)\nRequirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.10)\nRequirement already satisfied: urllib3&lt;1.27,&gt;=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests&lt;3.0.0,&gt;=2.13.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (1.24.3)\nRequirement already satisfied: confection&lt;1.0.0,&gt;=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (0.0.4)\nRequirement already satisfied: blis&lt;0.8.0,&gt;=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc&lt;8.2.0,&gt;=8.1.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (0.7.9)\nRequirement already satisfied: click&lt;9.0.0,&gt;=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer&lt;0.8.0,&gt;=0.3.0-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (7.1.2)\nRequirement already satisfied: MarkupSafe&gt;=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2-&gt;spacy&lt;3.5.0,&gt;=3.4.0-&gt;it-core-news-sm==3.4.0) (2.0.1)\nInstalling collected packages: it-core-news-sm\nSuccessfully installed it-core-news-sm-3.4.0\n✔ Download and installation successful\nYou can now load the package via spacy.load('it_core_news_sm')\n\n\nWe then need to load the models.\n\nspacy_en_nlp = spacy.load(\"en_core_web_sm\") #disable=[\"tagger\", \"parser\", \"ner\"]\nspacy_zh_nlp = spacy.load(\"zh_core_web_sm\") #disable=[\"tagger\", \"parser\", \"ner\"]\n\n\nCreating document objects\n\nen_doc = spacy_en_nlp(en_str)\nspacy_en_sent = [w.text for w in en_doc]\nprint(spacy_en_sent)\n\n['Homelessness', 'is', 'to', 'be', 'found', 'in', 'the', 'large', 'numbers', 'of', 'mentally', 'ill', 'and', 'substance', '-', 'abusing', 'people', 'in', 'the', 'homeless', 'population', ',', 'do', \"n't\", 'you', 'think', '?']\n\n\n\nzh_doc = spacy_en_nlp(zh_str)\nspacy_zh_sent = [w.text for w in zh_doc]\nprint(spacy_zh_sent)\n\n['林行止表示貿易戰促使在中國的工廠搬遷到越南、寮國、印度、台灣甚至是馬來西亞']"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/tokenization.html#footnotes",
    "href": "formal-and-practical-preliminaries/text-normalization/tokenization.html#footnotes",
    "title": "Tokenization",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe regular expression I use here has some symbols we didn’t cover in the Regular Expressions submodule: ?= and ?&lt;=. These are called the lookahead and lookbehind operators, which have negative variants ?! and ?&lt;!, respectively. Basically, what these do is to say that, we must match the pattern ... within (?=...) or (?&lt;=...), but that in the course of matching that pattern, we shouldn’t forego the possibility that we can match that pattern by some expression in the future (lookahead) or have matched that pattern in the past (lookbehind). Another way to conceive of what these things do is in terms of a pointer into a string: the process of matching can be thought of in terms to moving a pointer through the string to say what we have currently match (everything behind the pointer) and what we still have left to look at. The lookahead operator says “match my pattern against whatever is right after the pointer but don’t move the pointer to do the match”, while the lookbehind operator says “match my pattern against whatever is right before the pointer but don’t move the pointer backward to do the match”. The negative versions say something similar, except they say: “you should be able to match my pattern and to check that, don’t move the pointer”.↩︎\nWe also need to specify whether we want to use a GPU, since the models in stanza are deep learning models and can benefit from them. We won’t use them hre by setting use_gpu=False.↩︎"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/sentence-separation.html",
    "href": "formal-and-practical-preliminaries/text-normalization/sentence-separation.html",
    "title": "Sentence Separation",
    "section": "",
    "text": "When working with actually text, that text won’t come with clear sentence boundaries. So in addition to tokenization, you usually want to do sentence separation.\nWhen working with standard English orthography, you can usually look at cues such as periods, question marks, exclamation points, etc. to tell you when a sentence ends.\n\nen_str_canonical = \"The fictional Pandoran biosphere, from James Cameron's Avatar, teems with a biodiversity of bioluminescent species ranging from hexapodal animals to other types of exotic fauna and flora. The Pandoran ecology forms a vast neural network spanning the entire lunar surface into which the Na'vi and other creatures can connect. The strength of this collective consciousness is illustrated when the human invaders are defeated in battle by the Pandoran ecology, after the Na'vi were nearly defeated. Cameron utilized a team of expert advisors in order to make the various examples of fauna and flora as scientifically feasible as possible.\"\n\nText from other sources (e.g. twitter or reddit) can be significantly more difficult to correctly sentence separate, depending on the conventions of the community, and you sometimes need a model specific to that source.\n\nStanza\n\n# Processing English text\nen_doc = stanza_en_nlp(en_str_canonical)\n\nfor sent in en_doc.sentences:\n   print(' '.join([w.text for w in sent.words]))\n\nThe fictional Pandoran biosphere , from James Cameron 's Avatar , teems with a biodiversity of bioluminescent species ranging from hexapodal animals to other types of exotic fauna and flora .\nThe Pandoran ecology forms a vast neural network spanning the entire lunar surface into which the Na'vi and other creatures can connect .\nThe strength of this collective consciousness is illustrated when the human invaders are defeated in battle by the Pandoran ecology , after the Na'vi were nearly defeated .\nCameron utilized a team of expert advisors in order to make the various examples of fauna and flora as scientifically feasible as possible .\n\n\n\n\nSpacy\n\nfor sent in en_doc.sentences:\n   print(' '.join([w.text for w in sent.words]))\n\nThe fictional Pandoran biosphere , from James Cameron 's Avatar , teems with a biodiversity of bioluminescent species ranging from hexapodal animals to other types of exotic fauna and flora .\nThe Pandoran ecology forms a vast neural network spanning the entire lunar surface into which the Na'vi and other creatures can connect .\nThe strength of this collective consciousness is illustrated when the human invaders are defeated in battle by the Pandoran ecology , after the Na'vi were nearly defeated .\nCameron utilized a team of expert advisors in order to make the various examples of fauna and flora as scientifically feasible as possible ."
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/lemmatization.html",
    "href": "formal-and-practical-preliminaries/text-normalization/lemmatization.html",
    "title": "Lemmatization",
    "section": "",
    "text": "Lemmatization is the task of determining a canonicalized form of some token. When these tokens are words, we often say that a lemmatizer determines that word’s root form. For instance, The words am, are, and is have the same root form (or lemma) be; and the words dinner and dinners both have the lemma dinner. Formally, we might model lemmatiztion as a function from strings (^*) to some subset of those strings that we will call the roots (\\(R\\)).\n\\[\\text{lemmatize}: \\Sigma^* \\rightarrow R \\subset \\Sigma^*\\]\nIn general, lemmatization is assumed to only strip inflectional morphology from a word. For instance, derivations would be lemmatized to derivation rather than derive, since ation is a derivational morpheme, which we can see from the fact that it changes the words grammatical ctegory from verb to noun.\nLemmatization is definitionally a destructive procedure. If we want to retain the morphological structure of a word, we would need to do morphological analysis. Usually, morphological analysis is assumed to tokenize a string into a subset of strings of strings that we will call the morphemes (\\(M\\)); and usually we assume that these morphemes include both inflectional and derivational morphemes.\n\\[\\text{analyze-morphology}: \\Sigma^* \\rightarrow M^* \\subset \\Sigma^{**}\\]\nConstructing a morphological analyzer (especially when we want the analyzer to handle derivational morphology correctly) is a much harder problem than constructing a lemmatizer, and there are far fewer off-the-shelf sytems for doing it. In LING281/481 and LING282/482, I cover how to build such a system.\nTo get the lemmatized forms of words, we need to modify the processors list in the stanza.Pipeline.\n# Build an English pipeline, with all processors by default\nstanza_en_nlp = stanza.Pipeline(lang='en', processors = 'tokenize,lemma', use_gpu=False)\n\n# Build a Chinese pipeline, with customized processor list, and force it to use CPU\nstanza_zh_nlp = stanza.Pipeline(lang='zh', processors='tokenize,lemma', use_gpu=False)\n\nINFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\nINFO:stanza:Loading these models for language: en (English):\n========================\n| Processor | Package  |\n------------------------\n| tokenize  | combined |\n| lemma     | combined |\n========================\n\nINFO:stanza:Use device: cpu\nINFO:stanza:Loading: tokenize\nINFO:stanza:Loading: lemma\nINFO:stanza:Done loading processors!\nINFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\nINFO:stanza:\"zh\" is an alias for \"zh-hans\"\nINFO:stanza:Loading these models for language: zh-hans (Simplified_Chinese):\n=======================\n| Processor | Package |\n-----------------------\n| tokenize  | gsdsimp |\n| lemma     | gsdsimp |\n=======================\n\nINFO:stanza:Use device: cpu\nINFO:stanza:Loading: tokenize\nINFO:stanza:Loading: lemma\nINFO:stanza:Done loading processors!\n# Processing English text\nen_doc = stanza_en_nlp(en_str)\nprint(type(en_doc))\n\n# Processing Chinese text\nzh_doc = stanza_zh_nlp(zh_str)\nprint(type(zh_doc))\n\n&lt;class 'stanza.models.common.doc.Document'&gt;\n&lt;class 'stanza.models.common.doc.Document'&gt;\nfor sent in en_doc.sentences:\n    stanza_en_sent = [(word.text, word.lemma) for word in sent.words]\n    print(stanza_en_sent)\n\n[('Homelessness', 'homelessness'), ('is', 'be'), ('to', 'to'), ('be', 'be'), ('found', 'find'), ('in', 'in'), ('the', 'the'), ('large', 'large'), ('numbers', 'number'), ('of', 'of'), ('mentally', 'mentally'), ('ill', 'ill'), ('and', 'and'), ('substance', 'substance'), ('-', '-'), ('abusing', 'abuse'), ('people', 'people'), ('in', 'in'), ('the', 'the'), ('homeless', 'homeless'), ('population', 'population'), (',', ','), ('do', 'do'), (\"n't\", 'not'), ('you', 'you'), ('think', 'think'), ('?', '?')]\nfor i, sent in enumerate(zh_doc.sentences):\n    stanza_zh_sent = [(word.text, word.lemma) for word in sent.words]\n    print(stanza_zh_sent)\n\n[('林', '林'), ('行止', '行止'), ('表示', '表示'), ('貿易', '貿易'), ('戰', '戰'), ('促使', '促使'), ('在', '在'), ('中國', '中國'), ('的', '的'), ('工廠', '工廠'), ('搬遷', '搬遷'), ('到', '到'), ('越南', '越南'), ('、', '、'), ('寮國', '寮國'), ('、', '、'), ('印度', '印度'), ('、', '、'), ('台灣', '台灣'), ('甚至是', '甚至是'), ('馬', '馬'), ('來西', '來西'), ('亞', '亞')]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/lemmatization.html#spacy",
    "href": "formal-and-practical-preliminaries/text-normalization/lemmatization.html#spacy",
    "title": "Lemmatization",
    "section": "Spacy",
    "text": "Spacy\n\nen_str = \"I don't like him.\"\nen_doc = spacy_en_nlp(en_str)\nspacy_en_sent = [(w.text, w.lemma_) for w in en_doc]\nprint(spacy_en_sent)\n\n[('I', 'I'), ('do', 'do'), (\"n't\", 'not'), ('like', 'like'), ('him', 'he'), ('.', '.')]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/text-normalization/lemmatization.html#named-entity-recognition",
    "href": "formal-and-practical-preliminaries/text-normalization/lemmatization.html#named-entity-recognition",
    "title": "Lemmatization",
    "section": "Named Entity Recognition",
    "text": "Named Entity Recognition\nVanilla tokenization algorithms don’t always handle multi-word expressions (MWEs) well—e.g. idioms like “kick the bucket” or names. Names are somewhat easier than idioms for various reasons.\nWe can use the spacy package to extract entities from a text string as follows:\n\nen_str = '''\nThis is an example string. Harry Potter is a name of an individual. Ron Weasley and Draco Malfoy are his school-mates.\nVoldemort is another individual who has an army of Death Eaters.\nHe belongs to the Hogwart school. Nobody actually know if the school actually exists. But, nevertheless, people love the concept.\nDo you think the NER-System, Spacy, will capture all the named entities in this string correctly? The New York Times certainly does.\n'''\n\nen_doc = spacy_en_nlp(en_str)\nen_doc.ents\n\n(Harry Potter,\n Ron Weasley,\n Draco Malfoy,\n Voldemort,\n Death Eaters,\n NER-System,\n The New York Times)\n\n\nAs we can see above, the NER system is pretty good at finding the named entities in a text"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/index.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/index.html",
    "title": "Overview",
    "section": "",
    "text": "Reading\n\n\n\nMarcus, Santorini, and Marcinkiewicz (1993) on building the first large-scale constituency treebank: the Penn Treebank.\n\n\nIn this submodule, we’ll cover basic concepts relevant to how to work with annotated corpora—in particular, treebanks—by developing a system of classes for representing those corpora and their annotations from the ground up.\n\n\n\n\n\n\nReferences\n\nMarcus, Mitchell P., Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. “Building a Large Annotated Corpus of English: The Penn Treebank.” Edited by Julia Hirschberg. Computational Linguistics 19 (2): 313–30. https://aclanthology.org/J93-2004."
  },
  {
    "objectID": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/index.html",
    "href": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/index.html",
    "title": "Overview",
    "section": "",
    "text": "Reading\n\n\n\nJurafsky and Martin (2023, Ch. 2.5) on edit distance and string alignment.\n\n\nIn the regular expressions submodule, we talked about how to describe a language on \\(\\Sigma\\)–i.e. a set of strings \\(L \\subseteq \\Sigma^*\\)–using a string from some other language–the set of regular expressions. This approach to describing languages allowed us to do a variety of strings–most fundamentally, finding all and only string that are in the language described by the regular expression.\nSometimes, however, we don’t know exactly what set of strings we are searching for, but we know we are interested in strings in \\(\\Sigma^*\\) that are similar to some string of interest that’s also in \\(\\Sigma^*\\). For instance, maybe we know we want morphological variants of the word abstract, like abstraction, abstracts, abstracted, etc. This task is often referred to as fuzzy search.\nIn this context, it can be really useful to have a way of measuring the distance/similarity between two strings, where this measurement might help us determine that abstract and abstraction are more similar than abstract and obstruction (and maybe also that abstract and abstraction are exactly as similar as obstruct and obstruction).\nIn addition to deriving a measurement of distance/similarity, we often also want to be able to determine which elements of the two strings give rise to that similarity. For instance, we may furthermore want to know that abstract corresponds to the initial substring in abstraction; and therefore, that abstraction is abstract with a suffix ion on it. This task is known as string alignment, and it turns out that the methods we use to compute distance can be used to compute such alignments as well.\n\n\n\n\n\n\nReferences\n\nJurafsky, Daniel, and James H. Martin. 2023. Speech and Language Processing."
  },
  {
    "objectID": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/distance-in-the-abstract.html",
    "href": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/distance-in-the-abstract.html",
    "title": "Distance in the abstract",
    "section": "",
    "text": "The first question we need to ask is “what could it mean for a string to be close to (or far from) another one?” To answer this, let’s consider what it means to be close in other domains.\n\nReal numbers\nSuppose we have two numbers \\(a, b \\in \\mathbb{R}\\). How might we define the distance between them?\nWe could compute equality.\n\\[d_\\text{eq}(a, b) = \\begin{cases} 0 & \\text{if } a = b\\\\1 & \\text{otherwise}\\end{cases}\\]\nOr we could compute the absolute difference.\n\\[d_\\text{abs}(a, b) = |a - b|\\]\nMore generally, we could define a distance on elements of a set \\(E\\) to be any function \\(d: E \\times E \\rightarrow \\mathbb{R}_+\\) that satisfies a few constraints:\n\nThe distance between an object and itself is \\(0\\): \\(d(x, x) = 0\\)\nThe distance between an object and any other object is not \\(0\\): \\(d(x, y) &gt; 0\\) if \\(x \\neq y\\)\nThe order of comparison doesn’t matter: \\(d(x, y) = d(y, x)\\) (symmetry)\nThe distance between two elements is no more than the distance between the first element and any other element plus the distance between the second element and that other element: \\(d(x, y) \\leq d(x, z) + d(z, y)\\) (subadditivity or triangle inequality)\n\nMeasure Theory and General Topology study (in part) functions like \\(d\\) and more general classes of functions that arise from lifting some of the constraints 1-4. But for our purposes, this notion of a distance is useful.\n\n\nVectors of real numbers\nNow, suppose we have two vectors of real numbers \\(\\mathbf{a}, \\mathbf{b} \\in \\mathbb{R}^M\\)—basically, tuples of real numbers. How might we define the distance between them?\nWell, when \\(M=2\\), that’s just a point in a two-dimensional plane.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\na = [0,0]\nb = [1,1]\n\nab = np.array([a, b])\n\nab\n\narray([[0, 0],\n       [1, 1]])\n\n\n\n_ = plt.scatter(ab[:,0], ab[:,1])\n\n\n\n\nWe could again check for equality by checking for equality on each dimension.\n\\[d_\\text{eq-vec}(\\mathbf{a}, \\mathbf{b}) = \\begin{cases} 0 & \\text{if } d_\\text{eq}(a_1, b_1) = 0 \\text{ and } d_\\text{eq}(a_2, b_2) = 0\\\\1 & \\text{otherwise}\\end{cases}\\]\nOr we could again check for absolute difference, by checking for absolute difference on each dimension and then summing.\n\\[d_\\text{abs-vec}(\\mathbf{a}, \\mathbf{b}) = d_\\text{abs}(a_1, b_1) + d_\\text{abs}(a_2, b_2) = |a_1 - b_1| + |a_2 - b_2|\\]\nThis is called the Manhattan (or city block) distance.\n\n_ = plt.plot([a[0], b[0]], [a[1], a[1]])\n_ = plt.plot([b[1], b[1]], [a[1], b[1]])\n_ = plt.scatter(ab[:,0], ab[:,1])\n\n\n\n\nOr we could compute the Euclidean distance.\n\\[d_\\text{euc}(a, b) = \\sqrt{(a_1 - b_1)^2 + (a_2 - b_2)^2}\\]\n\n_ = plt.scatter(ab[:,0], ab[:,1])\n_ = plt.plot(ab[:,0], ab[:,1])\n\n\n\n\nWe could get even fancier by noticing that the absolute difference and Euclidean distance can be generalized—e.g. to the Minkowski \\(p\\)-norm for some \\(p\\).\n\\[d_{\\text{mink}, p}(\\mathbf{a}, \\mathbf{b}) = \\sqrt[p]{\\sum_i |a_i - b_i|^p}\\]\nwhere absolute difference is the case where \\(p=1\\) and the Euclidean distance is the case where \\(p=2\\). (And this isn’t even near as fancy as we can get.)\n\n\nDifficulties with strings\nThe reason for going through all of this is to notice:\n\nThere are many different reasonable notions of the distance between two things, even when we’re talking about relatively simple things like points in a plane.\nOne we have some reasonable notion, we can often parameterize it, providing us with an often infinite variety of new metrics.\n\nBoth of these notions are import for understanding string distance because we can think of strings as vectors—sort of like the vectors we discussed above.\nThere are two main differences between strings and real-valued vectors though:\n\nThe elements of a string are not necessarily ordered. And even if we treat them as ordered (e.g. by hashing them to their ASCII code or unicode) it’s not clear that the relevant ordering is useful in any way, especially for the computational linguist. (We could of course define lexicographic distance or something like that, but the order of the alphabet is arbitrary and doesn’t reallt reveal any interesting linguistic properties.)\nWe can only compare real-valued vectors of the same dimensions with the distance metrics we just defined, but we often want to compare strings of different dimensions (i.e. lengths).\n\nTo understand how to deal with the these issue, it’s useful to first start with boolean “vectors” because strings are just a generalization of boolean vectors.\n\n\n“Vectors” of booleans\nA boolean “vector” (I’m say shortly why I include quotes) is some tuple of boolean values \\(\\mathbf{a} \\in \\mathbb{B}^M = \\{\\top, \\bot\\}^M\\). Notice that the case where \\(M=2\\) gives us the vertices of a square with sides of length 1.\n\na = [0,0]\nb = [0,1]\nc = [1,1]\nd = [1,0]\n\nabcd = np.array([a, b, c, d])\nabcda = np.array([a, b, c, d, a])\n\n_ = plt.plot(abcda[:,0], abcda[:,1])\n_ = plt.scatter(abcd[:,0], abcd[:,1])\n\n\n\n\nIn the case where \\(M=3\\), we get a cube; \\(M=4\\) gives us a tesseract, etc. More generally, we refer to anything of higher dimension than a cube as an \\(M\\)-hypercube.\nSo how do we compute distance on boolean vectors? Let’s start with equality: how do we compute it? A natural way is the biconditional:\n\\(d_{eq}(a, b) = \\begin{cases}0 & \\text{if } a \\leftrightarrow b \\\\1 & \\text{otherwise}\\end{cases}\\)\nThis may look backward, but remember that (i) \\(d\\) is supposed to be a distance, so being equal means having no distance; and (ii) \\(d\\) has a real-valued codomain (even if it’s range is just \\(\\{0, 1\\}\\). Indeed, \\(d\\) actually just turns out to be isomorphic to XOR, which is just the negation of the biconditional. (It is crucially not equivalent to XOR or the biconditional, because \\(d\\) has \\(\\mathbb{R}\\) as the codomain and XOR has \\(\\mathbb{B}\\) as its codomain.)\nNow that we have a notion of distance for booleans, we can do the same thing we did for the real numbers:\n\\[d_\\text{eq-vec}(\\mathbf{a}, \\mathbf{b}) = \\begin{cases} 0 & \\text{if } d_\\text{eq}(a_1, b_1) = 0 \\text{ and } d_\\text{eq}(a_2, b_2) = 0\\\\1 & \\text{otherwise}\\end{cases}\\]\nAnd notice that if \\(M&gt;2\\), we can actually just generalize this to:\n\\[d_\\text{eq-vec}(\\mathbf{a}, \\mathbf{b}) = \\begin{cases} 0 & \\text{if } \\bigwedge_i d_\\text{eq}(a_i, b_i) = 0\\\\1 & \\text{otherwise}\\end{cases}\\]\nSo now I can say why I’m using quotes around vector. Technically, vectors must have elements that come from a field. If we use the notion of distance and the notion of combining distance mentioned above, we do get a field: the Galois Field. You don’t need to know this, but if I had chosen, say, conjunction for our notion of equality, we would provably not have a field.\nSo here’s a question: can we similarly define a reasonable distance that acts more like absolute difference? Sure. Instead of asking for strict equality at the vector level, we can count up the distances at the element level.\n\\[d_\\text{abs-vec}(\\mathbf{a}, \\mathbf{b}) = \\sum_i d_\\text{eq}(a_i, b_i)\\]\nThis is alternatively known as the Hamming distance.\nSo here’s why the thing about \\(M=2\\) being a square, \\(M=3\\) being a cube, etc. matters: basically what we’re doing here is counting how many sides of the square, cube, etc. we have to travel to get from one point to the other. It’s straightforward to check that this satisfies the distance constraints.\nThe thing to notice here is that we did perfectly well here without an ordering on the boolean elements because we know when they will be equal: basically, when the biconditional holds. Further, this didn’t force us to only have a 0-1 distance output, since we can count the mismatches.\nOkay, so what we wanted to compare variable length boolean vectors? So what if we had a vector \\(\\mathbf{a} \\in \\mathbb{B}^2\\) and another \\(\\mathbf{b} \\in \\mathbb{B}^3\\)?\nLet’s return to the idea about squares and cubes: \\(\\mathbf{a}\\) would be a vertex of a square while \\(\\mathbf{b}\\) would be a vertex of a cube. But here’s the trick: squares are just one side of a cube. So how can we leverage that?\nThe basic idea is to ask “what if I thought of \\(\\mathbf{a}\\) as actually denoting a vertex of the cube that \\(\\mathbf{b}\\) is a vertex of? Then, I could compute the distance between the two using whatever distance I already defined.”\nWhere this gets complicated is in how exactly you decide to “think of” (map) \\(\\mathbf{a}\\) as a cube vertex. Here’s a few ways:\nIf you’re mapping \\(\\mathbf{a}\\) from \\(\\mathbb{B}^M\\) to \\(\\mathbb{B}^N\\), where \\(N&gt;M\\), for comparison to \\(\\mathbf{b} \\in \\mathbb{B}^N\\)…\n\nAdd \\(N-M\\) \\(\\bot\\)s (or \\(\\top\\)s) to the beginning or end of \\(\\mathbf{a}\\)\nAdd the first (or last) \\(N-M\\) elements of \\(\\mathbf{b}\\) to the beginning (or end) of \\(\\mathbf{a}\\)\nCopy \\(N-M\\) elements of \\(\\mathbf{b}\\) into \\(\\mathbf{a}\\) (in order), slotting them between any two elements of \\(\\mathbf{a}\\) so that the distance between the two is minimized.\n\nThese all involve choosing a “face” of the higher dimensional thing to compare \\(\\mathbf{a}\\) to \\(\\mathbf{b}\\) on. The first just chooses the same face every time. The second chooses the the face dependent on \\(\\mathbf{b}\\) and will always result in a distance that is no longer than the one computed for the first, but it is not necessarily optimal. The last one will give us the shortest distance possible (by definition), but it is significantly more complicated to compute. You can get a sense for this by considering how many different ways you can spin a cube around to try to match a face.\nOf course, we could also go in the other direction, instead of trying to think of \\(\\mathbf{a}\\) as a cube vertex, we could try to think of \\(\\mathbf{b}\\) as a square vertex. This entails “flattening” the cube in some direction. But then the same challenge comes up: which direction do you flatten in? You again basically have the same choices, except that, when you’re deleting instead of adding, 1 and 2 end up looking the same.\n\nDelete \\(N-M\\) elements from the beginning or end of \\(\\mathbf{b}\\).\nDelete \\(N-M\\) elements from the beginning or end of \\(\\mathbf{b}\\).\nDelete \\(N-M\\) elements from \\(\\mathbf{b}\\) so that the distance between \\(\\mathbf{a}\\) and \\(\\mathbf{b}\\) is minimized.\n\nBut there’s actually a problem with the first way: the direction you choose can give you different answers! This is because the deletion method will give distances that are at most \\(M\\), whereas the addition method will give distances that are at most \\(N\\). That’s bad because then you don’t have something that looks like a distance anymore, since remember that \\(d(\\mathbf{a}, \\mathbf{b}) = d(\\mathbf{b}, \\mathbf{a})\\). To fix this, we could always define the distance to only delete or only add, but the problem still remains that methods one and two require us to should what to add arbitrarily.\nMethod 3 doesn’t turn out to have this directionality problem (as long as either addition and deletion don’t themselves add to the distance or they add the same amount), since you can always select your additions so that the added elements always match. And even if it did, we could back off to always moving from the smaller vector to the larger vector (or vice versa), with whatever penalty structure we wanted.\nThe problem is figuring out what the best way of inserting or deleting elements is, since regardless of which way you go, you always have \\({N \\choose N-M}\\) possible choices (in the deletion case) and \\({M \\choose N-M} \\times {N \\choose N-M}\\) in the insertion case (since you need to not only decide where to insert, but always where to insert from). And even in the deletion case, if you consider all of these choices naively, you end up having check for the equality of \\(M\\) elements, resulting in \\({N \\choose N-M} \\times M\\) equality checks.\nIt turns out there’s a better way, which we’ll see in a second. But first, how does any of this relate to strings?\n\n\nStrings\nEverything we just discussed about boolean vectors can be thought of in terms of strings by thinking of boolean vectors as strings from an alphabet with two elements: \\(\\top\\) and \\(\\bot\\). And indeed, Hamming distance (the analogue of absolute difference we talked about earlier) is straightforwardly defined for strings by replacing the element-wise biconditional with element-wise equality.\nThe reason to go through boolean vectors first is strings built from alphabets with more than two elements are hard to visualize, but the idea is basically the same: when we’ve got strings of different lengths, how do we determine how we should insert and/or delete elements?"
  },
  {
    "objectID": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/levenshtein-distance.html",
    "href": "formal-and-practical-preliminaries/edit-distance-and-string-alignment/levenshtein-distance.html",
    "title": "Levenshtein distance",
    "section": "",
    "text": "Okay, so how do we compute edit (or Levenshtein) distance? The basic idea is to define it recursively, deciding at each point in the string whether we want to insert/delete an element (each at some cost \\(c\\)) or whether we want to try matching the string.\n\\[d_\\text{lev}(\\mathbf{a}, \\mathbf{b}) = \\begin{cases}\nc_\\text{ins} \\times |\\mathbf{b}| & \\text{if } |\\mathbf{a}| = 0 \\\\\nc_\\text{del} \\times |\\mathbf{a}| & \\text{if } |\\mathbf{b}| = 0 \\\\\n\\min \\begin{cases}d_\\text{lev}(a_1\\ldots a_{|\\mathbf{a}|-1}, \\mathbf{b}) + c_\\text{del} \\\\\n                  d_\\text{lev}(\\mathbf{a}, b_1\\ldots b_{|\\mathbf{b}|-1}) + c_\\text{ins} \\\\\n                  d_\\text{lev}(a_1\\ldots a_{|\\mathbf{a}|-1}, b_1\\ldots b_{|\\mathbf{b}|-1}) + c_\\text{sub} \\times \\mathbb{1}[a_{|\\mathbf{a}|} \\neq b_{|\\mathbf{b}|}]\\end{cases} & \\text{otherwise}\\end{cases}\\]\nwhere \\(c_\\text{sub}\\) defaults to \\(c_\\text{del} + c_\\text{ins}\\).\n\nfrom collections import defaultdict\nfrom typing import Optional\n\nclass StringEdit1:\n    '''Distance between strings\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n    \n    def __init__(self, insertion_cost: float = 1.,\n                 deletion_cost: float = 1.,\n                 substitution_cost: Optional[float] = None):\n        self._insertion_cost = insertion_cost\n        self._deletion_cost = deletion_cost\n\n        if substitution_cost is None:\n            self._substitution_cost = insertion_cost + deletion_cost\n        else:\n            self._substitution_cost = substitution_cost\n         \n    def __call__(self, source: str, target: str) -&gt; float:\n        self._call_counter = defaultdict(int)\n        return self._naive_levenshtein(source, target)\n        \n    def _naive_levenshtein(self, source, target):\n        self._call_counter[(source, target)] += 1\n        \n        cost = 0\n        \n        # base case\n        if len(source) == 0:\n            return self._insertion_cost*len(target)\n        \n        if len(target) == 0:\n            return self._deletion_cost*len(source)\n\n        # test if last characters of the strings match\n        if (source[len(source)-1] == target[len(target)-1]):\n            sub_cost = 0.\n        else:\n            sub_cost = self._substitution_cost\n\n        # minimum of delete from source, deletefrom target, and delete from both\n        return min(self._naive_levenshtein(source[:-1], target) + self._deletion_cost,\n                   self._naive_levenshtein(source, target[:-1]) + self._insertion_cost,\n                   self._naive_levenshtein(source[:-1], target[:-1]) + sub_cost)\n    \n    @property\n    def call_counter(self):\n        return self._call_counter\n\n\neditdist = StringEdit1(1, 1)\n\neditdist('æbstɹækt', 'æbstɹækt'), editdist('æbstɹækt', 'æbstɹækʃən'), editdist('æbstɹækʃən', 'æbstɹækt'), editdist('æbstɹækt', '')\n\n(0.0, 4.0, 4.0, 8)\n\n\nOkay. So here’s the thing. This looks nice, but it’s actually not that efficient because we’re actually redoing a whole ton of work.\n\neditdist('æbstɹækʃən', 'æbstɹækt')\n\neditdist.call_counter\n\ndefaultdict(int,\n            {('æbstɹækʃən', 'æbstɹækt'): 1,\n             ('æbstɹækʃə', 'æbstɹækt'): 1,\n             ('æbstɹækʃ', 'æbstɹækt'): 1,\n             ('æbstɹæk', 'æbstɹækt'): 1,\n             ('æbstɹæ', 'æbstɹækt'): 1,\n             ('æbstɹ', 'æbstɹækt'): 1,\n             ('æbst', 'æbstɹækt'): 1,\n             ('æbs', 'æbstɹækt'): 1,\n             ('æb', 'æbstɹækt'): 1,\n             ('æ', 'æbstɹækt'): 1,\n             ('', 'æbstɹækt'): 1,\n             ('æ', 'æbstɹæk'): 19,\n             ('', 'æbstɹæk'): 20,\n             ('æ', 'æbstɹæ'): 181,\n             ('', 'æbstɹæ'): 200,\n             ('æ', 'æbstɹ'): 1159,\n             ('', 'æbstɹ'): 1340,\n             ('æ', 'æbst'): 5641,\n             ('', 'æbst'): 6800,\n             ('æ', 'æbs'): 22363,\n             ('', 'æbs'): 28004,\n             ('æ', 'æb'): 75517,\n             ('', 'æb'): 97880,\n             ('æ', 'æ'): 224143,\n             ('', 'æ'): 299660,\n             ('æ', ''): 332688,\n             ('', ''): 224143,\n             ('æb', 'æbstɹæk'): 17,\n             ('æb', 'æbstɹæ'): 145,\n             ('æb', 'æbstɹ'): 833,\n             ('æb', 'æbst'): 3649,\n             ('æb', 'æbs'): 13073,\n             ('æb', 'æb'): 40081,\n             ('æb', 'æ'): 108545,\n             ('æb', ''): 157184,\n             ('æbs', 'æbstɹæk'): 15,\n             ('æbs', 'æbstɹæ'): 113,\n             ('æbs', 'æbstɹ'): 575,\n             ('æbs', 'æbst'): 2241,\n             ('æbs', 'æbs'): 7183,\n             ('æbs', 'æb'): 19825,\n             ('æbs', 'æ'): 48639,\n             ('æbs', ''): 68464,\n             ('æbst', 'æbstɹæk'): 13,\n             ('æbst', 'æbstɹæ'): 85,\n             ('æbst', 'æbstɹ'): 377,\n             ('æbst', 'æbst'): 1289,\n             ('æbst', 'æbs'): 3653,\n             ('æbst', 'æb'): 8989,\n             ('æbst', 'æ'): 19825,\n             ('æbst', ''): 27008,\n             ('æbstɹ', 'æbstɹæk'): 11,\n             ('æbstɹ', 'æbstɹæ'): 61,\n             ('æbstɹ', 'æbstɹ'): 231,\n             ('æbstɹ', 'æbst'): 681,\n             ('æbstɹ', 'æbs'): 1683,\n             ('æbstɹ', 'æb'): 3653,\n             ('æbstɹ', 'æ'): 7183,\n             ('æbstɹ', ''): 9424,\n             ('æbstɹæ', 'æbstɹæk'): 9,\n             ('æbstɹæ', 'æbstɹæ'): 41,\n             ('æbstɹæ', 'æbstɹ'): 129,\n             ('æbstɹæ', 'æbst'): 321,\n             ('æbstɹæ', 'æbs'): 681,\n             ('æbstɹæ', 'æb'): 1289,\n             ('æbstɹæ', 'æ'): 2241,\n             ('æbstɹæ', ''): 2816,\n             ('æbstɹæk', 'æbstɹæk'): 7,\n             ('æbstɹæk', 'æbstɹæ'): 25,\n             ('æbstɹæk', 'æbstɹ'): 63,\n             ('æbstɹæk', 'æbst'): 129,\n             ('æbstɹæk', 'æbs'): 231,\n             ('æbstɹæk', 'æb'): 377,\n             ('æbstɹæk', 'æ'): 575,\n             ('æbstɹæk', ''): 688,\n             ('æbstɹækʃ', 'æbstɹæk'): 5,\n             ('æbstɹækʃ', 'æbstɹæ'): 13,\n             ('æbstɹækʃ', 'æbstɹ'): 25,\n             ('æbstɹækʃ', 'æbst'): 41,\n             ('æbstɹækʃ', 'æbs'): 61,\n             ('æbstɹækʃ', 'æb'): 85,\n             ('æbstɹækʃ', 'æ'): 113,\n             ('æbstɹækʃ', ''): 128,\n             ('æbstɹækʃə', 'æbstɹæk'): 3,\n             ('æbstɹækʃə', 'æbstɹæ'): 5,\n             ('æbstɹækʃə', 'æbstɹ'): 7,\n             ('æbstɹækʃə', 'æbst'): 9,\n             ('æbstɹækʃə', 'æbs'): 11,\n             ('æbstɹækʃə', 'æb'): 13,\n             ('æbstɹækʃə', 'æ'): 15,\n             ('æbstɹækʃə', ''): 16,\n             ('æbstɹækʃən', 'æbstɹæk'): 1,\n             ('æbstɹækʃən', 'æbstɹæ'): 1,\n             ('æbstɹækʃən', 'æbstɹ'): 1,\n             ('æbstɹækʃən', 'æbst'): 1,\n             ('æbstɹækʃən', 'æbs'): 1,\n             ('æbstɹækʃən', 'æb'): 1,\n             ('æbstɹækʃən', 'æ'): 1,\n             ('æbstɹækʃən', ''): 1})\n\n\nWe could try to get around this by memoizing using the lru_cache decorator.\n\nfrom functools import lru_cache\n\nclass StringEdit2(StringEdit1):\n    '''Distance between strings\n\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n    \n    @lru_cache(256)\n    def _naive_levenshtein(self, source, target):\n        self._call_counter[(source, target)] += 1\n        \n        cost = 0\n\n        # base case\n        if len(source) == 0:\n            return self._insertion_cost*len(target)\n        \n        if len(target) == 0:\n            return self._deletion_cost*len(source)\n\n        # test if last characters of the strings match\n        if (source[len(source)-1] == target[len(target)-1]):\n            sub_cost = 0\n        else:\n            sub_cost = self._substitution_cost\n\n        # minimum of delete from source, deletefrom target, and delete from both\n        return min(self._naive_levenshtein(source[:-1], target) + self._deletion_cost,\n                   self._naive_levenshtein(source, target[:-1]) + self._insertion_cost,\n                   self._naive_levenshtein(source[:-1], target[:-1]) + sub_cost)\n\n\n%%timeit\n\neditdist = StringEdit1(1, 1)\n\neditdist('æbstɹækt', 'æbstɹækt'), editdist('æbstɹækt', 'æbstɹækʃən'), editdist('æbstɹækʃən', 'æbstɹækt'), editdist('æbstɹækt', '')\n\n2.19 s ± 8.49 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n\n\n\n%%timeit\n\neditdist = StringEdit2(1, 1)\n\neditdist('æbstɹækt', 'æbstɹækt'), editdist('æbstɹækt', 'æbstɹækʃən'), editdist('æbstɹækʃən', 'æbstɹækt'), editdist('æbstɹækt', '')\n\n178 µs ± 892 ns per loop (mean ± std. dev. of 7 runs, 10000 loops each)\n\n\n\neditdist = StringEdit2(1, 1)\n\neditdist('æbstɹækʃən', 'æbstɹækt')\n\neditdist.call_counter\n\ndefaultdict(int,\n            {('æbstɹækʃən', 'æbstɹækt'): 1,\n             ('æbstɹækʃə', 'æbstɹækt'): 1,\n             ('æbstɹækʃ', 'æbstɹækt'): 1,\n             ('æbstɹæk', 'æbstɹækt'): 1,\n             ('æbstɹæ', 'æbstɹækt'): 1,\n             ('æbstɹ', 'æbstɹækt'): 1,\n             ('æbst', 'æbstɹækt'): 1,\n             ('æbs', 'æbstɹækt'): 1,\n             ('æb', 'æbstɹækt'): 1,\n             ('æ', 'æbstɹækt'): 1,\n             ('', 'æbstɹækt'): 1,\n             ('æ', 'æbstɹæk'): 1,\n             ('', 'æbstɹæk'): 1,\n             ('æ', 'æbstɹæ'): 1,\n             ('', 'æbstɹæ'): 1,\n             ('æ', 'æbstɹ'): 1,\n             ('', 'æbstɹ'): 1,\n             ('æ', 'æbst'): 1,\n             ('', 'æbst'): 1,\n             ('æ', 'æbs'): 1,\n             ('', 'æbs'): 1,\n             ('æ', 'æb'): 1,\n             ('', 'æb'): 1,\n             ('æ', 'æ'): 1,\n             ('', 'æ'): 1,\n             ('æ', ''): 1,\n             ('', ''): 1,\n             ('æb', 'æbstɹæk'): 1,\n             ('æb', 'æbstɹæ'): 1,\n             ('æb', 'æbstɹ'): 1,\n             ('æb', 'æbst'): 1,\n             ('æb', 'æbs'): 1,\n             ('æb', 'æb'): 1,\n             ('æb', 'æ'): 1,\n             ('æb', ''): 1,\n             ('æbs', 'æbstɹæk'): 1,\n             ('æbs', 'æbstɹæ'): 1,\n             ('æbs', 'æbstɹ'): 1,\n             ('æbs', 'æbst'): 1,\n             ('æbs', 'æbs'): 1,\n             ('æbs', 'æb'): 1,\n             ('æbs', 'æ'): 1,\n             ('æbs', ''): 1,\n             ('æbst', 'æbstɹæk'): 1,\n             ('æbst', 'æbstɹæ'): 1,\n             ('æbst', 'æbstɹ'): 1,\n             ('æbst', 'æbst'): 1,\n             ('æbst', 'æbs'): 1,\n             ('æbst', 'æb'): 1,\n             ('æbst', 'æ'): 1,\n             ('æbst', ''): 1,\n             ('æbstɹ', 'æbstɹæk'): 1,\n             ('æbstɹ', 'æbstɹæ'): 1,\n             ('æbstɹ', 'æbstɹ'): 1,\n             ('æbstɹ', 'æbst'): 1,\n             ('æbstɹ', 'æbs'): 1,\n             ('æbstɹ', 'æb'): 1,\n             ('æbstɹ', 'æ'): 1,\n             ('æbstɹ', ''): 1,\n             ('æbstɹæ', 'æbstɹæk'): 1,\n             ('æbstɹæ', 'æbstɹæ'): 1,\n             ('æbstɹæ', 'æbstɹ'): 1,\n             ('æbstɹæ', 'æbst'): 1,\n             ('æbstɹæ', 'æbs'): 1,\n             ('æbstɹæ', 'æb'): 1,\n             ('æbstɹæ', 'æ'): 1,\n             ('æbstɹæ', ''): 1,\n             ('æbstɹæk', 'æbstɹæk'): 1,\n             ('æbstɹæk', 'æbstɹæ'): 1,\n             ('æbstɹæk', 'æbstɹ'): 1,\n             ('æbstɹæk', 'æbst'): 1,\n             ('æbstɹæk', 'æbs'): 1,\n             ('æbstɹæk', 'æb'): 1,\n             ('æbstɹæk', 'æ'): 1,\n             ('æbstɹæk', ''): 1,\n             ('æbstɹækʃ', 'æbstɹæk'): 1,\n             ('æbstɹækʃ', 'æbstɹæ'): 1,\n             ('æbstɹækʃ', 'æbstɹ'): 1,\n             ('æbstɹækʃ', 'æbst'): 1,\n             ('æbstɹækʃ', 'æbs'): 1,\n             ('æbstɹækʃ', 'æb'): 1,\n             ('æbstɹækʃ', 'æ'): 1,\n             ('æbstɹækʃ', ''): 1,\n             ('æbstɹækʃə', 'æbstɹæk'): 1,\n             ('æbstɹækʃə', 'æbstɹæ'): 1,\n             ('æbstɹækʃə', 'æbstɹ'): 1,\n             ('æbstɹækʃə', 'æbst'): 1,\n             ('æbstɹækʃə', 'æbs'): 1,\n             ('æbstɹækʃə', 'æb'): 1,\n             ('æbstɹækʃə', 'æ'): 1,\n             ('æbstɹækʃə', ''): 1,\n             ('æbstɹækʃən', 'æbstɹæk'): 1,\n             ('æbstɹækʃən', 'æbstɹæ'): 1,\n             ('æbstɹækʃən', 'æbstɹ'): 1,\n             ('æbstɹækʃən', 'æbst'): 1,\n             ('æbstɹækʃən', 'æbs'): 1,\n             ('æbstɹækʃən', 'æb'): 1,\n             ('æbstɹækʃən', 'æ'): 1,\n             ('æbstɹækʃən', ''): 1})\n\n\nThat helps a lot. Why? Because it only every computes the distance for a substring once. This is effectively what the Wagner–Fischer algorithm that you read about is doing. This is our first instance of a dynamic programming algorithm. The basic idea for Wagner-Fisher (and other algorithms we’ll use later in the class) is to cache the memoized values for a function within a chart whose rows correspond to positions in the source string and whose columns correspond to positions in the target string.\n\nimport numpy as np\n\nclass StringEdit3(StringEdit2):\n    '''Distance between strings\n\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n\n    def __call__(self, source: str, target: str) -&gt; float:\n        return self._wagner_fisher(source, target)\n\n    \n    def _wagner_fisher(self, source: str, target: str):\n        n, m = len(source), len(target)\n        source, target = '#'+source, '#'+target\n\n        distance = np.zeros([n+1, m+1], dtype=float)\n        \n        for i in range(1,n+1):\n            distance[i,0] = distance[i-1,0]+self._deletion_cost\n\n        for j in range(1,m+1):\n            distance[0,j] = distance[0,j-1]+self._insertion_cost\n            \n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if source[i] == target[j]:\n                    substitution_cost = 0.\n                else:\n                    substitution_cost = self._substitution_cost\n                    \n                costs = np.array([distance[i-1,j]+self._deletion_cost,\n                                  distance[i-1,j-1]+substitution_cost,\n                                  distance[i,j-1]+self._insertion_cost])\n                    \n                distance[i,j] = costs.min()\n                \n        return distance[n,m]\n\n\neditdist = StringEdit3(1, 1)\n\neditdist('æbstɹækt', 'æbstɹækʃən')\n\n4.0\n\n\nSo why use Wagner-Fisher when we can just use memoization on the naive algorithm? The reason is that the chart used in Wagner-Fisher allows us to very easily store information about the implicit alignment between string elements. This notion of alignment is the same as the one we saw above when talking about how best to match up a square to the face of a cube when discussing boolean vectors.\nSo what do we need to do add to our previous implementation of Wagner-Fisher to store backtraces? Importantly, note that you will need to return a list of backtraces because there could be multiple equally good ones. (This point will come up for all of the dynamic programming algorithms we look at and, as we’ll see, is actually abstractly related to syntactic ambiguity.)\n\nfrom typing import Tuple, List\n\nclass StringEdit4(StringEdit3):\n    '''distance, alignment, and edit paths between strings\n\n\n    Parameters\n    ----------\n    insertion_cost : float\n    deletion_cost : float\n    substitution_cost : float | NoneType (default: None)\n    '''\n\n    def __call__(self, source: str, \n                 target: str) -&gt;  Tuple[float, List[List[Tuple[int, int]]]]:\n        return self._wagner_fisher(source, target)\n            \n    def _wagner_fisher(self, source, target):\n        '''compute minimum edit distance and alignment'''\n\n        n, m = len(source), len(target)\n\n        source, target = self._add_sentinel(source, target)\n\n        distance = np.zeros([n+1, m+1], dtype=float)\n        pointers = np.zeros([n+1, m+1], dtype=list)\n\n        pointers[0,0] = []\n        \n        for i in range(1,n+1):\n            distance[i,0] = distance[i-1,0]+self._deletion_cost\n            pointers[i,0] = [(i-1,0)]\n\n        for j in range(1,m+1):\n            distance[0,j] = distance[0,j-1]+self._insertion_cost\n            pointers[0,j] = [(0,j-1)]\n            \n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if source[i] == target[j]:\n                    substitution_cost = 0.\n                else:\n                    substitution_cost = self._substitution_cost\n                    \n                costs = np.array([distance[i-1,j]+self._deletion_cost,\n                                  distance[i-1,j-1]+substitution_cost,\n                                  distance[i,j-1]+self._insertion_cost])\n                    \n                distance[i,j] = costs.min()\n\n                best_edits = np.where(costs==distance[i,j])[0]\n\n                indices = [(i-1,j), (i-1,j-1), (i,j-1)]\n                pointers[i,j] = [indices[i] for i in  best_edits]\n\n        pointer_backtrace = self._construct_backtrace(pointers,\n                                                      idx=(n,m))\n                \n        return distance[n,m], [[(i-1,j-1) for i, j in bt] \n                               for bt in pointer_backtrace]\n\n\n    def _construct_backtrace(self, pointers, idx):\n        if idx == (0,0):\n            return [[]]\n        else:\n            pointer_backtrace = [backtrace+[idx]\n                                 for prev_idx in pointers[idx]\n                                 for backtrace in self._construct_backtrace(pointers,\n                                                                            prev_idx)]\n            \n            return pointer_backtrace\n\n    def _add_sentinel(self, source, target):\n        if isinstance(source, str):\n            source = '#'+source\n        elif isinstance(source, list):\n            source = ['#'] + source\n        elif isinstance(source, tuple):\n            source = ('#',) + source\n        else:\n            raise ValueError('source must be str, list, or tuple')\n            \n        if isinstance(target, str):\n            target = '#' + target\n        elif isinstance(target, list):\n            target = ['#'] + target\n        elif isinstance(target, tuple):\n            target = ('#',) + target\n        else:\n            raise ValueError('target must be str, list, or tuple')\n            \n        return source, target\n\n\neditdist = StringEdit4(1, 1)\n\neditdist('æbstɹækʃən', 'æbstɹækt')\n\n(4.0,\n [[(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (6, 7),\n   (7, 7),\n   (8, 7),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 7),\n   (8, 7),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 6),\n   (7, 7),\n   (8, 7),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 6),\n   (8, 7),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 6),\n   (8, 6),\n   (8, 7),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 6),\n   (8, 6),\n   (9, 7)],\n  [(0, 0),\n   (1, 1),\n   (2, 2),\n   (3, 3),\n   (4, 4),\n   (5, 5),\n   (6, 6),\n   (7, 6),\n   (8, 6),\n   (9, 6),\n   (9, 7)]])\n\n\nThis isn’t particularly interpretable, so we can postprocess the output slightly to better see what’s going on.\n\nclass StringEdit5(StringEdit4):\n    '''distance, alignment, and edit paths between strings\n\n\n    Parameters\n    ----------\n    insertion_cost : float\n    deletion_cost : float\n    substitution_cost : float | NoneType (default: None)\n    '''\n\n    def __call__(self, source: str, \n                 target: str) -&gt;  Tuple[float, List[List[Tuple[str, str]]]]:\n        distance, alignment = self._wagner_fisher(source, target)\n        \n        return distance, [[(source[i[0]], \n                            target[i[1]]) \n                           for i in a] \n                          for a in alignment]\n\n\neditdist = StringEdit5(1, 1)\n\neditdist('æbstɹækʃən', 'æbstɹækt')\n\n(4.0,\n [[('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('k', 't'),\n   ('ʃ', 't'),\n   ('ə', 't'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 't'),\n   ('ə', 't'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 'k'),\n   ('ʃ', 't'),\n   ('ə', 't'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 'k'),\n   ('ə', 't'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 'k'),\n   ('ə', 'k'),\n   ('ə', 't'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 'k'),\n   ('ə', 'k'),\n   ('n', 't')],\n  [('æ', 'æ'),\n   ('b', 'b'),\n   ('s', 's'),\n   ('t', 't'),\n   ('ɹ', 'ɹ'),\n   ('æ', 'æ'),\n   ('k', 'k'),\n   ('ʃ', 'k'),\n   ('ə', 'k'),\n   ('n', 'k'),\n   ('n', 't')]])"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/trees.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/trees.html",
    "title": "Trees",
    "section": "",
    "text": "We’re going to be working through how to load a treebank into memory, and the first thing we need to know is how to deal with the objects contained in a treebank: trees. To structure this discussion, we’ll use a motivating example: suppose I’m interested in finding all sentences with a definite determiner in a subject."
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/trees.html#initial-design",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/trees.html#initial-design",
    "title": "Trees",
    "section": "Initial design",
    "text": "Initial design\nThe first question we need to ask is: what are trees in the abstract?\nAn initial approximation is that a tree is something that is…\n\n…empty (base case)\n…a nonempty sequence of trees\n\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, children: list['Tree']=[]):\n        self._children = children\n\nTree([Tree(), Tree()])\n\n&lt;__main__.Tree at 0x1138ed510&gt;\n\n\nOne problem is that these sorts of abstract trees aren’t super useful. So we can augment our definition.\nA tree is something that is…\n\n…empty (base case)\n…a piece of data paired with a nonempty sequence of trees\n\n\nfrom typing import TypeVar\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n\nBy convention, we shouldn’t access the private attributes _data and _children, so a common thing to do is to build read-only accessors using the @property decorators.\n\nclass Tree(Tree):\n\n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n\nt = Tree('S', [Tree('NP', ['the', Tree('children')]), Tree('VP')])\n\nt.children[0].data\n\n'NP'\n\n\nOur class doesn’t currently enforce that the children be Trees. To enforce this, we can build a validator private method into the intialization.\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n\nSo now the following won’t work.\n\ntry:\n    Tree('S', ['NP', 'VP'])\nexcept TypeError as e:\n    print(\"TypeError:\", e)\n\nTypeError: all children must be trees\n\n\nBut these will.\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree1\n\n&lt;__main__.Tree at 0x11390d010&gt;\n\n\n\nStringifying the tree\nIf we try to look at the tree, the result isn’t very informative.\n\ntree1\n\n&lt;__main__.Tree at 0x7f81e0d481c0&gt;\n\n\nThis is because we need to tell python how to display objects of our class. There are two obvious things to do: print the yield of the tree or print some representation of the tree itself. We implement both using the __str__ (what is shown when we call print()) and __repr__ (what is shown when we evaluate) magic methods.\nWe’ll have __str__ return the yield and __repr__ return the tree representation.\n\nclass Tree(Tree):\n    \n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\nSo if we print a Tree, we get the sentence it corresponds to.\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\nprint(tree1)\n\na greyhound loves a greyhound\n\n\nAnd if we try to evaluate the Tree, we get a visualization of its structure.\n\ntree1\n\nS\n--NP\n  --D\n    --a\n  --N\n    --greyhound\n--VP\n  --V\n    --loves\n  --NP\n    --D\n      --a\n    --N\n      --greyhound"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/containment-tests.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/containment-tests.html",
    "title": "Containment Tests",
    "section": "",
    "text": "Suppose I’m interested in finding all sentences with a definite determiner in a subject. This means checking whether a particular subtree (corresponding to the subject) contains a particular element.\nLet’s figure out how to compute whether a tree contains a particular piece of data, and then we’ll get back to figuring out how to grab the relevant subtree.\nContainment tests for a particular class (viewed as a sort of container) are often implemented in the __contains__ magic method, which defines the behavior of in. The idea is that __contains__ should take a piece of data and tell us whether it matches a piece of data somewhere in the tree. So basically, we want the analogue of asking whether a list contains some element using in.\nFor a list, containment could be naturally implemented using a for-loop. So suppose we’re redefining the list class, __contains__ could be implemented something like this:\nThe way we’ll implement __contains__ for Tree is analogous, but we have to take into account that the tree has a more interesting structure to it. We’ll do this by implementing two kinds of tree search algorithms:\nIn both kinds of search, we start at the top of the tree and work our way down, the question is which nodes we look at."
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/containment-tests.html#depth-first-search",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/containment-tests.html#depth-first-search",
    "title": "Containment Tests",
    "section": "Depth-first search",
    "text": "Depth-first search\nWe’ll look at two ways we can implement depth-first search: pre-order and post-order.\n\nPre-order depth-first search\nTo conduct pre-order depth-first search, we start from the left-most child subtree and, moving right, look at the data at the root of that subtree; then do pre-order depth-first search on that subtree.\nThis search path is visualized in the image below, where the line is our traversal path and the dots are wehn we look at a piece of data in a node.\n\n\n\n500px-Sorted_binary_tree_preorder.svg.png\n\n\n\nfrom typing import TypeVar\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n\n\ntree = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\n\"loves\" in tree, \"hates\" in tree\n\n(True, False)\n\n\n\n%%timeit\n\"loves\" in tree\n\n581 ns ± 5.9 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n%%timeit\n\"hates\" in tree\n\n943 ns ± 10.4 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)\n\n\n\n\nPost-order depth-first search\nIn post-order depth-first search, we start from the left-most child subtree and moving right, do post-order depth-first search on that subtree, then look at the data at the root of that subtree.\n\n\n\n500px-Sorted_binary_tree_postorder.svg.png\n\n\n\nclass Tree(Tree):\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # post-order depth-first search\n        if not self._children:\n            return self._data == data\n        else:\n            for c in self._children:\n                if data in c:\n                    return True\n            \n            return self._data == data\n\n\ntree = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\n\"loves\" in tree, \"hates\" in tree\n\n(True, False)\n\n\n\nBreadth-first search\nRather than recursing on subtrees, bread-first search looks at the data in all nodes at depth \\(i\\) then to breadth-first search at depth \\(i+1\\).\n\n\n\n500px-Sorted_binary_tree_breadth-first_traversal.svg.png\n\n\nOne natural way to implement breadth-first search is using what’s called iteratively deepening depth-first search.\n\nclass Tree(Tree):\n\n    @property\n    def depth(self) -&gt; int:\n        if self._children:\n            return 1 + max(c.depth for c in self._children)\n        else:\n            return 0\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # breadth-first search\n        for d in range(self.depth):\n            if self._iddfs(data, d):\n                return True\n\n        return False\n\n    def _iddfs(self, data, depth):\n        # iterative deepening depth-first search\n        if depth == 0:\n            return self._data == data\n        elif depth &gt; 0:\n            for c in self._children:\n                if c._iddfs(data, depth-1):\n                    return True\n        \n        return False\n\n\ntree = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\n\"loves\" in tree, \"hates\" in tree\n\n(True, False)"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html",
    "title": "Indexation",
    "section": "",
    "text": "Definition of Tree up to this point\nfrom typing import TypeVar\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\nLet’s return to the motivating example from the last section: suppose I’m interested in finding all sentences with a definite determiner in a subject.\nWe know how to compute containment. The next question is: how do we find particular subtrees? The basic idea is going to be that, when searching, we want to return a subtree instead of a boolean. But it’s also going to be important to know where that subtree is with respect to other subtrees–e.g. to figure out whether it is a subject. This is going to require a way of indexing trees."
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html#indexation-by-search-traversal",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html#indexation-by-search-traversal",
    "title": "Indexation",
    "section": "Indexation by search traversal",
    "text": "Indexation by search traversal\nOne way to index trees is analogous to lists: an int. But what does that int represent? The idea is that it will represent when a particular search algorithm visits a node. One way to do this is to flatten or linearize the tree according to the order in which, say, pre-order depth-first search visits subtrees, then to index into the flattened version of the tree.\n\nclass Tree(Tree):\n            \n    def __getitem__(self, idx):\n        return self.flattened[idx]\n    \n    def __len__(self):\n        return len(self.flattened)\n\n    @property\n    def flattened(self):\n        try:\n            return self._flattened\n        except AttributeError:\n            # pre-order depth-first search\n            self._flattened = [self] +\\\n                              [elem \n                               for c in self._children\n                               for elem in c.flattened]\n            return self._flattened\n\n\ntree = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree[0]\n\nS\n--NP\n  --D\n    --a\n  --N\n    --greyhound\n--VP\n  --V\n    --loves\n  --NP\n    --D\n      --a\n    --N\n      --greyhound\n\n\n\ntree[1]\n\nNP\n--D\n  --a\n--N\n  --greyhound\n\n\n\ntree[2]\n\nD\n--a\n\n\n\ntree[4]\n\nN\n--greyhound\n\n\n\nfor i in range(len(tree1)):\n    print(i, tree[i].data)\n\n0 S\n1 NP\n2 D\n3 a\n4 N\n5 greyhound\n6 VP\n7 V\n8 loves\n9 NP\n10 D\n11 a\n12 N\n13 greyhound"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html#indexation-by-path-to-root",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/indexation.html#indexation-by-path-to-root",
    "title": "Indexation",
    "section": "Indexation by path to root",
    "text": "Indexation by path to root\nOne issue with this indexation scheme is that it makes it a bit hard to represent relations like parenthood or sisterhood in a tree. One way to deal with this issue is to instead index using tuples representing the index path to the root.\n\nclass Tree(Tree):\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; 'Tree':\n        idx = (idx,) if isinstance(idx, int) else idx\n        \n        try:\n            assert all(isinstance(i, int) for i in idx)\n            assert all(i &gt;= 0 for i in idx)\n        except AssertionError:\n            errmsg = 'index must be a positive int or tuple of positive ints'\n            raise IndexError(errmsg)\n        \n        if not idx:\n            return self\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        else:\n            return self._children[idx[0]][idx[1:]]\n\n\ntree = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree1[tuple()]\n\nS\n--NP\n  --D\n    --a\n  --N\n    --greyhound\n--VP\n  --V\n    --loves\n  --NP\n    --D\n      --a\n    --N\n      --greyhound\n\n\n\ntree1[0]\n\nNP\n--D\n  --a\n--N\n  --greyhound\n\n\n\ntree1[0,0]\n\nD\n--a\n\n\n\ntree1[0,1]\n\nN\n--greyhound\n\n\n\ntree1[0,1,0]\n\ngreyhound\n\n\n\ntree1[1]\n\nVP\n--V\n  --loves\n--NP\n  --D\n    --a\n  --N\n    --greyhound\n\n\n\ntree1[1,1]\n\nNP\n--D\n  --a\n--N\n  --greyhound\n\n\n\ntree1[1,1,0]\n\nD\n--a\n\n\n\ntree1[1,1,0,0]\n\na"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html",
    "title": "Finding data with tree pattern matching",
    "section": "",
    "text": "Definition of Tree up to this point\nfrom typing import TypeVar\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; 'Tree':\n        idx = (idx,) if isinstance(idx, int) else idx\n        \n        try:\n            assert all(isinstance(i, int) for i in idx)\n            assert all(i &gt;= 0 for i in idx)\n        except AssertionError:\n            errmsg = 'index must be a positive int or tuple of positive ints'\n            raise IndexError(errmsg)\n        \n        if not idx:\n            return self\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        else:\n            return self._children[idx[0]][idx[1:]]\nWe can get from indices to trees, but how would we go from data to indices? Similar to a list, we can implement an index() method.\nclass Tree(Tree):\n     \n    def index(self, data, index_path=tuple()):\n        indices = [index_path] if self._data==data else []\n        root_path = [] if index_path == -1 else index_path\n        \n        indices += [j \n                    for i, c in enumerate(self._children) \n                    for j in c.index(data, root_path+(i,))]\n\n        return indices\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\ndeterminer_indices = tree1.index('D')\n\ndeterminer_indices\n\n[(0, 0), (1, 1, 0)]\ntree1[determiner_indices[0]]\n\nD\n--a\ntree1[determiner_indices[1]]\n\nD\n--the"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#searching-on-tree-patterns",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#searching-on-tree-patterns",
    "title": "Finding data with tree pattern matching",
    "section": "Searching on tree patterns",
    "text": "Searching on tree patterns\nWhat if instead we wanted to find where a piece of data was based on an entire tree pattern?\n\ntree_pattern = Tree('S', \n                    [Tree('NP',\n                          [Tree('D', \n                                [Tree('the')])]),\n                     Tree('VP')])\n\ntree_pattern\n\nS\n--NP\n  --D\n    --the\n--VP\n\n\nWe could implement a find() method.\n\nclass Tree(Tree):\n    \n    def find(self, pattern: 'Tree', \n             subtree_idx: tuple=tuple()) -&gt; list[tuple]:\n        '''The subtrees matching the pattern\n        \n        Parameters\n        ----------\n        pattern\n            the tree pattern to match against\n        subtree_idx\n            the index of the subtree within the tree pattern to return\n            defaults to the entire match\n        '''\n        \n        #raise NotImplementedError\n        \n        match_indices = [i + subtree_idx\n                         for i in self.index(pattern.data) \n                         if self[i].match(pattern)]\n            \n        return match_indices\n   \n    def match(self, pattern: 'Tree') -&gt; bool:\n        if self._data != pattern.data:\n            return False\n        \n        for child1, child2 in zip(self._children, pattern.children):\n            if not child1.match(child2):\n                return False\n                \n        return True\n\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree2 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree3 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree4 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree2.find(tree_pattern, (0,0))\n\n[(0, 0)]\n\n\n\ntree_pattern = Tree('VP', \n                    [Tree('V'),\n                     Tree('NP', \n                          [Tree('D', \n                                [Tree('the')])])])\n\ntree_pattern\n\nVP\n--V\n--NP\n  --D\n    --the\n\n\n\ntree1.find(tree_pattern, subtree_idx=(1,))\n\n[]\n\n\n\ntree2.find(tree_pattern, subtree_idx=(1,))\n\n[]\n\n\n\ntree3.find(tree_pattern, subtree_idx=(1,))\n\n[(1, 1)]\n\n\n\ntree4.find(tree_pattern, subtree_idx=(1,))\n\n[(1, 1)]\n\n\nThis sort of treelet-based matching is somewhat weak as it stands. What if we wanted:\n\n…nodes to be allowed to have some value from a set?\n…arbitrary distance between the nodes we are matching on?\n…arbitrary boolean conditions on node matches?"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#expanding-pattern-based-search-with-sparql",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#expanding-pattern-based-search-with-sparql",
    "title": "Finding data with tree pattern matching",
    "section": "Expanding pattern-based search with SPARQL",
    "text": "Expanding pattern-based search with SPARQL\nTo handle this, we need both a domain-specific language (DSL) for specifying such queries and an interpeter for that language. We can use SPARQL for our DSL. To intepret SPARQL, we will use the existing interpreter in rdflib.\nTo use rdflib’s interpreter, we need to map our Tree objects into an in-memory format for which a SPARQL interpreter is already implemented. We will use Resource Description Format as implemented in rdflib.\n\nfrom rdflib import Graph, URIRef\n\nclass Tree(Tree):\n    \n    RDF_TYPES = {}\n    RDF_EDGES = {'is': URIRef('is-a'),\n                 'parent': URIRef('is-the-parent-of'),\n                 'child': URIRef('is-a-child-of'),\n                 'sister': URIRef('is-a-sister-of')}\n            \n    def to_rdf(self, graph=None, nodes={}, idx=tuple()) -&gt; Graph: \n        graph = Graph() if graph is None else graph\n        \n        idxstr = '_'.join(str(i) for i in idx)\n        nodes[idx] = URIRef(idxstr)\n            \n        if self._data not in Tree.RDF_TYPES:\n            Tree.RDF_TYPES[self._data] = URIRef(self._data)\n\n        typetriple = (nodes[idx], \n                      Tree.RDF_EDGES['is'],\n                      Tree.RDF_TYPES[self.data])\n\n        graph.add(typetriple)\n\n        for i, child in enumerate(self._children):\n            childidx = idx+(i,)\n            child.to_rdf(graph, nodes, childidx)\n                \n            partriple = (nodes[idx], \n                         Tree.RDF_EDGES['parent'],\n                         nodes[childidx])\n            chitriple = (nodes[childidx], \n                         Tree.RDF_EDGES['child'],\n                         nodes[idx])\n            \n            graph.add(partriple)\n            graph.add(chitriple)\n            \n        for i, child1 in enumerate(self._children):\n            for j, child2 in enumerate(self._children):\n                child1idx = idx+(i,)\n                child2idx = idx+(j,)\n                sistriple = (nodes[child1idx], \n                             Tree.RDF_EDGES['sister'],\n                             nodes[child2idx])\n                \n                graph.add(sistriple)\n        \n        self._rdf_nodes = nodes\n        \n        return graph\n    \n    @property\n    def rdf(self) -&gt; Graph:\n        if not hasattr(self, \"_rdf\"):\n            self._rdf = self.to_rdf()\n\n        return self._rdf\n    \n    def find(self, query: str) -&gt; list[tuple[int]]:\n        return [tuple([int(i) \n                       for i in str(res[0]).split('_')]) \n                for res in self.rdf.query(query)]\n\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree2 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree3 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree4 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree1.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;.\n                      ?node &lt;is-the-parent-of&gt;* ?child.\n                      ?node &lt;is-a-child-of&gt;* ?parent.\n                      ?parent &lt;is-a&gt; &lt;S&gt;.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?node &lt;is-a-sister-of&gt; ?sister.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[]\n\n\n\ntree2.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;.\n                      ?node &lt;is-the-parent-of&gt;* ?child.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?node &lt;is-a-sister-of&gt; ?sister.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[(0,)]\n\n\n\ntree2.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[(0,)]\n\n\n\ntree3.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[]\n\n\n\ntree4.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;V&gt;.\n                    }''')\n\n[(1, 1)]"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/building-a-corpus-reader.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/building-a-corpus-reader.html",
    "title": "Building a corpus reader",
    "section": "",
    "text": "Definition of Tree up to this point\nfrom typing import TypeVar\nfrom rdflib import Graph, URIRef\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; 'Tree':\n        idx = (idx,) if isinstance(idx, int) else idx\n        \n        try:\n            assert all(isinstance(i, int) for i in idx)\n            assert all(i &gt;= 0 for i in idx)\n        except AssertionError:\n            errmsg = 'index must be a positive int or tuple of positive ints'\n            raise IndexError(errmsg)\n        \n        if not idx:\n            return self\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        else:\n            return self._children[idx[0]][idx[1:]]\n\n    \n    RDF_TYPES = {}\n    RDF_EDGES = {'is': URIRef('is-a'),\n                 'parent': URIRef('is-the-parent-of'),\n                 'child': URIRef('is-a-child-of'),\n                 'sister': URIRef('is-a-sister-of')}\n            \n    def to_rdf(self, graph=None, nodes={}, idx=tuple()) -&gt; Graph: \n        graph = Graph() if graph is None else graph\n        \n        idxstr = '_'.join(str(i) for i in idx)\n        nodes[idx] = URIRef(idxstr)\n            \n        if self._data not in Tree.RDF_TYPES:\n            Tree.RDF_TYPES[self._data] = URIRef(self._data)\n\n        typetriple = (nodes[idx], \n                      Tree.RDF_EDGES['is'],\n                      Tree.RDF_TYPES[self.data])\n\n        graph.add(typetriple)\n\n        for i, child in enumerate(self._children):\n            childidx = idx+(i,)\n            child.to_rdf(graph, nodes, childidx)\n                \n            partriple = (nodes[idx], \n                         Tree.RDF_EDGES['parent'],\n                         nodes[childidx])\n            chitriple = (nodes[childidx], \n                         Tree.RDF_EDGES['child'],\n                         nodes[idx])\n            \n            graph.add(partriple)\n            graph.add(chitriple)\n            \n        for i, child1 in enumerate(self._children):\n            for j, child2 in enumerate(self._children):\n                child1idx = idx+(i,)\n                child2idx = idx+(j,)\n                sistriple = (nodes[child1idx], \n                             Tree.RDF_EDGES['sister'],\n                             nodes[child2idx])\n                \n                graph.add(sistriple)\n        \n        self._rdf_nodes = nodes\n        \n        return graph\n    \n    @property\n    def rdf(self) -&gt; Graph:\n        if not hasattr(self, \"_rdf\"):\n            self._rdf = self.to_rdf()\n\n        return self._rdf\n    \n    def find(self, query: str) -&gt; list[tuple[int]]:\n        return [tuple([int(i) \n                       for i in str(res[0]).split('_')]) \n                for res in self.rdf.query(query)]\n\n\nNow that we can search over individual trees, let’s now see how to automatically load all trees from a corpus. We’ll use the constituency-parsed English Web TreeBank for this purpose. This corpus is separated into different genres, sources, and documents, with each .tree file containing possibly multiple parse trees (one per line).\n\n!tar -xzf LDC2012T13.tgz --to-command=cat 'eng_web_tbk/data/newsgroup/penntree/groups.google.com_8TRACKGROUPFORCOOLPEOPLE_3b43577fb9121c9f_ENG_20050320_090500.xml.tree'\n\n( (S (S-IMP (NP-SBJ (-NONE- *PRO*)) (VP (VB Play) (NP (PRP$ your) (NML (NML (NNS CD's)) (, ,) (NML (CD 8) (HYPH -) (NNS tracks)) (, ,) (NML (NML (NN reel)) (PP (IN to) (NP (NNS reels)))) (, ,) (NML (NNS cassettes)) (, ,) (NML (NN vinyl) (CD 33) (SYM /) (NNS 45's)) (, ,) (CC and) (NML (NN shellac) (NNS 78's)))) (PP-MNR (IN through) (NP (DT this) (JJ little) (JJ integrated) (NN amp))))) (, ,) (S (NP-SBJ (PRP you)) (VP (MD 'll) (VP (VB get) (NP (DT a) (JJ big) (NN eye) (NN opener))))) (. !)) )\n( (FRAG (ADJP (JJ complete) (PP (IN with) (NP (JJ original) (NNP Magnavox) (NNS tubes)))) (, -) (S (S (NP-SBJ-1 (DT all) (NNS tubes)) (VP (VBP have) (VP (VBN been) (VP (VBN tested) (NP-1 (-NONE- *)))))) (S (NP-SBJ (PRP they)) (VP (VBP are) (RB all) (ADJP-PRD (JJ good))))) (, -) (NP (NN stereo) (NN amp))) )\n\n\nWe will talk about how to actually parse these sorts of strings against a grammar later in the class, but for current purposes, we’ll use pyparsing to define a grammar and parse threse strings to a list of lists.\n\nimport pyparsing\n\nLPAR = pyparsing.Suppress('(')\nRPAR = pyparsing.Suppress(')')\ndata = pyparsing.Regex(r'[^\\(\\)\\s]+')\n\nexp = pyparsing.Forward()\nexpList = pyparsing.Group(LPAR + data + exp[...] + RPAR)\nexp &lt;&lt;= data | expList\n\n\nimport tarfile\nfrom pprint import pprint\n\nfname = \"eng_web_tbk/data/newsgroup/penntree/groups.google.com_8TRACKGROUPFORCOOLPEOPLE_3b43577fb9121c9f_ENG_20050320_090500.xml.tree\"\n\nwith tarfile.open(\"LDC2012T13.tgz\") as corpus:\n    with corpus.extractfile(fname) as treefile:\n        treestr = treefile.readline().decode()[2:-2]\n        treelist = exp.parseString(treestr)[0]\n    \ntreelist\n\nParseResults(['S', ParseResults(['S-IMP', ParseResults(['NP-SBJ', ParseResults(['-NONE-', '*PRO*'], {})], {}), ParseResults(['VP', ParseResults(['VB', 'Play'], {}), ParseResults(['NP', ParseResults(['PRP$', 'your'], {}), ParseResults(['NML', ParseResults(['NML', ParseResults(['NNS', \"CD's\"], {})], {}), ParseResults([',', ','], {}), ParseResults(['NML', ParseResults(['CD', '8'], {}), ParseResults(['HYPH', '-'], {}), ParseResults(['NNS', 'tracks'], {})], {}), ParseResults([',', ','], {}), ParseResults(['NML', ParseResults(['NML', ParseResults(['NN', 'reel'], {})], {}), ParseResults(['PP', ParseResults(['IN', 'to'], {}), ParseResults(['NP', ParseResults(['NNS', 'reels'], {})], {})], {})], {}), ParseResults([',', ','], {}), ParseResults(['NML', ParseResults(['NNS', 'cassettes'], {})], {}), ParseResults([',', ','], {}), ParseResults(['NML', ParseResults(['NN', 'vinyl'], {}), ParseResults(['CD', '33'], {}), ParseResults(['SYM', '/'], {}), ParseResults(['NNS', \"45's\"], {})], {}), ParseResults([',', ','], {}), ParseResults(['CC', 'and'], {}), ParseResults(['NML', ParseResults(['NN', 'shellac'], {}), ParseResults(['NNS', \"78's\"], {})], {})], {})], {}), ParseResults(['PP-MNR', ParseResults(['IN', 'through'], {}), ParseResults(['NP', ParseResults(['DT', 'this'], {}), ParseResults(['JJ', 'little'], {}), ParseResults(['JJ', 'integrated'], {}), ParseResults(['NN', 'amp'], {})], {})], {})], {})], {}), ParseResults([',', ','], {}), ParseResults(['S', ParseResults(['NP-SBJ', ParseResults(['PRP', 'you'], {})], {}), ParseResults(['VP', ParseResults(['MD', \"'ll\"], {}), ParseResults(['VP', ParseResults(['VB', 'get'], {}), ParseResults(['NP', ParseResults(['DT', 'a'], {}), ParseResults(['JJ', 'big'], {}), ParseResults(['NN', 'eye'], {}), ParseResults(['NN', 'opener'], {})], {})], {})], {})], {}), ParseResults(['.', '!'], {})], {})\n\n\nFirst, we’ll define a method for building a Tree from this ParseResults object, which can be viewed as a list of list of lists…\n\nclass Tree(Tree):\n    \n    @classmethod\n    def from_string(cls, treestr):\n        treelist = cls.PARSER.parseString(treestr[2:-2])[0]\n        return cls.from_list(treelist)\n    \n    @classmethod\n    def from_list(cls, treelist):\n        if isinstance(treelist, str):\n            return cls(treelist[0])\n        elif isinstance(treelist[1], str):\n            return cls(treelist[0], [cls(treelist[1])])\n        else:\n            return cls(treelist[0], [cls.from_list(l) for l in treelist[1:]])\n\nWe can now build a lightweight container for our trees.\n\nimport tarfile\nfrom collections import defaultdict\n\nclass EnglishWebTreebank:\n    \n    def __init__(self, root='LDC2012T13.tgz'):\n        \n        def trees():\n            with tarfile.open(root) as corpus:\n                for fname in corpus.getnames():\n                    if '.xml.tree' in fname:\n                        with corpus.extractfile(fname) as treefile:\n                            treestr = treefile.readline().decode()\n                            yield fname, Tree.from_string(treestr)\n                        \n        self._trees = trees()\n                        \n    def items(self):\n        for fn, tlist in self._trees:\n              yield fn, tlist\n        \newt = EnglishWebTreebank()\n\nnext(ewt.items())\n\n('eng_web_tbk/data/answers/penntree/20070404104007AAY1Chs_ans.xml.tree',\n S\n --SBARQ\n   --WHADVP-9\n     --WRB\n       --where\n   --SQ\n     --MD\n       --can\n     --NP-SBJ\n       --PRP\n         --I\n     --VP\n       --VB\n         --get\n       --NP\n         --NNS\n           --morcillas\n       --PP-LOC\n         --IN\n           --in\n         --NP\n           --NNP\n             --tampa\n           --NNP\n             --bay\n       --ADVP-LOC-9\n         ---NONE-\n           --*T*\n --,\n   --,\n --S\n   --S\n     --NP-SBJ\n       --PRP\n         --I\n     --VP\n       --MD\n         --will\n       --VP\n         --VB\n           --like\n         --NP\n           --DT\n             --the\n           --JJ\n             --argentinian\n           --NN\n             --type\n   --,\n     --,\n   --CC\n     --but\n   --S\n     --NP-SBJ-1\n       --PRP\n         --I\n     --VP\n       --MD\n         --will\n       --S\n         --NP-SBJ-1\n           ---NONE-\n             --*PRO*\n         --VP\n           --TO\n             --to\n           --VP\n             --VB\n               --try\n             --NP\n               --NNS\n                 --anothers\n             --INTJ\n               --UH\n                 --please\n --.\n   --?)\n\n\nNow, we can run arbitrary queries across trees.\n\newt = EnglishWebTreebank()\n\nn_subj = 0\nn_subj_prp = 0\nn_obj_prp = 0\nn_obj = 0 \n\nfor _, tree in ewt.items():\n    idx_subj_prp = tree.find('''SELECT ?node\n                                WHERE { ?node &lt;is-a&gt; &lt;NP-SBJ&gt;;\n                                              &lt;is-the-parent-of&gt; ?child.\n                                        ?child &lt;is-a&gt; &lt;PRP&gt;.\n                                      }''')\n    idx_subj = tree.find('''SELECT ?node\n                                WHERE { ?node &lt;is-a&gt; &lt;NP-SBJ&gt;. }''')\n    idx_obj_prp = tree.find('''SELECT ?node\n                                WHERE { ?parent &lt;is-the-parent-of&gt; ?node.\n                                        { ?parent &lt;is-a&gt; &lt;VP&gt; } UNION { ?parent &lt;is-a&gt; &lt;PP&gt; }\n                                        ?node &lt;is-the-parent-of&gt; ?child;\n                                              &lt;is-a&gt; &lt;NP&gt;.\n                                        ?child &lt;is-a&gt; &lt;PRP&gt;.\n                                      }''')\n    idx_obj = tree.find('''SELECT ?node\n                                WHERE { ?parent &lt;is-the-parent-of&gt; ?node.\n                                        { ?parent &lt;is-a&gt; &lt;VP&gt; } UNION { ?parent &lt;is-a&gt; &lt;PP&gt; }\n                                        ?node &lt;is-a&gt; &lt;NP&gt;.\n                                      }''')"
  },
  {
    "objectID": "finite-state-models/index.html",
    "href": "finite-state-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "In the last module, we mainly focused on setting up the foundations for analyzing languages as formal objects. We defined a language \\(L\\) on an alphabet \\(\\Sigma\\) as a subset of the set of strings \\(\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\) on \\(\\Sigma\\) (i.e. \\(L \\in 2^{\\Sigma^*}\\)). We explored one way of describing languages in the form of regular expressions on \\(\\Sigma\\) and we discussed one way of describing a relation between strings in the form of minimum edit distance.1"
  },
  {
    "objectID": "finite-state-models/index.html#footnotes",
    "href": "finite-state-models/index.html#footnotes",
    "title": "Overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, we decribed minimum edit distance as a relation from tuples of strings to distances; but as we explored in Assignment 3, we can also think of a distance, in the context of a particular weighting on edit operations, as a relation on strings: namely, a relation on strings that are {at most, exactly, at least, …} that distance apart. This idea extends the notion of a ball or sphere, depending on the constraint, to strings.↩︎"
  },
  {
    "objectID": "finite-state-models/levels-of-abstraction.html",
    "href": "finite-state-models/levels-of-abstraction.html",
    "title": "Levels of Abstraction",
    "section": "",
    "text": "What we will now begin to do is to deploy these tools for describing and expressing generalizations about possible languages. To do this, we are going to need to start thinking in terms of sets of sets of languages. That is, we will begin to think about subsets \\(\\mathcal{L}\\) of the set of languages \\(2^{\\Sigma^*}\\) on \\(\\Sigma\\). This approach will yield four important levels of abstraction:\n\nPrimitive (unanalyzed) elements \\(\\sigma \\in \\Sigma\\)\nCollections of primitive elements: sets \\(\\Sigma\\) or sequences \\(\\boldsymbol\\sigma \\in \\Sigma^*\\)\nLanguages: collections of sequences of primitive elements \\(L \\in 2^{\\Sigma^*}\\)\nCollections of languages \\(\\mathcal{L} \\subseteq 2^{\\Sigma^*}\\)\n\nWe are going to be particularly interested in collections of languages that share some interesting properties. We will call such collections classes of languages (or families of languages), and we will be interested in ways of compactly describing those classes that leverage the property shared by languages in the class. We will call compact descriptions of a particular language grammars and collections thereof classes of grammars \\(\\mathcal{G}\\)."
  },
  {
    "objectID": "finite-state-models/generation.html",
    "href": "finite-state-models/generation.html",
    "title": "Generation",
    "section": "",
    "text": "We will characterize the relationship between grammars and languages as one of generation: a grammars \\(G\\) generates a language \\(L\\) if \\(G\\) is a description of that language. To express that \\(G\\) generates \\(L\\), we will say that \\(\\mathbb{L}(G) = L\\), where \\(\\mathbb{L}\\) is some function from grammars to languages. (It is important to note that generation sounds like a procedural concept, but it is really declarative. Know that \\(G\\) generates a language \\(L\\) does not require us to know how to build \\(L\\) from \\(G\\).)"
  },
  {
    "objectID": "finite-state-models/generation.html#example-regular-expressions",
    "href": "finite-state-models/generation.html#example-regular-expressions",
    "title": "Generation",
    "section": "Example: regular expressions",
    "text": "Example: regular expressions\nWe’ve already seen one kind of grammar under this definition: regulars expressions. Remember that regular expressions \\(R(\\Sigma)\\) on \\(\\Sigma\\) themselves are strings of a language on \\(\\Sigma \\cup\\{\\epsilon, \\emptyset, \\cup, \\circ, (, ), *\\}\\). We formally define these strings recursively.\n\\(\\rho\\) is a regular expression if and only if:\n\n\\(\\rho \\in \\Sigma \\cup \\{\\epsilon, \\emptyset\\}\\)\n\\(\\rho\\) is \\((\\rho_1 \\cup \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\((\\rho_1 \\circ \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\(\\rho_1^*\\) for some regular expression \\(\\rho_1\\)\n\nWe can generate all the regular expressions given an alphabet.\nRegular expressions (so defined) evaluate to sets of strings on \\(\\Sigma\\)–i.e. languages on \\(\\Sigma\\). Another way of thinking about this is that a regular expression on \\(\\Sigma\\) describes a language on \\(\\Sigma\\).\nWe can define this evaluation procedure formally as a function \\(\\text{eval}: R(\\Sigma) \\rightarrow 2^{\\Sigma^*}\\), where \\(R(\\Sigma)\\) is the set of regular expressions on \\(\\Sigma\\).\n\\(\\text{eval}(\\rho) = \\begin{cases}\\{\\} & \\text{if } \\rho = \\emptyset \\\\\\{\\_\\} & \\text{if } \\rho = \\epsilon \\\\ \\{\\rho\\} & \\text{if } \\rho \\in \\Sigma\\\\ \\text{eval}(\\rho_1) \\times \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\circ \\rho_2) \\\\ \\text{eval}(\\rho_1) \\cup \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\cup \\rho_2)\\\\ \\bigcup_{i = 0}^\\infty \\text{eval}(\\rho_1)^i & \\text{if } \\rho = \\rho_1^*\\\\ \\end{cases}\\)\nEach regular expression is thus a grammar; the set of all regular expressions is a class of grammars; and \\(\\text{eval}\\) is an implementation of \\(\\mathbb{L}\\). We will call the class of languages \\(\\mathcal{R}\\) generated by the regular expressions the regular languages: \\[\\mathcal{R} \\equiv \\mathbb{L}(R(\\Sigma)) = \\{\\mathbb{L}(r) \\mid r \\in R(\\Sigma)\\} = \\{\\text{eval}(r) \\mid r \\in R(\\Sigma)\\}\\]\nWe will be studying these languages in depth, because as I’ve mentiond before and as you will read about in Heinz 2018, it seems very likely that all phonological grammars of natural languages are a subset of this class. To study these languages, it will be useful to introduce a class of grammars that are equivalent to the regular expressions in that they generate exactly the same set of languages–i.e. the regular languages. These grammars are known as finite state automata.\nWe will refer to this sort of equivalence–i.e. that \\(\\mathbb{L}(\\mathcal{G}_1) = \\mathbb{L}(\\mathcal{G}_2)\\) for classes \\(\\mathcal{G}_1\\) and \\(\\mathcal{G}_2\\)–as weak equivalence or equivalence in weak generative capacity. We will discuss another notion–strong equivalence–later. First, though we should discuss the broader context."
  },
  {
    "objectID": "finite-state-models/the-generativist-conceit.html",
    "href": "finite-state-models/the-generativist-conceit.html",
    "title": "The Generativist Conceit",
    "section": "",
    "text": "Why would we want to do all this? In the first lecture, I discussed what I called the Generativist Conceit: that we can understand natural language by studying the class of grammars that generate all + only the classes of natural languages. Though certain strong ideas associated with the Generativist Conceit are controversial, as I have stated it (relatively weakly) here, it is relatively uncontroversial: if our aim is to provide a scientific theory that characterizes natural language (whatever that might be: a colection of actual utterances, an aspect of human psychology or some abstract object with an existence independent of human minds), we at least need some method for encoding that theory in a way that makes testable predictions. Grammars are definitionally such a method as I’ve described them.\nAnother relatively uncontroversial observation that drives the Generativist Conceit is that children must be able to start without a language and come to know one, and the one they come to know is determined in some important way from their environment (rather than their genetics) in a finite amount of time on the basis of a finite number of observations of actual language use. Why this observation is important is that it means that natural languages must be learnable from a finite number of examples. Grammars are usually finite objects that (may) describe countably infinite languages, and so they might be a useful tool for describing the process by which a child comes to know a language.\nMore formally, we may say that children have some method \\(c\\) for mapping sequences of strings (or texts) \\(\\mathbf{t} \\in \\Sigma^{**}\\) along with some extralinguistic experience \\(E \\in \\mathcal{E}\\) into a grammar \\(G \\in \\mathcal{G}\\): \\[c: \\Sigma^{**} \\times \\mathcal{E} \\rightarrow \\mathcal{G}\\]. A child’s extralinguistic experience is nontrivial to represent, so to make the problem more tractable, we will often ignore that part.\nTo demonstrate that the problem of learning a grammar from a text, consider the following formal model, known as language identification in the limit, due to Gold 1964.\nWe start from the idea that a learner should be modeled as a function \\(f: \\Sigma^{**} \\rightarrow 2^{\\Sigma^*}\\) that maps an environment \\(E = \\langle \\boldsymbol\\sigma_1, \\boldsymbol\\sigma_2, \\ldots \\rangle \\in \\Sigma^{**}\\) to a language \\(L\\), where an environment is a sequence of strings in \\(L\\). We will say that a learner \\(f\\) learns a language \\(L\\) given an environment (or text) \\(E \\subset L^*\\) if the learner outputs \\(L\\) after seeing enough examples from the environment and that it learns a language \\(L\\) if it learns that language in any environment. That is, \\(f\\) learns \\(L\\) if it doesn’t matter which sequence you give it: \\(f \\text{ learns } L \\leftrightarrow \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L\\). A language family \\(\\mathcal{L}\\) is learnable if there exists a language learner that can learn all languages in the family: \\(f \\text{ learns } \\mathcal{L} \\leftrightarrow \\forall L \\in \\mathcal{L}: \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L\\).\nGold (1967) showed that, if a language family \\(\\mathcal{L}\\) contains languages \\(L_1, L_2, \\ldots, L_\\infty\\), such that \\(L_1 \\subset L_2 \\subset \\ldots \\subset L_\\infty \\bigcup _{i=1}^\\infty L_i\\), then it is not learnable. Here’s the idea behind the proof: suppose \\(f\\) is a learner that can learn \\(L_1, L_2, \\ldots, L_\\infty\\); we’ll show that it cannot learn \\(L_\\infty\\), by constructing an environment for \\(L_\\infty\\) that “tricks” \\(f\\).\nFirst, we construct environments \\(E_1, E_2, \\ldots\\) such that \\(f(E_i) = L_i\\). Next, we construct environment \\(E_\\infty\\) for \\(L_\\infty\\) inductively as follows:\n\nPresent \\(f\\) with \\(E_1\\) until it outputs \\(L_{1}\\).\nSwitch to presenting \\(f\\) with an environment that alternates the rest of \\(E_1\\) and the entirety of \\(E_2\\). Since \\(L_1 \\subset L_2\\), this concatenated environment is still an environment for \\(L_2\\), so \\(f\\) must eventually output \\(L_2\\).\nSwitch to presenting the rest of \\(E_1 \\circ E_2\\) and the entirety of \\(E_3\\) alternatively. And so on for all \\(i\\).\n\nBy construction, the resulting environment \\(E\\) contains the entirety of \\(E_{1},E_{2},\\ldots\\), thus it contains \\(L_\\infty\\), so it is an environment for \\(L_\\infty\\). But since the learner always switches to \\(L_{i}\\) for some finite \\(i\\), it never converges to \\(L_{\\infty}\\).\nThe point here is that we need to be careful about the class of languages we pick out as possible ones the learner might select from. We’ll explore this idea in the context of finite state automata, which are equivalent to regular expressions. Importantly, the full class of finite state automata have the problematic containment structure above, but the hope is that if we constrain these grammars in particular ways, we can avoid these sorts of issues while retaining coverage of natural languages."
  },
  {
    "objectID": "finite-state-models/formal-definition/index.html",
    "href": "finite-state-models/formal-definition/index.html",
    "title": "Overview",
    "section": "",
    "text": "We’ll start with an application of FSAs to modeling licit English syllables to get a sense for what they look like, then we’ll formalize them. To visualize FSAs, we’ll use pynini (We’ll also use pynini later when we define finite state transducers.)"
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html",
    "title": "Two equivalent definitions",
    "section": "",
    "text": "A Deterministic Finite State Automaton (DFA) is a grammar with 5 components:\nA Nondeterministic Finite State Automaton (NFA) is also a grammar with 5 components:\nfrom copy import copy, deepcopy\nfrom typing import Union, Optional, Dict, List\nfrom functools import lru_cache\n\nclass TransitionFunction:\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def __init__(self, transition_graph: dict[tuple[str, str], set[str]]):\n        self._transition_graph = transition_graph\n\n    def __call__(self, state: str, symbol: str) -&gt; set[str]:\n        try:\n            return self._transition_graph[(state, symbol)]\n        except KeyError:\n            return set({})\n\n    def __or__(self, other):\n        return TransitionFunction(dict(self._transition_graph, \n                                       **other._transition_graph))\n        \n    def add_transitions(self, transition_graph):\n        self._transition_graph.update(transition_graph)\n\n    def validate(self, alphabet, states):\n        self._validate_input_values(alphabet, states)\n        self._validate_output_types()\n        self._homogenize_output_types()\n        self._validate_output_values(states)\n\n    def _validate_input_values(self, alphabet, states):\n        for state, symbol in self._transition_graph.keys():\n            try:\n                assert symbol in alphabet\n\n            except AssertionError:\n                msg = 'all input symbols in transition function ' +\\\n                      'must be in alphabet'\n                raise ValueError(msg)\n\n            try:\n                assert state in states\n\n            except AssertionError:\n                msg = 'all input states in transition function ' +\\\n                      'must be in set of states'\n                raise ValueError(msg)\n\n    def _validate_output_types(self):\n        for states in self._transition_graph.values():\n            try:\n                t = type(states)\n                assert t is str or t is set\n\n            except AssertionError:\n                msg = 'all outputs in transition function' +\\\n                      'must be specified via str or set'\n                raise ValueError(msg)            \n\n    def _homogenize_output_types(self):\n        outputs = self._transition_graph.values()\n\n        for inp, out in self._transition_graph.items():\n            if type(out) is str:\n                self._transition_graph[inp] = {out}\n\n    def _validate_output_values(self, states):\n        for out in self._transition_graph.values():\n            try:\n                assert all([state in states for state in out])\n            except AssertionError:\n                msg = 'all output symbols in transition function' +\\\n                      'must be in states'\n                raise ValueError(msg)\n\n    @property\n    def transition_graph(self):\n        return self._transition_graph\n\nclass FiniteStateAutomaton:\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __init__(self, alphabet: set[str], states: set[str], \n                 initial_state: str, final_states: set[str], \n                 transition_graph: 'TransitionFunction'):\n        self._alphabet = {''} | alphabet\n        self._states = states\n        self._initial_state = initial_state\n        self._final_states = final_states\n        self._transition_function = TransitionFunction(transition_graph)\n\n        self._validate_initial_state()\n        self._validate_final_states()\n        self._transition_function.validate(self._alphabet, states)\n\n        self._generator = self._build_generator()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._generator)\n\n    def _build_generator(self):\n        string_buffer = [(self._initial_state, '')]\n        \n        if self._initial_state in self._final_states:\n            stack = ['']\n        else:\n            stack = []\n\n        while string_buffer:\n            if stack:\n                yield stack.pop()\n\n            else:\n                # this is very inefficient when we have total\n                # transition functions with many transitions to the\n                # sink state; could be optimized by removing a string\n                # if it's looping in a sink state, but we don't know\n                # in general what the sink state is\n                new_buffer = []\n                for symb in self._alphabet:\n                    for old_state, string in string_buffer:\n                        new_states = self._transition_function(old_state, symb)\n                        for st in new_states:\n                            new_elem = (st, string+symb)\n                            new_buffer.append(new_elem)\n\n                stack += [string\n                          for state, string in new_buffer\n                          if state in self._final_states]\n\n                string_buffer = new_buffer\nWe often define (and draw) FSAs s.t. their transition functions are partial. We can assume all FSA transition functions \\(\\delta\\) are in fact total by adding a sink state \\(q_\\text{sink} \\not\\in F\\) to the FSA and mapping all \\(\\langle q, \\sigma \\rangle\\) pairs for which \\(\\delta\\) is undefined to \\(q_\\text{sink}\\).\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def istotalfunction(self, states, alphabet):\n        return all((s, a) in self._transition_graph\n                    for s in states\n                    for a in alphabet)\n\n    def totalize(self, states, alphabet):\n        if not self.istotalfunction(states, alphabet):\n            domain = {(s, a) for s in states for a in alphabet}\n\n            sink_state = 'qsink'\n\n            while sink_state in states:\n                sink_state += 'sink'\n\n            for inp in domain:\n                if inp not in self._transition_graph:\n                    self._transition_graph[inp] = {sink_state}\nIt turns out that these two ways of defining FSAs are at least weakly equivalent. That is, the class of languages generated by NFAs is the same as the class generated by DFAs; therefore, both generate exactly the regular languages."
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html#dfas-are-strongly-equivalent-to-nfas",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html#dfas-are-strongly-equivalent-to-nfas",
    "title": "Two equivalent definitions",
    "section": "DFAs are (strongly) equivalent to NFAs",
    "text": "DFAs are (strongly) equivalent to NFAs\nEvery DFA can be converted to a strongly equivalent NFA. A DFA \\(G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) can be converted to an NFA \\(G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle\\) by defining the \\(G'\\) transition function \\(\\delta'(q,\\sigma) = \\{\\delta(q,\\sigma)\\}\\)\n\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    @property\n    def isdeterministic(self):\n        return all(len(v) &lt; 2 for v in self._transition_graph.values())\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    @property\n    def isdeterministic(self) -&gt; bool:\n        return self._transition_function.isdeterministic"
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html#nfas-are-at-least-weakly-equivalent-to-dfas",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html#nfas-are-at-least-weakly-equivalent-to-dfas",
    "title": "Two equivalent definitions",
    "section": "NFAs are (at least weakly) equivalent to DFAs",
    "text": "NFAs are (at least weakly) equivalent to DFAs\nEvery NFA can be converted to a weakly equivalent DFA\nThe \\(\\epsilon\\)-closure of \\(\\delta\\) is \\[\\delta_\\epsilon(q, \\sigma) =\n\\begin{cases}\n\\emptyset & \\text{if }  \\delta(q, \\sigma) \\text{ is undefined}\\\\\n\\delta(q, \\sigma) \\cup \\bigcup_{r \\in \\delta(q, \\sigma)} \\delta_\\epsilon(r, \\epsilon) & \\text{otherwise} \\\\\n\\end{cases}\\]\nAn NFA \\(G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) can be converted to a DFA \\(G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle\\) by defining:\n\n\\(Q' = \\mathcal{P}(Q)\\)\n\\(\\delta'(R, \\sigma) = \\bigcup_{r\\in R}\\delta_\\epsilon(r, \\sigma)\\)\n\\(q'_0 = \\{q_0\\} \\cup \\delta_\\epsilon(q_0, \\epsilon)\\)\n\\(F' = \\{R\\;|\\; \\exists r \\in R: r \\in F\\}\\)\n\n\nfrom functools import reduce\nfrom itertools import chain, combinations\n\ndef powerset(iterable):\n    \"\"\"https://docs.python.org/3/library/itertools.html#recipes\"\"\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\ndef transitive_closure(edges: set[tuple[str]]) -&gt; set[tuple[str]]:\n    \"\"\"\n    the transitive closure of a graph\n\n    Parameters\n    ----------\n    edges\n        the graph to compute the closure of\n\n    Returns\n    ----------\n    set(tuple)\n    \"\"\"\n    while True:\n        new_edges = {(x, w)\n                     for x, y in edges\n                     for q, w in edges\n                     if q == y}\n\n        all_edges = edges | new_edges\n\n        if all_edges == edges:\n            return edges\n\n        edges = all_edges\n\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def validate(self, alphabet, states):\n        self._validate_input_values(alphabet, states)\n        self._validate_output_types()\n        self._homogenize_output_types()\n        self._validate_output_values(states)\n\n        self._add_epsilon_transitive_closure()\n\n    def _add_epsilon_transitive_closure(self):\n        # get the state graph of all epsilon transitions\n        transitions = {(instate, outstate)\n                       for (instate, insymb), outs in self._transition_graph.items()\n                       for outstate in outs if not insymb}\n        \n        # compute the transitive closure of the epsilon transition\n        # state graph; requires homogenization beforehand\n        for instate, outstate in transitive_closure(transitions):\n            self._transition_graph[(instate, '')] |= {outstate}\n\n        new_graph = dict(self._transition_graph)\n\n        # add alphabet transitions from all states q_i that exit\n        # with an alphabet transition (q_i, a) and enter states q_j\n        # with epsilon transitions to states q_k\n        for (instate1, insymb1), outs1 in self._transition_graph.items():\n            for (instate2, insymb2), outs2 in self._transition_graph.items():\n                if instate2 in outs1 and not insymb2:\n                    # vacuously adds the already added epsilon\n                    # transitions as well\n                    try:\n                        new_graph[(instate1,insymb1)] |= outs2\n                    except KeyError:\n                        new_graph[(instate1,insymb1)] = outs2\n\n        self._transition_graph = new_graph\n\n    def determinize(self, initial_state):\n        new_initial_state = {initial_state} | self(initial_state, \"\")\n        new_transition_graph = {(instate, insymb): outstates\n                                for (instate, insymb), outstates \n                                in self._transition_graph.items()\n                                if insymb}\n        \n        return new_initial_state, TransitionFunction(new_transition_graph)\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n        \n    def determinize(self) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        if nondeterministic, determinize the FSA\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        new_states = {'_'.join(sorted(s)) for s in powerset(self._states) if s}\n        # doesn't handle epsilons coming from the start state\n        new_init, new_trans = self._transition_function.determinize(self._initial_state)\n        new_final = {'_'.join(sorted(s)) for s in powerset(self._states)\n                     if any([t in s for t in self._final_states])\n                     if s}\n        \n        new_transition = {('_'.join(sorted(s)), a): {'_'.join(sorted(t))}\n                          for s in powerset(self._states)\n                          for t in powerset(self._states)\n                          for a in self._alphabet\n                          if any([tprime in new_trans(sprime,a)\n                                  for sprime in s for tprime in t])\n                          if s and t}\n\n        return FiniteStateAutomaton(self._alphabet-{''}, new_states,\n                                    '_'.join(sorted(new_init)), new_final,\n                                    new_transition)\n\n    @property\n    def isdeterministic(self) -&gt; bool:\n        return self._transition_function.isdeterministic"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html",
    "href": "finite-state-models/formal-definition/the-regular-operations.html",
    "title": "The regular operations",
    "section": "",
    "text": "Given NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{union}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A \\cup B = \\mathbb{L}(M) = \\mathbb{L}(\\text{union}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive and do not contain \\(q_0\\); \\(Q = Q'_1 \\cup Q'_2 \\cup \\{q_0\\}\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\{q_1, q_2\\} & \\text{if } q=q_0 \\land \\sigma=\\epsilon \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\\\\n\\delta_2(q, \\sigma) & \\text{if } q\\in Q_2 \\\\\n\\text{undefined} & \\text{otherwise} \\\\\n\\end{cases}\\]\nDefine \\(F = F_1 \\cup F_2\\)\n\n\nclass FiniteStateAutomaton:\n    pass\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def _deepcopy(self):\n        fsa = copy(self)\n        del fsa._generator\n\n        return deepcopy(fsa)\n        \n    def _relabel_fsas(self, other):\n        \"\"\"\n        append tag to the input/ouput states throughout two FSAs\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n        \"\"\"\n\n        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n\n        return fsa1, fsa2\n\n    def _relabel_states(self, tag):\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        state_map = {s: s+'_'+tag for s in self._states}\n        \n        self._states = {state_map[s] for s in self._states}    \n        self._initial_state += '_'+tag\n        self._final_states = {state_map[s] for s in self._final_states}\n\n        self._transition_function = self._transition_function\n        self._transition_function.relabel_states(state_map)\n        \n        return self\n\n    def union(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        union this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.union\"\n        raise NotImplementedError(msg)"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#union",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#union",
    "title": "The regular operations",
    "section": "",
    "text": "Given NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{union}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A \\cup B = \\mathbb{L}(M) = \\mathbb{L}(\\text{union}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive and do not contain \\(q_0\\); \\(Q = Q'_1 \\cup Q'_2 \\cup \\{q_0\\}\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\{q_1, q_2\\} & \\text{if } q=q_0 \\land \\sigma=\\epsilon \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\\\\n\\delta_2(q, \\sigma) & \\text{if } q\\in Q_2 \\\\\n\\text{undefined} & \\text{otherwise} \\\\\n\\end{cases}\\]\nDefine \\(F = F_1 \\cup F_2\\)\n\n\nclass FiniteStateAutomaton:\n    pass\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def _deepcopy(self):\n        fsa = copy(self)\n        del fsa._generator\n\n        return deepcopy(fsa)\n        \n    def _relabel_fsas(self, other):\n        \"\"\"\n        append tag to the input/ouput states throughout two FSAs\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n        \"\"\"\n\n        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n\n        return fsa1, fsa2\n\n    def _relabel_states(self, tag):\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        state_map = {s: s+'_'+tag for s in self._states}\n        \n        self._states = {state_map[s] for s in self._states}    \n        self._initial_state += '_'+tag\n        self._final_states = {state_map[s] for s in self._final_states}\n\n        self._transition_function = self._transition_function\n        self._transition_function.relabel_states(state_map)\n        \n        return self\n\n    def union(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        union this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.union\"\n        raise NotImplementedError(msg)"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#concatenation",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#concatenation",
    "title": "The regular operations",
    "section": "Concatenation",
    "text": "Concatenation\nGiven NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{concatenate}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_1, F_2 \\rangle\\) recognizing \\(A \\circ B = \\mathbb{L}(M) = \\mathbb{L}(\\text{concatenate}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive; \\(Q = Q'_1 \\cup Q'_2\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\land q\\not\\in F_1 \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in F_1 \\land \\sigma \\neq \\epsilon \\\\\n\\delta_1(q, \\sigma) \\cup \\{q_2\\} & \\text{if } q\\in F_1 \\land \\sigma = \\epsilon \\\\\n\\delta_2(q, \\sigma)& \\text{if } q\\in Q_2 \\\\\n\\end{cases}\\]\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n        \n    def __add__(self, other):\n        return self.concatenate(other)\n\n    def __pow__(self, k):\n        return self.exponentiate(k)\n\n    def concatenate(self, other: 'FiniteStateAutomaton'):\n        \"\"\"\n        concatenate this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.concatenate\"\n        raise NotImplementedError(msg)\n\n    def exponentiate(self, k: int) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        concatenate this FSA k times\n\n        Parameters\n        ----------\n        k : int\n            the number of times to repeat; must be &gt;1\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        if k &lt;= 1:\n            raise ValueError(\"must be &gt;1\")\n\n        new = self\n\n        for i in range(1,k):\n            new += self\n\n        return new"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#kleene-closure",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#kleene-closure",
    "title": "The regular operations",
    "section": "Kleene Closure",
    "text": "Kleene Closure\nGiven an NFA \\(M = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A = \\mathbb{L}(M)\\), the NFA \\(N = \\text{kleene}(M) = \\langle Q \\cup \\{q'_0\\}, \\Sigma, \\delta', q'_0\\not\\in Q, F' \\rangle\\) recognizing \\(A^* = \\mathbb{L}(N) = \\mathbb{L}(\\text{kleene}(M))\\):\n\nDefine \\[\\delta'(q, \\sigma) = \\begin{cases}\nq_0 & \\text{if } (q = q'_0 \\lor q \\in F) \\land \\sigma = \\epsilon \\\\\n\\delta(q, \\sigma) & \\text{otherwise} \\\\\n\\end{cases}\\]\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def kleene_star(self)  -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        construct the kleene closure machine\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        fsa = self._deepcopy()\n        \n        new_transition = fsa._transition_function\n        new_transition.add_transitions({(s, ''): fsa._initial_state\n                                        for s in fsa._final_states})\n\n        return FiniteStateAutomaton(fsa._alphabet, fsa._states,\n                                    fsa._initial_state, fsa._final_states,\n                                    new_transition.transition_graph)"
  },
  {
    "objectID": "finite-state-models/generative-capacity/index.html",
    "href": "finite-state-models/generative-capacity/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 20, 2024."
  },
  {
    "objectID": "finite-state-models/recognition-and-parsing/index.html",
    "href": "finite-state-models/recognition-and-parsing/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 18, 2024."
  },
  {
    "objectID": "finite-state-models/phonological-rules-as-fsas/index.html",
    "href": "finite-state-models/phonological-rules-as-fsas/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 25, 2024."
  },
  {
    "objectID": "context-free-models/index.html",
    "href": "context-free-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 27, 2024."
  },
  {
    "objectID": "mildly-context-sensitive-models/index.html",
    "href": "mildly-context-sensitive-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around April 15, 2024."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html",
    "href": "assignments/assignments-1-and-2.html",
    "title": "Assignments 1 and 2",
    "section": "",
    "text": "Assignment 1 will consist of Tasks 1-3 and Assignment 2 will consist of Tasks 4-8.\nIn these assignments, you will be implementing and testing a vowel harmony rule system for Turkish. Vowel harmony rule systems are intended to explain the fact that, in some languages, vowels in a word must have the same value on certain phonological features. Your job in this assignment will not be to derive the rule system itself. Rather, I’m going to give you a rule system to implement that works reasonably well, and we’ll ask where it fails."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#mathematical-objects",
    "href": "assignments/assignments-1-and-2.html#mathematical-objects",
    "title": "Assignments 1 and 2",
    "section": "Mathematical objects",
    "text": "Mathematical objects\nThroughout the assignments, I will be asking you to say what kind of mathematical object you are implementing in a particular task. The kind of answers you might give here are relation and function. If your response is function, it should be as specific as possible–e.g. the function may be partial or total. In addition to specifying partiality and totality, I’d also like you to specify whether a function is injective and/or surjective. An injective function is one where, if \\(f(x) = f(y)\\), then \\(x = y\\) for all \\(x\\) and \\(y\\). A surjective function is one where, if \\(f: X \\rightarrow Y\\), then \\(f(X) = Y\\)—i.e. the range of \\(f\\) is the same as its codomain; or said another way, the image of \\(X\\) under \\(f: X \\rightarrow Y\\) is \\(Y\\)."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#data",
    "href": "assignments/assignments-1-and-2.html#data",
    "title": "Assignments 1 and 2",
    "section": "Data",
    "text": "Data\nThis assignment uses Bruce Hayes’ phonological features spreadsheet—his FeaturesDoulosSIL.xls sheet, which I have converted into a UTF-8 encoded CSV for easier processing in Python. This file contains the equivalent of the IPA charts familiar to you from LIN110.\nYou do not need the full chart for this assignment, since we will only need access to four features–SYLLABIC, HIGH, FRONT, and ROUND–and the phones that Turkish has. We’ll work with the slightly altered version of the chart below, which only contains the features for these phones and maps 0 to -.\n\nfeatures = '''phone,syllabic,high,front,round\nɑ,+,-,-,-\nb,-,-,-,-\nd͡ʒ,-,-,-,-\nt͡ʃ,-,-,-,-\nd,-,-,-,-\ne,+,-,+,-\nf,-,-,-,-\nɟ,-,+,+,-\nj,-,+,+,-\nh,-,-,-,-\nɯ,+,+,-,-\ni,+,+,+,-\nʒ,-,-,-,-\nc,-,+,+,-\nl,-,-,-,-\nm,-,-,-,-\nn,-,-,-,-\no,+,-,-,+\nø,+,-,+,+\np,-,-,-,-\nɾ,-,-,-,-\ns,-,-,-,-\nʃ,-,-,-,-\nt,-,-,-,-\nu,+,+,-,+\ny,+,+,+,+\nv,-,-,-,-\nj,-,+,+,-\nz,-,-,-,-'''\n\nwith open('features.csv', 'w') as fout:\n    fout.write(features)\n\n\n%%bash\ncat features.csv\n\nphone,syllabic,high,front,round\nɑ,+,-,-,-\nb,-,-,-,-\nd͡ʒ,-,-,-,-\nt͡ʃ,-,-,-,-\nd,-,-,-,-\ne,+,-,+,-\nf,-,-,-,-\nɟ,-,+,+,-\nj,-,+,+,-\nh,-,-,-,-\nɯ,+,+,-,-\ni,+,+,+,-\nʒ,-,-,-,-\nc,-,+,+,-\nl,-,-,-,-\nm,-,-,-,-\nn,-,-,-,-\no,+,-,-,+\nø,+,-,+,+\np,-,-,-,-\nɾ,-,-,-,-\ns,-,-,-,-\nʃ,-,-,-,-\nt,-,-,-,-\nu,+,+,-,+\ny,+,+,+,+\nv,-,-,-,-\nj,-,+,+,-\nz,-,-,-,-\n\n\nIf you are interested in doing further work in computational phonology, you might also check out the panphon package, which provides various tools for working with featurizations of phones."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#definition",
    "href": "assignments/assignments-1-and-2.html#definition",
    "title": "Assignments 1 and 2",
    "section": "Definition",
    "text": "Definition\nTo represent (e.g. FRONT, ROUND, etc.) and feature values (+, -), we will use two Enum classes: Feature and FeatureValue. Using Enums here allows us to define the set of possible feature names and feature values and thereby constrain the values that can appear in feature valuations. This functionality is useful as an additional check on the correctness of our code–e.g. in the case that we get invalid feature names or feature values.\n\nfrom enum import Enum\n\nclass Feature(Enum):\n    SYLLABIC = \"syllabic\"\n    HIGH = \"high\"\n    FRONT = \"front\"\n    ROUND = \"round\"\n\n    def __repr__(self):\n        return self.value\n\n    def __str__(self):\n        return self.__repr__()\n\nclass FeatureValue(Enum):\n    PLUS = \"+\"\n    MINUS = \"-\"\n\n    def __repr__(self):\n        return self.value\n\n    def __str__(self):\n        return self.__repr__()\n\nTo represent the relationship between feature names and feature values—encoded in the rows of features.csv—we’ll be using FeatureValuation objects, which are just thin wrappers around a dictionary with feature names (e.g. FRONT, ROUND, etc.) as keys and feature values (+, -) as values.\nImportantly, note that, unlike dictionaries, FeatureValuations are hashable, since they implement the __hash__ magic method. Usually, we want hashables to be immutable–e.g. lists and sets are mutable and not hashable while tuples and frozensets are immutable and hashable–though python does not enforce this. In this case, I will demarcate that we want the core data of the feature valuation to be a private instance attribute FeatureValuation._valuation by prepending an underscore to the attribute name: when you see an underscore prepended like this, it is a convention that you should not modify its value from outside the object it is an attribute of. If you need to access the raw dictionary (and you will need to), you should use the FeatureValuation.valuation property.\nThe __hash__ magic method more specifically determines what the hash function from the python standard library outputs when applied to a FeatureValuation object. This output will be an integer that is used in determining how to identify when to instances of the class are the same for the purposes of uniquely identifying them within a collection—e.g. when an element of a set or a dict key.\nThe upshot for our purposes is that, if a class implements __hash__, its objects can be used as dictionary keys. The class also implements comparison between feature valuations: == (__eq__), &gt; (__gt__), &lt; (__lt__), &gt;= (__ge__), and &lt;= (__le__). This behavior will be very useful for some tasks.\n\nclass FeatureValuation:\n    '''A mapping from feature names to feature values\n    \n    Parameters\n    ----------\n    valuation\n        the feature valuation as a dictionary\n    '''\n    \n    def __init__(self, valuation: dict[str, str]):\n        self._valuation = {\n            Feature(f): FeatureValue(v) \n            for f, v in valuation.items()\n        }\n    \n    def __hash__(self) -&gt; int:\n        return hash(tuple(self._valuation.items()))\n    \n    def __getitem__(self, key: Feature) -&gt; FeatureValue:\n        return self._valuation[key]\n    \n    def __eq__(self, other: 'FeatureValuation') -&gt; bool:\n        self.__class__._check_type(other)\n        \n        return self._valuation == other._valuation\n    \n    def __lt__(self, other: 'FeatureValuation') -&gt; bool:\n        self.__class__._check_type(other)\n        \n        if set(self._valuation) &lt; set(other._valuation):\n            return all(other._valuation[k] == v \n                       for k, v in self._valuation.items())\n        else:\n            return False\n    \n    def __gt__(self, other: 'FeatureValuation') -&gt; bool:        \n        return other &lt; self\n\n    def __le__(self, other: 'FeatureValuation') -&gt; bool:\n        return self == other or self &lt; other\n    \n    def __ge__(self, other: 'FeatureValuation') -&gt; bool:\n        return self == other or self &gt; other\n\n    def __repr__(self):\n        return self._valuation.__repr__()\n\n    def __str__(self):\n        return self._valuation.__str__()\n    \n    @property\n    def valuation(self) -&gt; dict[Feature, FeatureValue]:\n        return dict(self._valuation) # makes a copy\n\n    @classmethod\n    def _check_type(cls, obj):\n        try:\n            assert isinstance(obj, cls)\n        except AssertionError:\n            raise ValueError(\n                'can only compute equality between'\n                ' two FeatureValuation objects'\n            )\n\nWe can construct a FeatureValuation by calling its __init__ magic method on a Dict[str, str].\n\nfv1 = FeatureValuation({'syllabic': '+', 'round': '+'})\nfv2 = FeatureValuation({'syllabic': '+', 'round': '+', 'high': '+'})\n\nAnd note that because FeatureValuations are hashable, we can use them as dictionary keys.\n\nv1 = {fv1: {'o', 'ø', 'u', 'y'}}\nv2 = {fv2: {'u', 'y'}}\n\nAnd because we have defined __eq__, __lt__, and __gt__, we can compare FeatureValuations. Make sure you understand what each comparison does. You will need at least one of these operations for the tasks below.\n\nfv1 == fv1, fv1 &lt; fv2, fv1 &gt; fv2\n\n(True, True, False)\n\n\nFinally, to show you that hash works and returns an integer:\n\nhash(fv2)\n\n-2436770590250344338"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-1",
    "href": "assignments/assignments-1-and-2.html#task-1",
    "title": "Assignments 1 and 2",
    "section": "Task 1",
    "text": "Task 1\nLines: 5\nDefine a class method from_csv in the PhonologicalFeatureChart1 class defined below. This method should take as input a string representation of the directory path fpath to features.csv and return a PhonologicalFeatureChart1 object. This object should have a dictionary-valued private attribute _phone_to_features with phones as keys and FeatureValuation objects as values.\n(Note: I’m calling this class PhonologicalFeatureChart1 so that we can subclass it later without a bunch of copying and pasting. This isn’t strictly necessary for subclassing purposes, since you could simply subclass an new version of PhonologicalFeatureChart with an old version; but it’s useful here so that, if you run the cells out of order, you know exactly which version of the class you’re working with.) I’ll do this for other classes below without comment.)\n\nclass PhonologicalFeatureChart1:\n    '''The phonological features of different phones'''\n\n    def __init__(self, phone_to_features: Dict[str, FeatureValuation]):\n        self._phone_to_features = phone_to_features\n\n    def __repr__(self):\n        return self._phone_to_features.__repr__()\n\n    def __str__(self):\n        return self._phone_to_features.__str__()\n\n    @classmethod\n    def from_csv(cls, fpath: str='features.csv') -&gt; 'PhonologicalFeatureChart1':\n        '''Load Hayes' phonological feature chart\n\n        Parameters\n        ----------\n        fpath\n            path to phonological feature chart as a csv\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\n    def phone_to_features(self, phone: str) -&gt; FeatureValuation:\n        return self._phone_to_features[phone]\n\nWrite a test that checks for the correctness of from_csv by calling phone_to_features on some phone and making sure that it returns the correct feature valuation. (The fact that feature valuations implement __eq__ will be useful for this.) This (and all future) test should use standard Python exception handling facilities (try-except).\n\ntry:\n    phonological_feature_chart = PhonologicalFeatureChart1.from_csv()\nexcept NotImplementedError:\n    print(\"You still need to implement PhonologicalFeatureChart1.from_csv.\")\n\n# WRITE TESTS HERE\n\nYou still need to implement PhonologicalFeatureChart1.from_csv\n\n\nReferring to the set of feature as \\(F = \\{\\text{FRONT}, \\text{ROUND}, \\text{HIGH}, \\text{SYLLABIC}\\}\\) and the set of feature values as \\(V = \\{+, -\\}\\), explain what kind of mathematical object the feature valuations you just constructed are. If they are functions, say whether they are injective and/or surjective. Note that I am not asking about all possible feature valuations—just the ones constructed in from_csv.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-2",
    "href": "assignments/assignments-1-and-2.html#task-2",
    "title": "Assignments 1 and 2",
    "section": "Task 2",
    "text": "Task 2\nLines: 2\nDefine an instance method phone_from_features in the PhonologicalFeatureChart2 class that takes as input a FeatureValuation object and returns the set of phones that match that feature valuation. Assume that feature valuations need not specify a feature value for all feature names—e.g. the following should still return something (namely, all the high vowels).\n\ntry:\n    chart = PhonologicalFeatureChart2.from_csv('features.csv')\n    valuation = FeatureValuation({'syllabic': '+', 'high': '+'})\n    chart.phone_from_features(valuation)\nexcept NameError:\n    print(\"You still need to define PhonologicalFeatureChart2.\")\n\nYou still need to define PhonologicalFeatureChart2\n\n\nWe will refer to valuations like this as partial feature valuations.\nNote that you need to return a set because some phones are not uniquely determined by the features in features.csv—e.g. all consonants (besides the semivowels) will be - on these features. Further, it may return an empty set, since some feature combinations do not show up in features.csv—e.g. [-SYLLABIC, +ROUND].\n\nclass PhonologicalFeatureChart2(PhonologicalFeatureChart1):\n    '''The phonological features of different phones'''\n\n    def phone_from_features(self, features: FeatureValuation) -&gt; set[str]:\n        '''The phones that have a particular feature valuation\n\n        Parameters\n        ----------\n        features\n            the feature valuation\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of phone_from_features. This test should check at least five cases: (i) one where a singleton set should be returned when a total feature valuation is input; (ii) one where an empty set should be returned when a total feature valuation is input; (iii) one where a non-empty, non-singleton set should be returned when a total feature valuation is input; (iv) one where an empty set should be returned when a partial feature valuation is input; and (v) one where a non-empty, non-singleton set should be returned when a partial feature valuation is input.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object phone_from_features implements and what kind of object a partial feature valuation is, referring to the set of phones as \\(P\\). There are two possible answers here depending on what you take the right side of the relation/function to be.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-3",
    "href": "assignments/assignments-1-and-2.html#task-3",
    "title": "Assignments 1 and 2",
    "section": "Task 3",
    "text": "Task 3\nLines: 2\nUsing your phone_from_features method, define an instance method alter_features_of_phone in PhonologicalFeatureChart (our final version, so no number) that takes as input a phone and a (partial) feature valuation like valuation above. This function should return the set of phones that correspond to setting that phone’s features to the values listed in the feature valuation. For instance, if I passed this function the phone /u/ and the (partial) feature valuation [-ROUND], the function should return {/ɯ/}, but if I passed it /u/ and the feature valuation [-SYLLABIC, -HIGH, -LOW, -ROUND], it should return the set of consonants.\n\nclass PhonologicalFeatureChart(PhonologicalFeatureChart2):\n    '''The phonological features of different phones'''\n\n    def alter_features_of_phone(\n        self, phone: str, \n        features: FeatureValuation\n    ) -&gt; Set[str]:\n        '''The phones with features altered\n\n        Parameters\n        ----------\n        phone\n            the phone whose features we want to alter\n        features\n            the feature to alter\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of alter_features_of_phone. This test should check the same five kinds of cases that your test for Task 2 checked.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object alter_features_of_phone implements. There are two possible answers here depending on what you take the right side of the relation/function to be. Note that the left side of the relation is a tuple.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#data-1",
    "href": "assignments/assignments-1-and-2.html#data-1",
    "title": "Assignments 1 and 2",
    "section": "Data",
    "text": "Data\nThe remainder of this assignment is based on data from the UniMorph project – specifically, Turkish UniMorph. The UniMorph project provides a schema for annotating word forms with their root form and the morphological features they express across languages, as well as annotated data for (currently) 168 languages. Take a look at the Turkish dataset. You’ll notice that it consists of three columns.\n    hamsi          hamsiler          N;NOM;PL\n    hamsi          hamsilere         N;DAT;PL\n    hamsi          hamsilerden       N;ABL;PL\n    hamsi          hamsinin          N;GEN;SG\n    hamsi          hamsiye           N;DAT;SG\n    hamsi          hamsiyi           N;ACC;SG\n    hamsi          hamsilerin        N;GEN;PL\n    hamsi          hamsileri         N;ACC;PL\n    hamsi          hamsiden          N;ABL;SG\n    hamsi          hamsilerde        N;LOC;PL\n    hamsi          hamside           N;LOC;SG\n    hamsi          hamsi             N;NOM;SG\nThe second column contains word forms; the first contains the root corresponding to that form; and the third corresponds to the part of speech of and morphological features expressed by that form, separated by ;.\nI have included some code below that should make working with these data easier by loading Turkish Unimorph as an instance of my custom Unimorph class, defined below. Before moving forward, read through this code to make sure you understand what turkish_unimorph is.\n\nfrom collections import defaultdict\n\nclass Unimorph:\n\n    def __init__(self, fpath, pos_filter=lambda x: True, root_filter=lambda x: True,\n                 word_filter=lambda x: True, feature_filter=lambda x: True,\n                 graph_to_phone_map=None):\n\n        self._graph_to_phone_map = graph_to_phone_map\n\n        self._pos_filter = pos_filter\n        self._root_filter = root_filter\n        self._word_filter = word_filter\n        self._feature_filter = feature_filter\n        \n        self._load_unimorph(fpath)\n\n    def __getitem__(self, key):\n        return self._pos_to_word_to_features[key]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            return next(self._gen)\n        except StopIteration:\n            self._initialize_gen()\n            raise\n\n    def _load_unimorph(self, fpath):\n        '''load unimorph file and convert graphs to ipa\n\n        Parameters\n        ----------\n        fpath : str\n            path to unimorph data\n        \n\n        Returns\n        -------\n        tuple(dict)\n        '''\n\n        pos_to_word_to_features = defaultdict(lambda:\n                                              defaultdict(lambda:\n                                                          defaultdict(set)))\n\n        with open(fpath) as f:\n            for line in f:\n                line_split = line.strip().split('\\t')\n\n                if len(line_split) != 3:\n                    continue\n\n                root, word, pos_features = line_split\n\n                pos_features_split = pos_features.split(';')\n\n                pos = pos_features_split[0]\n                features = set(pos_features_split[1:])\n\n                if self._graph_to_phone_map is not None:\n                    try:\n                        root = self._convert_graph_to_phone(root)\n                        word = self._convert_graph_to_phone(word)\n                    except KeyError:\n                        continue\n                else:\n                    root = tuple(root)\n                    word = tuple(word)\n                        \n\n                keep = self._pos_filter(pos)\n                keep &= self._root_filter(root)\n                keep &= self._word_filter(word)\n                keep &= self._feature_filter(features)\n\n                if keep:\n                    pos_to_word_to_features[pos][root][word] = features\n\n        # freeze dict so it is no longer a defaultdict\n        self._pos_to_word_to_features = dict(pos_to_word_to_features)\n\n        self._initialize_gen()\n\n    def _initialize_gen(self):\n        self._gen = ((pos, root, word, features)\n                     for pos, d1 in self._pos_to_word_to_features.items()\n                     for root, d2 in d1.items()\n                     for word, features in d2.items())\n        \n    def _convert_graph_to_phone(self, word):\n        '''map graphs to phones\n\n        Parameters\n        ----------\n        word : str\n            the word as a string of graphs\n\n        Returns\n        -------\n        str\n        '''\n\n        # this takes the last phone in the list\n        # it should maybe create a set of possible words\n        return tuple([self._graph_to_phone_map[graph][-1]\n                      for graph in word])\n\n\ngraph_to_phone_map = {'a': ['ɑ'],\n                      'b': ['b'],\n                      'c': ['d͡ʒ'],\n                      'ç': ['t͡ʃ'],\n                      'd': ['d'],\n                      'e': ['e'],\n                      'f': ['f'],\n                      'g': ['ɡ̟', 'ɟ'],\n                      'ğ': ['ː', '‿', 'j'],\n                      'h': ['h'],\n                      'ı': ['ɯ'],\n                      'i': ['i'],\n                      'j': ['ʒ'],\n                      'k': ['k', 'c'],\n                      'l': ['ɫ', 'l'],\n                      'm': ['m'],\n                      'n': ['n'],\n                      'o': ['o'],\n                      'ö': ['ø'],\n                      'p': ['p'],\n                      'r': ['ɾ'],\n                      's': ['s'],\n                      'ş': ['ʃ'],\n                      't': ['t'],\n                      'u': ['u'],\n                      'ü': ['y'],\n                      'v': ['v'],\n                      'y': ['j'],\n                      'z': ['z'],\n                      ' ': [' ']}\n\n\nimport requests\nfrom io import BytesIO\nfrom zipfile import ZipFile\n\nturkish_unimorph_url = 'https://github.com/unimorph/tur/archive/master.zip'\nturkish_unimorph_zip = requests.get(turkish_unimorph_url).content\n\nwith ZipFile(BytesIO(turkish_unimorph_zip)) as zf:\n    with zf.open('tur-master/tur') as f_in:\n        with open('tur.txt', 'w') as f_out:\n            f_out.write(f_in.read().decode())\n\n\nturkish_unimorph = Unimorph('tur.txt',\n                            pos_filter=lambda x: x == 'N',\n                            root_filter=lambda x: ' ' not in x,\n                            word_filter=lambda x: ' ' not in x,\n                            feature_filter=lambda x: x.issubset({'PL', 'GEN'}),\n                            graph_to_phone_map=graph_to_phone_map)\n\nThere are two important things to notice. First, words and roots are represented as tuples of strings, instead of strings. The reason for this is that (i) I map each root and word in Turkish Unimorph to a phonetic/phonemic representation using a fixed mapping from graphs to phones; and (ii) some phones are represented as digraphs or trigraphs in unicode (e.g. t͡ʃ), so if we mapped from strings of graphs to strings of phones, it would be difficult to recover which characters in a string are a single phone and which are part of a phone that unicode represents with multiple symbols. Second, my Unimorph class allows the user to pass filters to the constructor __init__. In the current case, I have set these filters so our Unimorph instance only contains plural and/or genitive nouns."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-4",
    "href": "assignments/assignments-1-and-2.html#task-4",
    "title": "Assignments 1 and 2",
    "section": "Task 4",
    "text": "Task 4\nLines: 24\nIn standard descriptions of Turkish, the vowel harmony rule system plays out on three features: height [+/-HIGH], frontness [+/-FRONT], and roundedness [+/-ROUND]. Roughly, if a vowel is high, it must match with the immediately previous vowel on both frontness and roundedness; and if it is not high and not round, it must match with the immediately previous vowel on frontness.\nUsing your alter_features_of_phone method, define a class TurkishVowelHarmony1 whose instances take as input a word and applies the vowel harmony rule system to it (implemented using the __call__ magic method). Pay special attention to the fact that this system only looks at the immediately previous vowel.\n\nString = tuple[str]\n\nclass TurkishVowelHarmony1:\n    '''The Turkish vowel harmony system'''\n    \n    def __call__(self, word: String) -&gt; String:\n        '''Apply the vowel harmony rule\n        \n        Parameters\n        ----------\n        word\n            the word to apply the vowel harmony rule to\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of __call__. It should check at least six cases: (i) three randomly selected words found in Turkish Unimorph where the result of applying a TurkishVowelHarmony1 object to those words returns the same word back; and (ii) three randomly selected words found in Turkish Unimorph where it doesn’t.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object turkish_vowel_harmony implements, referring to the set of Turkish phones as \\(\\Sigma\\) and the set of strings over those phones as \\(\\Sigma^*\\). (Remember that \\(\\Sigma^* \\equiv \\bigcup_i^\\infty \\Sigma^i\\).)\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-5",
    "href": "assignments/assignments-1-and-2.html#task-5",
    "title": "Assignments 1 and 2",
    "section": "Task 5",
    "text": "Task 5\nLines: 1\nA disharmonic form is a root/word that does not obey the vowel harmony rule. Write an instance method disharmonic in TurkishVowelHarmony that maps a root or word to a boolean indicating whether or not it that root or word is disharmonic.\n\nclass TurkishVowelHarmony2(TurkishVowelHarmony1):\n    '''The Turkish vowel harmony system'''\n    \n    def disharmonic(self, word: Tuple[str]) -&gt; bool:\n        '''Whether the word is disharmonic\n        \n        Parameters\n        ----------\n        word\n            the word to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of disharmonic. It should check the same six cases you used to test __call__.\n\n# WRITE TESTS HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-6",
    "href": "assignments/assignments-1-and-2.html#task-6",
    "title": "Assignments 1 and 2",
    "section": "Task 6",
    "text": "Task 6\nLines: 2\nUsing your disharmonic method, write another instance method proportion_disharmonic_roots to compute the proportion of roots that are disharmonic in Turkish Unimorph.\n\nclass TurkishVowelHarmony3(TurkishVowelHarmony2):\n    '''The Turkish vowel harmony system'''\n    \n    def proportion_disharmonic_roots(self, lexicon: Unimorph) -&gt; float:\n        '''The proportion of words that are disharmonic in the lexicon\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-7",
    "href": "assignments/assignments-1-and-2.html#task-7",
    "title": "Assignments 1 and 2",
    "section": "Task 7",
    "text": "Task 7\nLines: 7\nUsing your disharmonic method, write an instance method xtab_root_word_harmony to cross-tabulate the proportion of words that are disharmonic against whether those words’ roots are disharmonic. The method should print that cross-tabulation as a \\(2 \\times 2\\) table with root (dis)harmony along the rows and word (dis)harmony along the columns.\n\nclass TurkishVowelHarmony4(TurkishVowelHarmony3):\n    '''The Turkish vowel harmony system'''\n    \n    def xtab_root_word_harmony(self, lexicon: Unimorph) -&gt; None:\n        '''Cross-tabulate word disharmony against root disharmony\n        \n        This should print (not return) a table represented as a list of lists:\n        \n                         | harmonic word | disharmonic word |\n                         ------------------------------------\n           harmonic root |               |                  |\n        disharmonic root |               |                  |\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nExplain the pattern that you see in this table.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-8",
    "href": "assignments/assignments-1-and-2.html#task-8",
    "title": "Assignments 1 and 2",
    "section": "Task 8",
    "text": "Task 8\nLines: 1\nUsing your disharmonic function, write an instance method get_disharmonic to find all of the words of some category (e.g. N, V, etc.) with a particular set of features (e.g. {plural, genitive}, etc.). Use that method to find all the plural and/or genitive nouns with disharmonic roots. Note that I’ve prefiltered Turkish Unimorph to just the plural and genitive nouns, but this method should still work for arbitrary categories and morphological features.\n\nclass TurkishVowelHarmony(TurkishVowelHarmony4):\n    '''The Turkish vowel harmony system'''\n    \n    def get_disharmonic(self, \n                        lexicon: Unimorph, \n                        category: str,\n                        features: Set[str]) -&gt; Set[Tuple[str]]:\n        '''Find all of the words of some category with a particular set of features\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        category\n            some category (e.g. \"N\", \"V\", etc.)\n        features\n            some set of features (e.g. {\"PL\", \"GEN\"}, etc.)\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nExplain what pattern you see in the vowels of the plural and genitive affixes. (A prerequisite for answering this question is figuring out what the plural and genitive affixes are.)\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html",
    "href": "assignments/assignments-3-and-4.html",
    "title": "Assignments 3 and 4",
    "section": "",
    "text": "Like Assignments 1 and 2, Assignments 3 and 4 are bundled together. You only need to do Task 1 for Assignment 3 and Tasks 2 and 3 for Assignment 4.\nThese assignments focus on implementing fuzzy tree search. In class, we developed various tree search algorithms that look for an exact match between a query and the data contained in particular trees. I’ve copied in the relevant class below as TreeOld.\nimport pyparsing\nfrom typing import TypeVar, Union\nfrom rdflib import Graph, URIRef\n\nDataType = TypeVar(\"DataType\")\nTreeList = list[str | tuple[Union[str, 'TreeList']]]\n\nclass TreeOld:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n    children\n    \"\"\"\n    \n    RDF_TYPES = {}\n    RDF_EDGES = {'is': URIRef('is-a'),\n                 'parent': URIRef('is-the-parent-of'),\n                 'child': URIRef('is-a-child-of'),\n                 'sister': URIRef('is-a-sister-of')}\n    \n    LPAR = pyparsing.Suppress('(')\n    RPAR = pyparsing.Suppress(')')\n    DATA = pyparsing.Regex(r'[^\\(\\)\\s]+')\n\n    PARSER = pyparsing.Forward()\n    SUBTREE = pyparsing.ZeroOrMore(PARSER)\n    PARSERLIST = pyparsing.Group(LPAR + DATA + SUBTREE + RPAR)\n    PARSER &lt;&lt;= DATA | PARSERLIST\n    \n    def __init__(self, data: DataType, children: list['TreeOld'] = []):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n  \n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string()\n     \n    def to_string(self, depth: int = 0) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; DataType:\n        if isinstance(idx, int):\n            return self._children[idx]\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        elif idx:\n            return self._children[idx[0]].__getitem__(idx[1:])\n        else:\n            return self\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['TreeOld']:\n        return self._children\n        \n    def _validate(self):\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n            \n    def index(self, data: DataType, index_path: tuple[int] = tuple()):\n        indices = [index_path] if self._data==data else []\n        root_path = [] if index_path == -1 else index_path\n        \n        indices += [j \n                    for i, c in enumerate(self._children) \n                    for j in c.index(data, root_path+(i,))]\n\n        return indices\n            \n    def to_rdf(\n        self, \n        graph: Graph | None=None, \n        nodes: dict[int, URIRef] = {}, \n        idx: tuple[int] = tuple()\n    ) -&gt; Graph: \n        graph = Graph() if graph is None else graph\n        \n        idxstr = '_'.join(str(i) for i in idx)\n        nodes[idx] = URIRef(idxstr)\n            \n        if self._data not in self.RDF_TYPES:\n            self.RDF_TYPES[self._data] = URIRef(self._data)\n\n        typetriple = (nodes[idx], \n                      self.RDF_EDGES['is'],\n                      self.RDF_TYPES[self.data])\n\n        graph.add(typetriple)\n\n        for i, child in enumerate(self._children):\n            childidx = idx+(i,)\n            child.to_rdf(graph, nodes, childidx)\n                \n            partriple = (nodes[idx], \n                         self.RDF_EDGES['parent'],\n                         nodes[childidx])\n            chitriple = (nodes[childidx], \n                         self.RDF_EDGES['child'],\n                         nodes[idx])\n            \n            graph.add(partriple)\n            graph.add(chitriple)\n            \n        for i, child1 in enumerate(self._children):\n            for j, child2 in enumerate(self._children):\n                child1idx = idx+(i,)\n                child2idx = idx+(j,)\n                sistriple = (nodes[child1idx], \n                             Tree.RDF_EDGES['sister'],\n                             nodes[child2idx])\n                \n                graph.add(sistriple)\n        \n        self._rdf_nodes = nodes\n        \n        return graph\n    \n    @property\n    def rdf(self) -&gt; Graph:\n        return self.to_rdf()\n    \n    def find(self, query):\n        return [tuple([int(i) \n                       for i in str(res[0]).split('_')]) \n                for res in self.rdf.query(query)]\n    \n    @classmethod\n    def from_string(cls, treestr: str) -&gt; 'TreeOld':\n        treelist = cls.PARSER.parseString(treestr[2:-2])[0]\n        \n        return cls.from_list(treelist)\n    \n    @classmethod\n    def from_list(cls, treelist: TreeList):\n        if isinstance(treelist, str):\n            return cls(treelist[0])\n        elif isinstance(treelist[1], str):\n            return cls(treelist[0], [cls(treelist[1])])\n        else:\n            return cls(treelist[0], [cls.from_list(l) for l in treelist[1:]])\nIn fuzzy search, we allow this exact matching restriction to be loosened by instead allowing that matches be (i) within some fixed edit distance; and/or (ii) closest (in terms of edit distance) to the query among all pieces of data. I’ve copied the relevant edit distance class that we developed in class below as EditDistance.\nimport numpy as np\n\nclass EditDistance:\n    '''Distance between strings\n\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n    \n    def __init__(self, insertion_cost: float = 1., \n                 deletion_cost: float = 1., \n                 substitution_cost: float | None = None):\n        self._insertion_cost = insertion_cost\n        self._deletion_cost = deletion_cost\n\n        if substitution_cost is None:\n            self._substitution_cost = insertion_cost + deletion_cost\n        else:\n            self._substitution_cost = substitution_cost\n\n    def __call__(self, source: str | list[str], target: str | list[str]) -&gt;  float:\n        '''The edit distance between the source and target\n        \n        The use of lists enables digraphs to be identified\n        \n        Parameters\n        ----------\n        source\n        target\n        '''\n        \n        # coerce to list if not a list\n        if isinstance(source, str):\n            source = list(source)\n            \n        if isinstance(target, str):\n            target = list(target)\n        \n        n, m = len(source), len(target)\n        source, target = ['#']+source, ['#']+target\n\n        distance = np.zeros([n+1, m+1], dtype=float)\n        \n        for i in range(1,n+1):\n            distance[i,0] = distance[i-1,0]+self._deletion_cost\n\n        for j in range(1,m+1):\n            distance[0,j] = distance[0,j-1]+self._insertion_cost\n            \n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if source[i] == target[j]:\n                    substitution_cost = 0.\n                else:\n                    substitution_cost = self._substitution_cost\n                    \n                costs = np.array([distance[i-1,j]+self._deletion_cost,\n                                  distance[i-1,j-1]+substitution_cost,\n                                  distance[i,j-1]+self._insertion_cost])\n                    \n                distance[i,j] = costs.min()\n                \n        return distance[n,m]"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-1",
    "href": "assignments/assignments-3-and-4.html#task-1",
    "title": "Assignments 3 and 4",
    "section": "Task 1",
    "text": "Task 1\nLines: 14\nDefine an instance method fuzzy_find. This method should take a piece of query data and optionally a distance and return all of the nodes that have data that is within edit distance distance from data; and/or if , closest is True, it should return all of the nodes closest to data among all nodes in the tree.\nFor instance:\n\nfuzzy_find('review', distance=3., closest=False) will return a tuple of every piece of data in the tree within edit distance 3 from review (e.g. view, reviewer, reviews, etc.), its distance to review, and its index; if there is nothing within that edit distance, an empty list will be returned\nfuzzy_find('review', distance=3., closest=True) will return a tuple of the closest pieces of data in the tree that are also within edit distance 3 from review (e.g. view, reviewer, reviews, etc.), its distance to review, and its index; if there is nothing within that edit distance, an empty list will be returned\nfuzzy_find('review', distance=np.inf, closest=True) will return a tuple of the closest pieces of data in the tree to review (e.g. view, reviewer, reviews, etc.), regardless of edit distance, its distance to review, and its index; this will always return something\nfuzzy_find('review', distance=np.inf, closest=False) will return a tuple of every piece of data in the tree to review (e.g. view, reviewer, reviews, etc.), regardless of edit distance, its distance to review, and its index; this will always return a list with as many elements as there are nodes in the tree\n\nThis method should also support only searching the terminal nodes (leaves) of the tree with the flag terminals_only.\nHint: you should look back at the methods we defined for searching and indexing the tree above. Specifically, to understand why you might want something like index_path defaulting to the empty tuple, look at the index method of TreeOld.\n\nFuzzyFindResult = tuple[tuple, str | list[str], float]\n\nclass Tree(TreeOld):\n    \n    DIST = EditDistance(1., 1.)\n    \n    def fuzzy_find(self, data: Union[str, list[str]], \n                   closest: bool = True, \n                   distance: float = np.inf,\n                   case_fold: bool = True,\n                   terminals_only: bool = True,\n                   index_path: tuple = tuple()) -&gt; list[FuzzyFindResult]:\n        \n        '''Find the (closest) strings within a certain distance\n        \n        Defaults to computing the closest strings among the terminals and ignoring case\n        \n        The format of the things returned is [((0,1,0), \"view\", 2.0), ...]. Note that \n        edit distance can be computed on either a `str` or `List[str]`; that's why\n        the middle element of each tuple might be either.\n        \n        Parameters\n        ----------\n        data\n            the data to match against\n        closest\n            whether to return only the closest strings or all strings within distance\n        distance\n            the distance within which a string must be\n        case_fold\n            whether to lower-case the data\n        terminals_only\n            whether to only search the terminals\n        index_path\n        '''\n        raise NotImplementedError\n\nWrite tests that use the following tree as input data.\n\ntreestr = '( (SBARQ (WHNP-1 (WP What)) (SQ (NP-SBJ-1 (-NONE- *T*)) (VP (VBZ is) (NP-PRD (NP (DT the) (JJS best) (NN place)) (SBAR (WHADVP-2 (-NONE- *0*)) (S (NP-SBJ (-NONE- *PRO*)) (VP (TO to) (VP (VB get) (NP (NP (NNS discounts)) (PP (IN for) (NP (NML (NNP San) (NNP Francisco)) (NNS restaurants)))) (ADVP-LOC-2 (-NONE- *T*))))))))) (. ?)) )'\n\ntesttree = Tree.from_string(treestr)\n\ntesttree\n\nSBARQ\n--WHNP-1\n  --WP\n    --What\n--SQ\n  --NP-SBJ-1\n    ---NONE-\n      --*T*\n  --VP\n    --VBZ\n      --is\n    --NP-PRD\n      --NP\n        --DT\n          --the\n        --JJS\n          --best\n        --NN\n          --place\n      --SBAR\n        --WHADVP-2\n          ---NONE-\n            --*0*\n        --S\n          --NP-SBJ\n            ---NONE-\n              --*PRO*\n          --VP\n            --TO\n              --to\n            --VP\n              --VB\n                --get\n              --NP\n                --NP\n                  --NNS\n                    --discounts\n                --PP\n                  --IN\n                    --for\n                  --NP\n                    --NML\n                      --NNP\n                        --San\n                      --NNP\n                        --Francisco\n                    --NNS\n                      --restaurants\n              --ADVP-LOC-2\n                ---NONE-\n                  --*T*\n--.\n  --?\n\n\nThe tests should test the four combinations of distance and closest listed above with the same query data, both with and without terminals_only=True and terminals_only=False (eight tests in total). Two further tests should test distance=np.inf, closest=True, terminals_only=True for a case where only a single element should be returned and a case where multiple elements in the tree should be returned.\n\n# write tests here\n\nRemember that we talked in class about what it would mean to take the distance between a string and a collection of strings: basically, the minimum of the edit distances between the string and each string in that set.\nWe can use this concept in two ways here. The first is to view the tree as a container for some data and to compute the minimum distance between a query and any data contained in the tree. Alternatively, we can think of the query itself as determining a set and compute the minimum distance of each piece of data in the tree to that set. Task 2 will implement the former and Task 3 the latter.\nI’ve copied the corpus reader we developed for the English Web Treebank in class below. We’ll make use of this for Task 2. (You’ll need to grab LDC2012T13.tgz from the course Google drive.)\n\n!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1ygMIl1w6wz6A24oxkLwirunSKXb9EW12' -O 'LDC2012T13.tgz'\n\n\nimport tarfile\nfrom collections import defaultdict\n\nclass EnglishWebTreebankOld:\n    \n    def __init__(self, root='LDC2012T13.tgz'):\n        \n        def trees():\n            with tarfile.open(root) as corpus:\n                for fname in corpus.getnames():\n                    if '.xml.tree' in fname:\n                        with corpus.extractfile(fname) as treefile:\n                            treestr = treefile.readline().decode()\n                            yield fname, Tree.from_string(treestr)\n                        \n        self._trees = trees()\n                        \n    def items(self):\n        for fn, tlist in self._trees:\n              yield fn, tlist"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-2",
    "href": "assignments/assignments-3-and-4.html#task-2",
    "title": "Assignments 3 and 4",
    "section": "Task 2",
    "text": "Task 2\nLines: 3\nDefine an instance method fuzzy_find for the corpus reader class that computes the minimum distance between a query and a tree for all trees in the corpus. It should return a list of tuples with the first element a tree ID, the second an index in that tree, the third the data at that index and the fourth the distance between the query and that index. A tuple should be included in the returned list only if the distance is equal to the minimum across trees in the corpus.\nHint: this should be very straightforward using a particular parameterization for Tree1.fuzzy_find. Which one?\n\nclass EnglishWebTreebank(EnglishWebTreebankOld):\n    \n    def fuzzy_find(self, data: str | list[str]) -&gt; list[FuzzyFindResult]:\n        '''Find the trees in the corpus closest to the query data\n        \n        Parameters\n        ----------\n        data\n        '''\n        raise NotImplementedError\n\nNow, load this corpus.\n\newt = EnglishWebTreebank()\n\nWrite a single test for a piece of data you know exists in some tree in the corpus. (Determiners or auxiliary verbs are good candidates.) Thus, the minimum distance will be zero and your method should return only trees that contain that element. Note that this test should use some already existing method to produce the correct set of trees.\nHint: such a method already exists in the TreeOld class.\n\n# write test here\n\nThe next task will look at computing distance between the elements of a tree and a query set defined by a regular expression. Here is a regular expression class based on the formal definition of regular expressions I gave you in class.\n\nfrom itertools import product\n\nclass Regex:\n    \"\"\"A regular expression\n    \n    Parameters\n    ----------\n    regex_parsed\n    maxlength\n    \"\"\"\n\n    CHAR = pyparsing.Word(pyparsing.alphas, exact=1).setName(\"character\") # &lt;- use 'exact', not 'max'\n\n    LPAR = pyparsing.Suppress('(')\n    RPAR = pyparsing.Suppress(')')\n\n    PARSER = pyparsing.Forward()\n    GROUP = pyparsing.Group(LPAR + PARSER + RPAR)\n    QUANT = pyparsing.oneOf(\"* ? +\")\n    DSJ = '|'\n\n    ITEM = pyparsing.Group((CHAR | GROUP) + QUANT) | pyparsing.Group(CHAR + DSJ + CHAR) | CHAR | GROUP\n    ITEMSEQ = pyparsing.OneOrMore(ITEM)\n\n    PARSER &lt;&lt;= pyparsing.delimitedList(ITEMSEQ, pyparsing.Empty())\n    \n    def __init__(self, regex_parsed: List[Union[str, List]], maxlength: int):\n        self._regex_parsed = regex_parsed\n        self._maxlength = maxlength\n    \n    @classmethod\n    def from_string(cls, regexstr: str, maxlength: int = 30):\n        if regexstr[0] != '(':\n            regexstr = '(' + regexstr\n            \n        if regexstr[-1] != ')':\n            regexstr = regexstr +')'\n            \n        regex_parsed = cls.PARSER.parseString(regexstr)[0]\n        \n        return cls(regex_parsed, maxlength)\n    \n    def __iter__(self):\n        self._gen = self._construct_regex_generator()\n        return self\n    \n    def __next__(self):\n        return next(self._gen)\n        \n    def _construct_regex_generator(self, regex=None):\n        if regex is None:\n            regex = self._regex_parsed\n        \n        if isinstance(regex, str):\n            if len(regex) &gt; self._maxlength:\n                raise StopIteration\n\n            yield regex\n        \n        elif regex[1] in ['*', '+']:\n            i = 0 if regex[1] == '*' else 1\n            while True:\n                for s in self._construct_regex_generator(regex[0]):\n                    yield s*i\n\n                i += 1\n                \n                if i &gt; self._maxlength:\n                    break\n                    \n        elif regex[1] == '?':\n            yield ''\n            yield regex[0]\n\n        elif regex[1] == '|':\n            left = self._construct_regex_generator(regex[0])\n            right = self._construct_regex_generator(regex[2])\n            \n            s1 = s2 = ''\n            \n            while True:\n                if len(s1) &lt;= self._maxlength:                \n                    s1 = next(left)\n                    yield s1\n                \n                if len(s2) &lt;= self._maxlength:\n                    s2 = next(right)\n                    yield s2\n\n                if len(s1) &gt; self._maxlength and len(s2) &gt; self._maxlength:\n                    break\n        \n        else:\n            evaluated = [self._construct_regex_generator(r) for r in regex]\n            for p in product(*evaluated):\n                c = ''.join(p)\n                \n                if len(c) &lt;= self._maxlength:\n                    yield c\n\nThe way to use this class to generate the set of strings associated with a regular expression is tree an instance of the Regex class as a generator.\nImportantly, I’ve include a way of only generating strings of less than some length threshold maxlength in the case that your regular expression evaluates to an infinite set.\n\nfor s in Regex.from_string('co+lou?r', 20):\n    print(s)"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-3",
    "href": "assignments/assignments-3-and-4.html#task-3",
    "title": "Assignments 3 and 4",
    "section": "Task 3",
    "text": "Task 3\nLines: 15\nDefine a new version of fuzzy_find that behaves exactly the same as your version from Task 1 except that it allows the query data to be a regular expression parsable by Regex.from_string. Make sure that you correctly handle infinite sets.\nHint: your new fuzzy_find will be nearly identical to the old one. My implementation only has a single additional line.\n\nclass Tree(TreeOld):\n    \n    DIST = EditDistance(1., 1., 1.)\n    \n    def fuzzy_find(self, data: str | list[str], \n                   closest: bool = True, \n                   distance: float = np.inf,\n                   case_fold: bool = True,\n                   terminals_only: bool = True,\n                   index_path: tuple[int] = tuple()) -&gt; list[FuzzyFindResult]:\n        \n        '''Find the (closest) strings within a distance of the set defined by a regex\n        \n        Defaults to computing the closest strings among the terminals and ignoring case\n        \n        Parameters\n        ----------\n        data\n            the regex to match against\n        closest\n            whether to return only the closest strings or all strings within distance\n        distance\n            the distance within which a string must be\n        case_fold\n            whether to lower-case the data\n        terminals_only\n            whether to only search the terminals\n        index_path\n        '''\n        raise NotImplementedError\n\nWrite tests analogous to the ones you wrote for Task 1.\n\n# write tests here"
  },
  {
    "objectID": "assignments/assignments-5-and-6.html",
    "href": "assignments/assignments-5-and-6.html",
    "title": "Assignments 5 and 6",
    "section": "",
    "text": "Like Assignments 1-4, Assignments 5 and 6 are bundled together. You only need to do Tasks 1 and 2 for Assignment 5 and Task 3 for Assignment 6. There is additionally a Task 4 that you are not required to even attempt because of its difficulty. I have left that task in in case you’d like to take a crack at it. Whether or not you actually attempt Task 4, please at least read through the entire notebook to see what the task entails and how you might handle it.\nThese assignments focus on implementing a natural language inference system. In natural language inference, we receive a premise sentence and a hypothesis sentence and we must say whether we can infer the premise from the hypothesis. For instance, if (1) were our premise and (2) were our hypothesis, our system should respond yes.\nIn MacCartney & Manning 2009 (henceforth, M&M), you read about one sort of system for doing this: a natural logic system. This system works by (i) obtaining an edit path from the premise and the hypothesis; (ii) mapping that edit path into an inference path; (iii) computing the join of the inferences in this path to obtain a relation between the premise and the hypothesis; and (iv) checking whether there is a forward entailment relation between the premise and the hypothesis.\nThe definition of the relations is given in Table 2 of the paper.\nThe table of joins is given below.\nrelations = ['≡', '[', ']', '^', '|', 'u', '#']\n\njoin_table = {('≡', '≡'): {'≡'},\n              ('≡', '['): {'['},\n              ('≡', ']'): {']'},\n              ('≡', '^'): {'^'},\n              ('≡', '|'): {'|'},\n              ('≡', 'u'): {'u'},\n              ('≡', '#'): {'#'},\n              ('[', '≡'): {'['},\n              ('[', '['): {'['},\n              ('[', ']'): {'#', '|', '≡', '[', ']'},\n              ('[', '^'): {'|'},\n              ('[', '|'): {'|'},\n              ('[', 'u'): {'#', '^', 'u', '|', '['},\n              ('[', '#'): {'#', '|', '['},\n              (']', '≡'): {']'},\n              (']', '['): {'#', 'u', '≡', '[', ']'},\n              (']', ']'): {']'},\n              (']', '^'): {'u'},\n              (']', '|'): {'#', '^', 'u', '|', ']'},\n              (']', 'u'): {'u'},\n              (']', '#'): {'#', 'u', ']'},\n              ('^', '≡'): {'^'},\n              ('^', '['): {'u'},\n              ('^', ']'): {'|'},\n              ('^', '^'): {'≡'},\n              ('^', '|'): {']'},\n              ('^', 'u'): {'['},\n              ('^', '#'): {'#'},\n              ('|', '≡'): {'|'},\n              ('|', '['): {'[', '^', '|', 'u', '#'},\n              ('|', ']'): {'|'},\n              ('|', '^'): {'['},\n              ('|', '|'): {'#', '|', '≡', '[', ']'},\n              ('|', 'u'): {'['},\n              ('|', '#'): {'#', '|', '['},\n              ('u', '≡'): {'u'},\n              ('u', '['): {'u'},\n              ('u', ']'): {'#', '^', 'u', '|', ']'},\n              ('u', '^'): {']'},\n              ('u', '|'): {']'},\n              ('u', 'u'): {'#', 'u', '≡', '[', ']'},\n              ('u', '#'): {'#', 'u', ']'},\n              ('#', '≡'): {'#'},\n              ('#', '['): {'#', 'u', '['},\n              ('#', ']'): {']', '|', '#'},\n              ('#', '^'): {'#'},\n              ('#', '|'): {'#', '|', ']'},\n              ('#', 'u'): {'#', 'u', '['},\n              ('#', '#'): set()}\n\nprint('\\t'.join(['']+relations))\nfor r1 in relations:\n    row = '\\t'.join(''.join([r3 for r3 in relations \n                             if r3 in join_table[r1, r2]]) \n                    for r2 in relations)\n    print(f'{r1}\\t{row}\\n')\n\n    ≡   [   ]   ^   |   u   #\n≡   ≡   [   ]   ^   |   u   #\n\n[   [   [   ≡[]|#   |   |   [^|u#   [|#\n\n]   ]   ≡[]u#   ]   u   ]^|u#   u   ]u#\n\n^   ^   u   |   ≡   ]   [   #\n\n|   |   [^|u#   |   [   ≡[]|#   [   [|#\n\nu   u   u   ]^|u#   ]   ]   ≡[]u#   ]u#\n\n#   #   [u# ]|# #   ]|# [u#\nIn Tasks 1 and 2, you will be developing the core of this system, using the minimum edit distance-based edit paths we developed in class and assuming default inference relations associated with each atomic edit operation (as discussed in Section 4 of M&M). In Tasks 3 and 4, you will enrich this system with lexical relation information from WordNet (Task 3) and with more intelligent handling of inferences associated with certain environments (as discussed in Section 5 of M&M). You will then test the system on the classic FraCaS dataset."
  },
  {
    "objectID": "assignments/assignments-5-and-6.html#task-1",
    "href": "assignments/assignments-5-and-6.html#task-1",
    "title": "Assignments 5 and 6",
    "section": "Task 1",
    "text": "Task 1\nLines: 4\nDefine the __add__ magic method for the Inference class below. This method should use join_table (defined above) to produce a set of Inferences by joining two inferences—e.g. animal \\(\\sqsupset\\) dog \\(\\bowtie\\) dog \\(\\sqsupset\\) greyhound = {animal \\(\\sqsupset\\) greyhound}. __add__ must return a set because, as M&M discussed, the result of joining two relations can result in indeterminacy. (In their implementation, M&M actually treat all such indetrminate joins as #. We will not do that here, since it is useful to see why they do that.)\nImportantly, note that __add__ should not be symmetric for the same reason joins are not: animal \\(\\sqsupset\\) dog \\(\\bowtie\\) dog \\(\\sqsubset\\) mammal = {animal \\(\\equiv\\) mammal, animal \\(\\sqsupset\\) mammal, animal \\(\\sqsubset\\) mammal, animal \\(\\smile\\) mammal, animal \\(\\#\\) mammal}, but dog \\(\\sqsubset\\) mammal \\(\\bowtie\\) animal \\(\\sqsupset\\) dog isn’t even a licit join.\n\nclass Inference:\n    '''An inference from one linguistic expression to another\n    \n    Parameters\n    ----------\n    premise\n        The premise in the relation\n    hypothesis\n        The hypothesis in the relation\n    relation\n        The relation\n    '''\n    \n    def __init__(self, premise: list[str], hypothesis: list[str], relation: str):\n        if relation not in relations:\n            raise ValueError(f'relation must be in {relations}')\n        \n        self.premise = premise\n        self.hypothesis = hypothesis\n        self.relation = relation\n    \n    def __repr__(self):\n        return ' '.join(self.premise) + ' ' + self.relation + ' ' + ' '.join(self.hypothesis)\n    \n    def __hash__(self):\n        return hash((tuple(self.premise), tuple(self.hypothesis), self.relation))\n        \n    def __add__(self, other: 'Inference') -&gt; set['Inference']:\n        raise NotImplementedError\n    \n    def __eq__(self, other: 'Inference') -&gt; bool:\n        return self.premise == other.premise &\\\n               self.hypothesis == other.hypothesis &\\\n               self.relation == other.relation\n\nTest your implementation of Inference.__add__ using the Editor subclasses below.\n\nfrom abc import ABC\nfrom typing import Tuple, Optional\n\nclass Editor(ABC):\n    \n    def __init__(self, *args):\n        raise NotImplementedError\n\n    def __call__(self, input: list[str], idx: int) -&gt; Inference:\n        raise NotImplementedError\n        \n    @property\n    def input(self):\n        return self._input\n    \n    @property\n    def output(self):\n        return self._output\n        \n\nclass Substitution(Editor):\n    \"\"\"A substitution editor\n    \n    Parameters\n    ----------\n    input\n        The string in the input to replace\n    output\n        The string to replace the input string with\n    relation\n        The inference relation that results\n    \"\"\"\n    \n    default_relation = None\n    \n    def __init__(self, input: str, output: str, relation: str):\n        self._input = input\n        self._output = output\n        self._relation = relation\n        \n    def __repr__(self):\n        return f'&lt;SUB \"{self._output}\" for \"{self._input}\" resulting in {self._relation}&gt;'\n    \n    def __call__(self, input: list[str], idx: int) -&gt; Inference:\n        \"\"\"Substitute input for output at location idx\"\"\"\n        if input[idx] != self._input:\n            raise ValueError(f'SUB \"{self._input}\" -&gt; \"{self._output}\" at {idx} '\n                             f'cannot be applied to {input}')\n        \n        output = input[:idx] + [self._output] + input[(idx+1):]\n        \n        return Inference(input, output, self._relation)\n        \nclass Deletion(Editor):\n    \"\"\"A deletion editor\n    \n    Parameters\n    ----------\n    input\n        The string in the input to delete\n    relation\n        The inference relation that results\n    \"\"\"\n    \n    def __init__(self, input: str, relation: str='['):\n        self._input = input\n        self._relation = relation\n        \n    def __repr__(self):\n        return f'&lt;DEL \"{self._input}\" resulting in {self._relation}&gt;'\n        \n    def __call__(self, input: list[str], idx: int) -&gt; Inference:\n        \"\"\"Substitute input for output at location idx\"\"\"\n        if input[idx] != self._input:\n            raise ValueError(f'DEL \"{self._input}\" at {idx} '\n                             f'cannot be applied to {input}')\n        \n        output = input[:idx] + input[(idx+1):]\n        \n        return Inference(input, output, self._relation)\n        \nclass Insertion(Editor):\n    \"\"\"An insertion editor\n    \n    Parameters\n    ----------\n    input\n        The string to insert into the output\n    relation\n        The inference relation that results\n    \"\"\"\n    \n    def __init__(self,  output: str, relation: str=']'):\n        self._output = output\n        self._relation = relation\n      \n    def __repr__(self):\n        return f'&lt;INS \"{self._output}\" resulting in {self._relation}&gt;'\n    \n    def __call__(self, input: list[str], idx: int) -&gt; Inference:\n        \"\"\"Substitute input for output at location idx\"\"\"\n        output = input[:idx] + [self._output] + input[idx:]\n        \n        return Inference(input, output, self._relation)\n\nThese subclasses are initialized with input and/or output strings and a relation. For instance, “brindle” and “fawn” are two different colorings of greyhounds—no greyhound is both brindle and fawn—and so they are in the | relation. Each is at least a subsective modifier (all brindle greyhounds are greyhounds), so if we delete one, we obtain a \\(\\sqsubset\\) relation, and if we insert one, we get a \\(\\sqsupset\\) relation (the default relations for deletion and insertion, as discussed in M&M).\n\nsubstitute_fawn_for_brindle = Substitution('brindle', 'fawn', '|')\ndelete_brindle = Deletion('brindle')\ninsert_brindle = Insertion('brindle')\n\nsubstitute_fawn_for_brindle, delete_brindle, insert_brindle\n\n(&lt;SUB \"fawn\" for \"brindle\" resulting in |&gt;,\n &lt;DEL \"brindle\" resulting in [&gt;,\n &lt;INS \"brindle\" resulting in ]&gt;)\n\n\nNote that not all insertions or deletions of adjectives will be associated with \\(\\sqsubset\\) or \\(\\sqsupset\\): privative adjectives like “fake” will introduce a \\(|\\): fake greyhounds are not greyhounds (fake greyhound \\(|\\) greyhound) and greyhounds are not fake greyhounds (greyhound \\(|\\) fake greyhound).\n\ndelete_fake = Deletion('fake', relation='|')\ninsert_fake = Insertion('fake', relation='|')\n\nIndeed, most substitutions involving “fake” will also yield a \\(|\\) relation.\n\nsubstitute_fake_for_virtuosic = Substitution('virtuosic', 'fake', '|')\nsubstitute_virtuosic_for_fake = Substitution('fake', 'virtuosic', '|')\n\nBut insertion and deletion edits involving “virtuosic” should act like “brindle”.\n\ndelete_virtuosic = Deletion('virtuosic')\ninsert_virtuosic = Insertion('virtuosic')\n\nUse the following four sentences to write your tests. These tests should involve applying an edit \\(e_1\\) to sentence \\(s_i\\) to yield sentence \\(e_1(s_i)\\), then applying an edit \\(e_2\\) to \\(e_1(s_i)\\) to yield sentence. You should then combine the inferences associated with \\(e_1\\) and \\(e_2\\) using your Inference.__add__ and check that it is correct. Make sure to test at least one case where the result should be a non-singleton set of inferences.\n\ntest_sentence1 = ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\ntest_sentence2 = ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\ntest_sentence3 = ['a', 'fake', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\ntest_sentence4 = ['a', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n\n\n# write tests here"
  },
  {
    "objectID": "assignments/assignments-5-and-6.html#task-2",
    "href": "assignments/assignments-5-and-6.html#task-2",
    "title": "Assignments 5 and 6",
    "section": "Task 2",
    "text": "Task 2\nLines: 20\nWe don’t want to have to hand-compute the edits that are required to convert one sentence into another. Instead, we will use a modified form of the StringEdit class we developed in class. What we need in particular are the edit paths that that class produces.\n\nimport numpy as np\nfrom typing import Union\n\nEditorType = str\nEditLocation = int\nEditorInputOutput = Union[str, tuple[str, str]]\nEditorParameters = tuple[EditorInputOutput, EditLocation]\n\nEditPath = list[tuple[EditorType, EditorParameters]]\nAlignment = list[tuple[int, int]]\n\ndef shift_edit_path(edit_path: EditPath):\n    edit_path_shifted = []\n    shifts = []\n\n    for edit_type, (edit, idx) in edit_path:\n        \n        if edit_type == 'substitute':\n            idx -= 1\n            \n            if edit[0] == edit[1]:\n                continue\n\n        if edit_type == 'delete':\n            idx -= 1\n            shifts.append((idx, -1))\n\n        if edit_type == 'insert':\n            shifts.append((idx, 1))\n\n        for j, s in shifts[:-1]:\n            idx = idx + s if idx &gt;= j else idx     \n\n        edit_path_shifted.append((edit_type, (edit, idx)))\n\n    return edit_path_shifted\n\n\nclass StringEdit:\n    '''distance, alignment, and edit paths between strings\n\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n    \n    def __init__(self, insertion_cost: float = 1., deletion_cost: float = 1., substitution_cost: float | None = None):\n        self._insertion_cost = insertion_cost\n        self._deletion_cost = deletion_cost\n\n        if substitution_cost is None:\n            self._substitution_cost = insertion_cost + deletion_cost\n        else:\n            self._substitution_cost = substitution_cost\n\n    def __call__(self, source: list[str], target: list[str], only_distance: bool = False) -&gt;  float | tuple[float, Alignment, EditPath]:\n        return self._wagner_fisher(source, target, only_distance)\n            \n    def _wagner_fisher(self, source: list[str], target: list[str], only_distance: bool) -&gt;  float | tuple[float, Alignment, EditPath]:\n        '''compute minimum edit distance, alignment, and edit sequence'''\n\n        n, m = len(source), len(target)\n        source, target = self._add_sentinel(source, target)\n\n        distance = np.zeros([n+1, m+1], dtype=float)\n        pointers = np.zeros([n+1, m+1], dtype=list)\n        edits = np.zeros([n+1, m+1], dtype=list)\n\n        pointers[0,0] = []\n        edits[0,0] = []\n        \n        for i in range(1,n+1):\n            distance[i,0] = distance[i-1,0]+self._deletion_cost\n            pointers[i,0] = [(i-1,0)]\n            edits[i,0] = [('delete', (source[i], i))]\n\n        for j in range(1,m+1):\n            distance[0,j] = distance[0,j-1]+self._insertion_cost\n            pointers[0,j] = [(0,j-1)]\n            edits[0,j] = [('insert', (target[j], j))]\n            \n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if source[i] == target[j]:\n                    substitution_cost = 0.\n                else:\n                    substitution_cost = self._substitution_cost\n                    \n                costs = np.array([distance[i-1,j]+self._deletion_cost,\n                                  distance[i-1,j-1]+substitution_cost,\n                                  distance[i,j-1]+self._insertion_cost])\n                    \n                distance[i,j] = costs.min()\n\n                best_edits = np.where(costs==distance[i,j])[0]\n\n                indices = [(i-1,j), (i-1,j-1), (i,j-1)]\n                pointers[i,j] = [indices[k] for k in best_edits]\n \n                edit_types = list(zip([\"delete\", \"substitute\", \"insert\"],\n                                      [(source[i], i), \n                                       ((source[i], target[j]), i), \n                                       (target[j], i)]))\n                edits[i,j] = [edit_types[k] for k in best_edits]\n\n        if only_distance:\n            return distance[n,m]\n\n        pointer_backtrace, edit_backtrace = self._construct_backtrace(pointers, edits)\n        \n        return distance[n,m], pointer_backtrace, [shift_edit_path(bt) for bt in edit_backtrace]\n\n    def _construct_backtrace(self, pointers, edits):\n        last_idx = (\n            pointers.shape[0] - 1, \n            pointers.shape[1] - 1\n        )\n        \n        incomplete_pointer_backtraces = [([last_idx], [])]\n        complete_pointer_backtraces = []\n        \n        complete_edit_backtraces = []\n\n        while incomplete_pointer_backtraces:\n            new_backtraces = [\n                ([ptr] + ptr_bt, [edit] + edit_bt)\n                for ptr_bt, edit_bt in incomplete_pointer_backtraces\n                for ptr, edit in zip(pointers[ptr_bt[0]], edits[ptr_bt[0]])\n            ]\n\n            complete_pointer_backtraces += [\n                bt for bt, _ in new_backtraces\n                if bt[0] == (0, 0)\n            ]\n            complete_edit_backtraces += [\n                edit_bt for ptr_bt, edit_bt in new_backtraces\n                if ptr_bt[0] == (0, 0)\n            ]\n\n            incomplete_pointer_backtraces = [\n                (ptr_bt, edit_bt) \n                for ptr_bt, edit_bt in new_backtraces\n                if ptr_bt[0] != (0, 0)\n            ]\n            \n        return complete_pointer_backtraces, complete_edit_backtraces\n        \n    def _add_sentinel(self, source, target):\n        if isinstance(source, str):\n            source = '#'+source\n        elif isinstance(source, list):\n            source = ['#'] + source\n        elif isinstance(source, tuple):\n            source = ('#',) + source\n        else:\n            raise ValueError('source must be str, list, or tuple')\n            \n        if isinstance(target, str):\n            target = '#' + target\n        elif isinstance(target, list):\n            target = ['#'] + target\n        elif isinstance(target, tuple):\n            target = ('#',) + target\n        else:\n            raise ValueError('target must be str, list, or tuple')\n            \n        return source, target\n\nIn the original implementation, the edit path indexed into the source string. This made sense at the time because we wanted to know which words, relative to their original position in the string, are operated on by an edit. It’s problematic for current purposes, because once we compute insertions and deletions, the position of later insertions or deletions change. The implementation below now corrects for this, but just make sure you’re taking into account that the order of edits matters for this reason.\n\neditdist = StringEdit(1, 1, 1)\n\nprint('Source:   ', test_sentence1)\nprint('Target:   ', test_sentence2)\nprint('Pointer path:', editdist(test_sentence1, test_sentence2)[1])\nprint('Edit path:', editdist(test_sentence1, test_sentence2)[2])\n\nSource:    ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\nTarget:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\nPointer path: [[(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)]]\nEdit path: [[('delete', ('virtuosic', 1)), ('delete', ('brindle', 5))]]\n\n\n\nprint('Source:   ', test_sentence2)\nprint('Target:   ', test_sentence1)\nprint('Edit path:', editdist(test_sentence2, test_sentence1)[2])\n\nSource:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\nTarget:    ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\nEdit path: [[('insert', ('virtuosic', 1)), ('insert', ('brindle', 6))]]\n\n\nImplement the __call__ method for the NaturalLogic class. This should take a premise sentence and a hypothesis sentence, and it should produce two things: (i) the path of inferences (computed from the path of edits) that take you from premise to hypothesis; and (ii) the set of inferences that results from joining the inferences in that path.\n\nEditorLibrary = dict[str, dict[EditorInputOutput, Editor]]\nInferencePath = list[Inference]\nNaturalLogicResult = tuple[InferencePath, set[Inference]]\n\nempty_library = {'substitute': {}, 'delete': {}, 'insert': {}}\n\nclass NaturalLogic:\n    \n    EDIT = StringEdit(1, 1, 1)\n    \n    def __init__(self, editor_library: EditorLibrary=empty_library):\n        self._editor_library = editor_library\n    \n    def __getitem__(self, key: tuple[EditorType, EditorInputOutput]):\n        editor_type, edit = key\n        \n        if edit not in self._editor_library[editor_type]:\n            self._add_default_editor(editor_type, edit)\n            \n        return self._editor_library[editor_type][edit]\n    \n    def __call__(self, premise: list[str], hypothesis: list[str]) -&gt; list[NaturalLogicResult]:\n        raise NotImplementedError\n    \n    def add_editor(self, editor: Editor):\n        if isinstance(editor, Substitution):\n            self._editor_library['substitute'][(editor.input, editor.output)] = editor\n            \n        if isinstance(editor, Insertion):\n            self._editor_library['insert'][editor.output] = editor\n            \n        if isinstance(editor, Deletion):\n            self._editor_library['delete'][editor.input] = editor\n            \n    def _add_default_editor(self, editor_type: str, edit: EditorInputOutput):\n        if editor_type == 'substitute':\n            self.add_editor(Substitution(input=edit[0], output=edit[1], relation='#'))\n        \n        elif editor_type == 'insert':\n            self.add_editor(Insertion(output=edit))\n            \n        elif editor_type == 'delete':\n            self.add_editor(Deletion(input=edit))\n            \n        else:\n            raise ValueError(f'{editor_type} is not a recognized edit type')\n            \n\nImplement tests using the four test sentences above. For now, you can just assume that the editor library contains the editors defined for Task 1. (We don’t need to explicitly specify any insertions that result in \\(\\sqsupset\\) or deletions that result in \\(\\sqsubset\\), since those are added by default by NaturalLogic.add_editor.) In Task 3, we will expand the library using WordNet.\n\nlibrary = {'substitute': {('virtuosic', 'fake'): substitute_fake_for_virtuosic,\n                          ('fake', 'virtuosic'): substitute_virtuosic_for_fake,\n                          ('brindle', 'fawn'): substitute_fawn_for_brindle}, \n           'delete': {\"fake\": delete_fake}, \n           'insert': {\"fake\": insert_fake}}\n\n\n# write tests here"
  },
  {
    "objectID": "assignments/assignments-5-and-6.html#evaluating-against-fracas",
    "href": "assignments/assignments-5-and-6.html#evaluating-against-fracas",
    "title": "Assignments 5 and 6",
    "section": "Evaluating against FraCaS",
    "text": "Evaluating against FraCaS\nFor the remainder of the assignment (Tasks 3 and 4), we will evaluate our NaturalLogic implementation using the FraCaS textual inference test suite. FraCaS is shipped as XML.\n\n%%bash\n\nwget https://nlp.stanford.edu/~wcmac/downloads/fracas.xml\ncat fracas.xml\n\nI’ve included a simple corpus reader below.\n\n!pip install beautifulsoup4\n!pip install lxml\n\n\nfrom bs4 import BeautifulSoup, Tag\n\nclass Fracas:\n    \"\"\"Corpus reader for the FraCaS textual inference problem set\"\"\"\n    \n    def __init__(self, root: str=\"fracas.xml\"):\n        with open(root) as fp:\n            self._data = BeautifulSoup(fp, 'lxml')\n            \n        self._construct_problem_generator()\n            \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        return next(self._problem_generator)\n    \n    def __repr__(self):\n        return self._data.comment.string\n     \n    def _construct_problem_generator(self):\n        for problem in self.problems:\n            yield problem\n    \n    @property\n    def problems(self):\n        return [FracasProblem(problem) \n                for problem in self._data.find_all('problem')]\n\nclass FracasProblem:\n    \"\"\"A FraCaS problem\"\"\"\n    \n    problem_type_map = {'001'}\n    \n    def __init__(self, problem: Tag):\n        self.id = problem.get('id')\n        self.answer = problem.get('fracas_answer')\n        \n        self.premise = problem.p.string.strip()\n        self.question = problem.q.string.strip()\n        self.hypothesis = problem.h.string.strip()\n        \n    def __repr__(self):\n        return (f\"id: {self.id}\"\n                f\"\\n\\npremise: {self.premise}\"\n                f\"\\nquestion: {self.question}\"\n                f\"\\nhypothesis: {self.hypothesis}\"\n                f\"\\n\\nanswer: {self.answer}\")\n\n\nfracas = Fracas()\n\nfracas\n\nSince the sentences are just raw strings, to get them in the form of a list of strings, you will need a tokenizer. I would suggest using the one available in the stanza package. For our purposes, it is also simpler to use the lemma, rather than the token itself, because your WordNet editor library won’t handle inflectional morphology (unless you explicitly engineered it to).\n\n!pip install stanza\n\nimport stanza\n\nstanza.download('en')\nlemmatizer = stanza.Pipeline('en', processors='tokenize, mwt, pos, lemma')\n\nlemmatizer('Every virtuosic synthesist loves some greyhounds.')\n\nTo use this dataset to test your NaturalLogic implementation, you will need to convert the inference produced by __call__ into a “yes”, “no”, or “don’t know” answer. (Don’t worry about any items not labeled with one of these three. This will require you to define a mapping from inference types to answers. You should then compute the accuracy, precision, recall, and F1 of your system.\nEach of these metrics can be defined in terms of…\n\nThe true positive count for class \\(c\\): \\[\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i = \\hat{y}^\\mathrm{test}_i = c\\}|\\]\nThe true negative count for class \\(c\\): \\[\\mathrm{tn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i \\neq c \\land \\hat{y}^\\mathrm{test}_i \\neq c\\}|\\]\nThe false positve count for class \\(c\\): \\[\\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i \\neq c \\land \\hat{y}^\\mathrm{test}_i = c\\}|\\]\nThe false negative count for class \\(c\\): \\[\\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i = c \\land \\hat{y}^\\mathrm{test}_i \\neq c\\}|\\]\n\n…where the class is “yes”, “no”, or “unknown”; \\(y^\\mathrm{test}_i\\) is the true label for item \\(i\\) (found in FraCaS) and \\(\\hat{y}^\\mathrm{test}_i\\) is your system’s prediction for the class of item \\(i\\). (Ignore cases where the class is not one of these three.)\n\nAccuracy\nFor what proportion of the test data \\(\\{(x^\\mathrm{test}_{1}, y^\\mathrm{test}_1), ..., (x^\\mathrm{test}_N, y^\\mathrm{test}_N)\\}\\) does the model’s predicted class \\(f(x^\\mathrm{test}_i) = \\hat{y}^\\mathrm{test}_i\\) for an item match the ground truth class for that item?\n\\[\\mathrm{accuracy}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{\\sum_{c \\in \\mathcal{Y}}\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{tn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{N}\\]\nsklearn.metrics technically provides an accuracy_score function, but generally it’s just as straightforward to compute it yourself.\n\n!pip install sklearn\n\n\nfrom sklearn.metrics import accuracy_score\n\n\n\nPrecision\nFor a particular class \\(c\\), what proportion of the test items that the model said have that class actually have that class?\n\\[\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}\\]\nFor giving an aggregate precision across classes, it’s common to distinguish micro-average precision and macro-average precision.\n\\[\\mathrm{microprecision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{\\sum_{c \\in \\mathcal{Y}} \\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\sum_{c \\in \\mathcal{Y}} \\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}\\]\n\\[\\mathrm{macroprecision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{1}{|\\mathcal{Y}|}\\sum_{c \\in \\mathcal{Y}} \\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)\\]\n\nfrom sklearn.metrics import precision_score\n\n\n\nRecall\nFor a particular class \\(c\\), what proportion of the test items that have that class did the model correctly predict to have that class?\n\\[\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}\\]\nSimilar definitions for micro- and macro-average recall can be given.\n\nfrom sklearn.metrics import recall_score\n\n\n\nF1\nFor a class \\(c\\), what is the harmonic mean of precision and recall?\n\\[F_1\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{2}{\\frac{1}{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)} + \\frac{1}{\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}} = 2\\frac{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)\\;\\cdot\\;\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) + \\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}\\]\nTo define micro- and macro-average \\(F_1\\) it can be useful to reexpress it.\n\\[F_1\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{2\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{2\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}\\]\nDefinitions similar to those for precision can be given for micro- and macro-average \\(F_1\\).\n\nfrom sklearn.metrics import f1_score"
  },
  {
    "objectID": "assignments/assignments-5-and-6.html#task-3",
    "href": "assignments/assignments-5-and-6.html#task-3",
    "title": "Assignments 5 and 6",
    "section": "Task 3",
    "text": "Task 3\nDefine an instance method NaturalLogic.load_wordnet that constructs an editor library from WordNet hypernymy, hyponymy, and antonymy relations.\n\n!pip install nltk\n\nimport nltk\n\nnltk.download('wordnet')\n\nfrom nltk.corpus import wordnet\n\n\nclass NaturalLogic(NaturalLogic):\n\n    def load_wordnet(self):\n        raise NotImplementedError\n    \n    @classmethod\n    def from_wordnet(cls):\n        natlog = cls()\n        natlog.load_wordnet()\n        \n        return natlog\n\nTest your new library by writing examples that require knowledge of hypernymy, hyponymy, and antonymy to correctly handle.\n\n# write tests here\n\nEvaluate your new library on FraCaS by computing precision, recall, and F1 for the items that are either labeled “yes”, “no”, or “don’t know”. Remember that this is going to require you to define a way of mapping inference types to answers.\n\n# write evaluation here\n\nThese numbers will be bad. The point is to see that handling even the apparently simple cases in FraCaS is very difficult, even with a fairly extensive edit library. PArt of the reason for this is that we are not handling quantification or negation at all."
  },
  {
    "objectID": "assignments/assignments-5-and-6.html#task-4",
    "href": "assignments/assignments-5-and-6.html#task-4",
    "title": "Assignments 5 and 6",
    "section": "Task 4",
    "text": "Task 4\nUpdate your implementation of NaturalLogic.__call__ to correctly handle negation and the quantifiers discussed in Section 5 of M&M. Assume that “a” behaves as “some”; that “all” and “each” behave like “every”; that “not all” behaves like “not every”; and that “none” behaves like “no”. (There are many quantifiers this won’t cover—e.g. “most”, “many”, etc.—don’t worry about trying to figure out what the projectivity signatures for these looks like.)\nTo do this, you will need to identify the first and second arguments of the quantifier. For instance, for (3), the first argument of “every” is “virtuosic synthesist” and the second argument is “loves a grehound”. (If you’ve taken semantics, you know I’m fudging a little here.)\n\nEvery virtuosic synthesist loves some grehyound.\n\nI have provided an implementation below. This implementation will only work for simple cases, like (3). Finding the arguments of quantifiers in general can be highly nontrivial for reason you’ll need to take a formal semantics course to truly appreciate.\n\nfrom collections import defaultdict\nfrom functools import lru_cache\n\nclass DependencyParse:\n    \n    parser = stanza.Pipeline('en')\n    \n    def __init__(self, sentence: str):\n        self.parsed_sentence = self.parser(sentence).sentences[0].words\n    \n    def parent_of(self, idx: int) -&gt; dict[str, Union[int, str]]:\n        paridx = self.parsed_sentence[idx].head - 1\n        return self.parsed_sentence[paridx]\n    \n    @lru_cache(256)\n    def children_of(self, idx: int, closure: bool=False):\n        immediate_children = [word \n                              for word in self.parsed_sentence \n                              if word.head == (idx+1)]\n        \n        if closure:\n            return immediate_children +\\\n                   [word for child in immediate_children \n                    for word in self.children_of(child.id-1, closure)]\n        \n        else:\n            return immediate_children\n        \n    def find_quantifier_arguments(self):\n        \"\"\"Find the first and second arguments of each quantifier in the sentence.\"\"\"\n\n        first_argument = defaultdict(list)\n        second_argument = defaultdict(list)\n        \n        for quantifier in self.parsed_sentence:\n            if quantifier.lemma in ['every', 'all', 'some', 'a', 'no', 'none']:\n                parent = self.parent_of(quantifier.id-1)\n                first_argument[quantifier.id-1].append(parent)\n\n                for dependent in self.children_of(parent.id-1):\n                    if quantifier.id != dependent.id:\n                        first_argument[quantifier.id-1].append(dependent)\n\n                predicate = self.parent_of(parent.id-1)\n\n                second_argument[quantifier.id-1] = [predicate] +\\\n                                                   [word \n                                                    for word in self.children_of(predicate.id-1,\n                                                                                 closure=True) \n                                                    if word not in first_argument[quantifier.id-1] \n                                                    if quantifier.id != word.id]\n\n        return first_argument, second_argument\n            \n\nNote that this implementation produces second arguments for one quantifier that can overlap with the first arguments of another.\n\nquantifier_arguments = DependencyParse('Every virtuosic synthesist loves a greyhound.').find_quantifier_arguments()\n    \nprint('first arguments\\n\\n', quantifier_arguments[0].values())\nprint()\nprint()\nprint('second arguments\\n\\n', quantifier_arguments[1])\n\nThis overlap means that you are going to need to choose an order in which to project the inference relations through the quantifiers—e.g. in (3), do you first consider “every”, then “a”; or vice versa. This order will necessarily be heuristic. A reasonable order might be the reverse of the linear (or “surface”) order. Other options might be based on depth in the tree.\n\nclass NaturalLogic(NaturalLogic):\n\n    def __call__(self, premise: list[str], hypothesis: list[str]) -&gt; tuple[list[InferencePath], \n                                                                           set[Inference]]:\n        raise NotImplementedError\n\nApply the same evaluation you developed for Task 3 to your new implementation of NaturalLogic.__call__.\n\n# write evaluation here"
  },
  {
    "objectID": "assignments/assignments-7-and-8.html",
    "href": "assignments/assignments-7-and-8.html",
    "title": "Assignments 7 and 8",
    "section": "",
    "text": "Assignment 7 will consist of Tasks 1-3 and Assignment 8 will consist of Tasks 4-5.\nIn these assignments, you will be implementing finite state automata for generating/recognizing/parsing English syllables and words (Assignment 7) as well as finite state transducers for recognizing/parsing/transducing English sentences (Assigment 8). You will implement these using the FiniteStateAutomaton class that we developed in class as well as a FiniteStateTransducer class that builds on it."
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#finite-state-automata",
    "href": "assignments/assignments-7-and-8.html#finite-state-automata",
    "title": "Assignments 7 and 8",
    "section": "Finite State Automata",
    "text": "Finite State Automata\nThe first set of utilities are used for determinization and epsilon closure.\n\nfrom functools import reduce\nfrom itertools import chain, combinations\n\nfrom typing import Set, Tuple\n\ndef powerset(iterable):\n    \"\"\"https://docs.python.org/3/library/itertools.html#recipes\"\"\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\ndef transitive_closure(edges: set[Tuple[str]]) -&gt; set[Tuple[str]]:\n    \"\"\"\n    the transitive closure of a graph\n\n    Parameters\n    ----------\n    edges\n        the graph to compute the closure of\n\n    Returns\n    ----------\n    set(tuple)\n    \"\"\"\n    while True:\n        new_edges = {(x, w)\n                     for x, y in edges\n                     for q, w in edges\n                     if q == y}\n\n        all_edges = edges | new_edges\n\n        if all_edges == edges:\n            return edges\n\n        edges = all_edges\n\ntransitive_closure in particular is useful in computing the epsilon closure (though epsilon closure requires quite a bit more code, as you can see below).\n\ntransitive_closure({(0, 1), (1, 2), (2, 3), (3, 4)})\n\nAnd here’s the base FiniteStateAutomaton class, which relies on a TransitionFunction class to hide some of the operations on transition functions that can get nasty.\n\nfrom logging import warn\nfrom copy import copy, deepcopy\nfrom functools import lru_cache\nfrom typing import Literal\n\nTransitionGraph = dict[tuple[str, str], str | set[str]]\nFSAParse = tuple[tuple[str, str]]\nFSAMode = Literal[\"recognize\", \"parse\"]\n\nclass FiniteStateAutomaton:\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __init__(self, alphabet: set[str], states: set[str], \n                 initial_state: str, final_states: set[str], \n                 transition_graph: TransitionGraph):\n        self._alphabet = alphabet | {''}\n        self._states = states\n        self._initial_state = initial_state\n        self._final_states = final_states\n        self._transition_function = TransitionFunction(\n            self._alphabet, \n            states,\n            transition_graph\n        )\n\n        self._validate_initial_state()\n        self._validate_final_states()\n\n        self._generator = self._build_generator()\n\n    def __contains__(self, string):\n        return self._recognize(string)\n        \n    def __add__(self, other):\n        return self.concatenate(other)\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def __and__(self, other):\n        return self.intersect(other)\n\n    def __pow__(self, k):\n        return self.exponentiate(k)\n\n    def __neg__(self):\n        return self.complement()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._generator)\n\n    def _build_generator(self):\n        string_buffer = [(self._initial_state, '')]\n        \n        if self._initial_state in self._final_states:\n            stack = ['']\n        else:\n            stack = []\n\n        while string_buffer:\n            if stack:\n                yield stack.pop()\n\n            else:\n                # this is very inefficient when we have total\n                # transition functions with many transitions to the\n                # sink state; could be optimized by removing a string\n                # if it's looping in a sink state, but we don't know\n                # in general what the sink state is\n                new_buffer = []\n                for symb in self._alphabet:\n                    for old_state, string in string_buffer:\n                        new_states = self._transition_function(old_state, symb)\n                        for st in new_states:\n                            new_elem = (st, string+symb)\n                            new_buffer.append(new_elem)\n\n                stack += [string\n                          for state, string in new_buffer\n                          if state in self._final_states]\n\n                string_buffer = new_buffer\n    \n    def __call__(self, string: str | list[str], mode: FSAMode = \"recognize\") -&gt; bool | set[FSAParse]:\n        \"\"\"\n        whether/how a string is accepted/parsed by the FSA\n\n        Parameters\n        ----------\n        string\n            the string to recognize or parse\n        mode\n            whether to run in \"recognize\" or \"parse\" mode\n        \"\"\"\n\n        if mode == 'recognize':\n            return self._recognize(string)\n        elif mode == 'parse':\n            return self._parse(string)\n        else:\n            msg = 'mode must be \"recognize\" or \"parse\"'\n            raise ValueError(msg)\n\n    def _add_epsilon_extensions(self, paths: set[tuple[str]]) -&gt; set[tuple[str]]:\n        paths_extended = {\n            s1 + (s2,) \n            for s1 in paths \n            for s2 in self._transition_function(s1[-1], '')\n        }\n        \n        break_counter = 0\n\n        while not paths &gt;= paths_extended:\n            paths |= paths_extended\n            paths_extended = {\n                s1 + (s2,) \n                for s1 in paths\n                for s2 in self._transition_function(s1[-1], '')\n            }\n\n            break_counter += 1\n\n            if break_counter &gt; 10:\n                warn(\"epsilon path limit reached\")\n                break\n            \n        return paths\n\n    @lru_cache(512)\n    def _recognize(self, string: str | list[str], prev_state: str | None = None) -&gt; bool:\n        \"\"\"Whether a string is accepted by the FSA\n\n        Parameters\n        ----------\n        string\n            the string to recognize or parse\n        \"\"\"\n        paths = {(self._initial_state,)} if prev_state is None else {(prev_state,)}\n        paths = self._add_epsilon_extensions(paths)\n\n        if string:\n            return any(\n                self._recognize(string[1:], state)\n                for p in paths\n                for state in self._transition_function(p[-1], string[0])\n            )\n\n        else:\n            return any(\n                s[-1] in self._final_states for s in paths\n            )\n\n    @lru_cache(512)\n    def _parse(self, string: str | list[str], \n               prev_state: str | None = None) -&gt; set[FSAParse]:\n        \"\"\"How a string is parsed by the FSA\n        \n        This should return the list of transitions that \n        the machine could go through to parse a string\n\n        Parameters\n        ----------\n        string\n            the string to recognize or parse\n        \"\"\"\n        paths = {(self._initial_state,)} if prev_state is None else {(prev_state,)}\n        paths = self._add_epsilon_extensions(paths)\n\n        if string:\n            return {\n                tuple((s,'') for s in p[1:]) + ((state, string[0]),) + parse\n                for p in paths\n                for state in self._transition_function(p[-1], string[0])\n                for parse in self._parse(string[1:], state)\n            }\n\n        else:\n            return {\n                tuple((s,'') for s in p[1:])\n                for p in paths\n                if p[-1] in self._final_states\n            }\n\n    def _validate_initial_state(self):\n        try:\n            assert self._initial_state in self._states\n\n        except AssertionError:\n            raise ValueError('initial state must be in set of states')\n\n    def _validate_final_states(self):\n        try:\n            assert all([s in self._states for s in self._final_states])\n\n        except AssertionError:\n            raise ValueError('final states must all be in set of states')\n\n    def _deepcopy(self):\n        fsa = copy(self)\n        del fsa._generator\n\n        return deepcopy(fsa)\n        \n    def _relabel_fsas(self, other):\n        \"\"\"\n        append tag to the input/ouput states throughout two FSAs\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n        \"\"\"\n\n        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n\n        return fsa1, fsa2\n\n    def _relabel_states(self, tag: str) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        state_map = {s: s+'_'+tag for s in self._states}\n        \n        self._states = {state_map[s] for s in self._states}    \n        self._initial_state += '_'+tag\n        self._final_states = {state_map[s] for s in self._final_states}\n\n        self._transition_function.relabel_states(state_map)\n        \n        return self\n\n    def concatenate(self, other: 'FiniteStateAutomaton'):\n        \"\"\"\n        concatenate this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.concatenate\"\n        raise NotImplementedError(msg)\n\n    def kleene_star(self)  -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        construct the kleene closure machine\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        fsa = self._deepcopy()\n        \n        new_transition = fsa._transition_function\n        new_transition.add_transitions({(s, ''): fsa._initial_state\n                                        for s in fsa._final_states})\n\n        return FiniteStateAutomaton(fsa._alphabet, fsa._states,\n                                    fsa._initial_state, fsa._final_states,\n                                    new_transition.transition_graph)\n\n    def exponentiate(self, k: int) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        concatenate this FSA k times\n\n        Parameters\n        ----------\n        k : int\n            the number of times to repeat; must be &gt;1\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        if k &lt;= 1:\n            raise ValueError(\"must be &gt;1\")\n\n        new = self\n\n        for i in range(1,k):\n            new += self\n\n        return new\n\n    def union(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        union this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.union\"\n        raise NotImplementedError(msg)\n\n    def complement(self) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        complement this FSA\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        fsa = self._deepcopy()\n        fsa = fsa.determinize()\n        fsa._transition_function.totalize()\n        fsa._final_states = fsa._states - fsa._final_states\n\n        return fsa\n\n    def intersect(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        intersect this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        fsa1 = self.complement()\n        fsa2 = other.complement()\n\n        return fsa1.union(fsa2).complement()\n        \n    def determinize(self) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        if nondeterministic, determinize the FSA\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        new_states, new_initial_state, new_transition_graph = self._transition_function.determinize(self._initial_state)\n\n        new_states = {'_'.join(sorted(s)) for s in new_states}\n        new_initial_state = \"_\".join(sorted(new_initial_state))\n        new_final_states = {\n            '_'.join(sorted(s)) for s in powerset(self._states)\n            if any([t in s for t in self._final_states])\n            if s\n        }\n        new_transition_graph = {\n            (\"_\".join(sorted(instates)), symb): {\"_\".join(sorted(o)) for o in outstates}\n            for (instates, symb), outstates in new_transition_graph.items()\n        }\n\n        return FiniteStateAutomaton(\n            self._alphabet-{''}, \n            new_states,\n            new_initial_state, \n            new_final_states,\n            new_transition_graph\n        )\n\n    @property\n    def isdeterministic(self) -&gt; bool:\n        return self._transition_function.isdeterministic\n\nclass TransitionFunction(object):\n    \"\"\"\n    A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic : bool\n    istotalfunction : bool\n    transition_graph : dict\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def __init__(self, alphabet: set[str], states: set[str], transition_graph: TransitionGraph):\n        self._alphabet = alphabet\n        self._states = states\n        self._transition_graph = transition_graph\n\n        self._validate()\n\n    def __call__(self, state: str, symbol: str) -&gt; set[str]:\n        try:\n            return self._transition_graph[(state, symbol)]\n        except KeyError:\n            return set({})\n\n    def __or__(self, other: 'TransitionFunction') -&gt; 'TransitionFunction':\n        return TransitionFunction(\n            dict(\n                self._transition_graph, \n                **other._transition_graph\n            )\n        )\n\n    def _add_epsilon_transitive_closure(self):\n        # get the state graph of all epsilon transitions\n        transitions = {\n            (instate, outstate)\n            for (instate, insymb), outs in self._transition_graph.items()\n            for outstate in outs if not insymb\n        }\n        \n        # compute the transitive closure of the epsilon transition\n        # state graph; requires homogenization beforehand\n        for instate, outstate in transitive_closure(transitions):\n            self._transition_graph[(instate, '')] |= {outstate}\n\n        new_graph = dict(self._transition_graph)\n\n        # add alphabet transitions from all states q_i that exit\n        # with an alphabet transition (q_i, a) and enter states q_j\n        # with epsilon transitions to states q_k\n        for (instate1, insymb1), outs1 in self._transition_graph.items():\n            for (instate2, insymb2), outs2 in self._transition_graph.items():\n                if instate2 in outs1 and not insymb2:\n                    # vacuously adds the already added epsilon\n                    # transitions as well\n                    try:\n                        new_graph[(instate1,insymb1)] |= outs2\n                    except KeyError:\n                        new_graph[(instate1,insymb1)] = outs2\n\n        return new_graph\n\n    def determinize(self, initial_state: str) -&gt; tuple[set[frozenset[str]], frozenset[str], dict[frozenset[str],str], frozenset[str]]:\n        # define the new states as the elements of the power set of the old \n        # states\n        new_states = {frozenset(s) for s in powerset(self._states) if s}\n\n        # add epsilon transitive closure\n        epsilon_closed_graph = self._add_epsilon_transitive_closure()\n\n        # the new initial states is the set containing the old initial set \n        # along with any state that could be reached by an epsilon transition\n        # from the original initial state; because we have formed the epsilon\n        # transitive closure, we only need to take one hope from the initial\n        # state to get all such states \n        new_initial_state = frozenset(\n            {initial_state} | epsilon_closed_graph[initial_state, '']\n        )\n\n        # filter epislon transitions\n        filtered_transition_graph = {\n            (instate, insymb): outstates\n            for (instate, insymb), outstates in epsilon_closed_graph.items()\n            if insymb\n        }\n\n        # construct the new transition graph\n        new_transition_graph = {\n            (frozenset(s), a): {frozenset(t)}\n            for s in new_states\n            for t in new_states\n            for a in self._alphabet\n            if s and t\n            if any(\n                (sprime, a) in filtered_transition_graph and \n                tprime in filtered_transition_graph[sprime, a]\n                for sprime in s for tprime in t\n            )\n        }\n\n        return new_states, new_initial_state, new_transition_graph\n        \n    def add_transitions(self, transition_graph):\n        self._transition_graph.update(transition_graph)\n\n    def _validate(self):\n        self._validate_input_values()\n        self._validate_output_types()\n        self._homogenize_output_types()\n        self._validate_output_values()\n\n    def _validate_input_values(self):\n        for state, symbol in self._transition_graph.keys():\n            if symbol not in self._alphabet:\n                msg = 'all input symbols in transition function ' +\\\n                      'must be in alphabet'\n                raise ValueError(msg)\n\n            if state not in self._states:\n                msg = 'all input states in transition function ' +\\\n                      'must be in set of states'\n                raise ValueError(msg)\n\n    def _validate_output_types(self):\n        for states in self._transition_graph.values():\n            if type(states) is not str and type(states) is not set:\n                msg = 'all outputs in transition function ' +\\\n                      'must be specified via str or set'\n                raise ValueError(msg)            \n\n    def _homogenize_output_types(self):\n        for inp, out in self._transition_graph.items():\n            if type(out) is str:\n                self._transition_graph[inp] = {out}\n\n    def _validate_output_values(self):\n        for out in self._transition_graph.values():\n            try:\n                assert all([state in self._states for state in out])\n            except AssertionError:\n                msg = 'all output symbols in transition function' +\\\n                      'must be in states'\n                raise ValueError(msg)\n\n    @property\n    def isdeterministic(self):\n        return all(\n            len(v) &lt; 2 for v in self._transition_graph.values()\n        )\n\n    @property\n    def istotalfunction(self):\n        return all(\n            (s, a) in self._transition_graph\n            for s in self._states\n            for a in self._alphabet\n        )\n\n    def relabel_states(self, state_map: dict[str, str]):\n        \"\"\"Append tag to the input/ouput states in the transition function\n\n        Parameters\n        ----------\n        state_map\n            A mapping from old states to new states\n        \"\"\"\n\n        new_transition_graph = {}\n\n        for (instate, insymb), outs in self._transition_graph.items():\n            new_inp = (state_map[instate], insymb)\n            new_outs = {state_map[o] for o in outs}\n            new_transition_graph[new_inp] = new_outs\n\n        self._transition_graph = new_transition_graph\n\n    def totalize(self):\n        if not self.istotalfunction:\n            domain = {\n                (s, a) \n                for s in self._states \n                for a in self._alphabet\n            }\n\n            sink_state = 'qsink'\n\n            while sink_state in self._states:\n                sink_state += 'sink'\n\n            for inp in domain:\n                if inp not in self._transition_graph:\n                    self._transition_graph[inp] = {sink_state}\n\n    @property\n    def transition_graph(self):\n        return self._transition_graph\n\nAn FSA can then be defined in a way that pretty closely matches the formal definition.\n\nfsa1 = FiniteStateAutomaton(alphabet={\"a\", \"b\", \"\"},\n                            states={\"q0\", \"q1\", \"q2\"},\n                            initial_state=\"q0\",\n                            final_states={\"q0\", \"q1\"},\n                            transition_graph={(\"q0\", \"a\"): {\"q0\", \"q1\"},\n                                              (\"q0\", \"\"): {\"q1\"},\n                                              (\"q1\", \"\"): {\"q2\"},\n                                              (\"q1\", \"a\"): {\"q0\"},\n                                              (\"q1\", \"b\"): {\"q0\"}})\n\nYou won’t need to ever access the transition function directly, but note that it won’t necessarily be equivalent to the dictionary you passed, since the epsilon closure is computed when the TransitionFunction is initialized. (This means that the machine may only be weakly equivalent to the one defined, since computing the epsilon transitive closure may add edges that allow states along an epsilon-only path the be skipped.)\n\nfsa1._transition_function._transition_graph\n\nYou also won’t need to determinize the FSA directly, but just to show you that this is also implemented:\n\nfsa1_det = fsa1.determinize()\n\nfsa1_det._initial_state\n\n\nfsa1_det._transition_function._transition_graph\n\nAnd as expected, our original FSA is nondeterministic, while the determinized version is deterministic.\n\nfsa1.isdeterministic, fsa1_det.isdeterministic\n\nYou also won’t need to compute the complement directly—it is used as part of computing intersection—but I have implemented that operation as well.\n\nfsa1_comp = fsa1.complement()\n\nprint(fsa1_det._states)\nprint(fsa1_comp._states)\n\n\nprint(fsa1_det._final_states)\nprint(fsa1_comp._final_states)\n\nNotice that, because taking the complement requires a totalized transition function, we have exactly the deterministic machine from above, except with explicit sink states.\n\nfsa1_comp._transition_function._transition_graph\n\nWe don’t need explicit determinization with the power set construction to have a deterministic machine. It’s also possible to define a DFA directly. This machine gets implicitly converted to the strongly equivalent NFA.\n\nfsa2 = FiniteStateAutomaton(alphabet={\"c\", \"d\"},\n                            states={\"q0\"},\n                            initial_state=\"q0\",\n                            final_states={\"q0\"},\n                            transition_graph={(\"q0\", \"c\"): \"q0\",\n                                              (\"q0\", \"d\"): \"q0\"})\n\nfsa2.isdeterministic\n\n\nfsa2._transition_function._transition_graph\n\nImportant for our purposes, we can compute strings that are in the language generated by the FSA by simply iterating through the FiniteStateAutomaton object. This behavior is implemented in FiniteStateAutomaton.__iter__ and FiniteStateAutomaton.__next__, which rely heavily on FiniteStateAutomaton._build_generator.\n\nfor i, string in enumerate(fsa1):\n    if i &lt; 50:\n        print(string)\n    else:\n        break\n\nNote that we end up with some repeated elements. This is because different paths through an FSA can result in the same string—which, of course, is why there can be multiple parses associated with a string. We could make it so that elements are not repeated, but I have not done this for the sake of clarity (for Task 1).\nCompare the determinized machine.\n\nfor i, string in enumerate(fsa1_det):\n    if i &lt; 50:\n        print(string)\n    else:\n        break\n\nAlso for the sake of clarity, I have implemented iteration in a simplistic way that will often result in the iteration hanging for long periods of time (or infinitely) when there are many sink states and/or when the machine computes the empty set. This could be fixed, but it would require a bit of extra code that would be harder to understand, and so I have retained the simpler implementation.\n\n# this will hang\nfor i, string in enumerate(fsa1_comp):\n    if i &lt; 50:\n        print(string)\n    else:\n        break\n\nAnd just to show that our DFA works as expected:\n\nfor i, string in enumerate(fsa2):\n    if i &lt; 50:\n        print(string)\n    else:\n        break\n\nFinally (and probably most importantly), I have implemented the recognition and parsing algorithms for you. The FiniteStateAutomaton.__call__ method implements an interface to both the recognizer and the parser. To use the recognizer, set mode=\"recognize\". As you would expect, the output will be a boolean.\n\nfsa1(\"ab\", mode=\"recognize\"), fsa1(\"ac\", mode=\"recognize\")\n\nTo use the parser, set mode=\"parse\". If the string is recognized, a set of parses will be output.\n\nfsa1(\"ab\", mode=\"parse\")\n\nIf the string is not recognized, an empty set will be returned.\n\nfsa1(\"ac\", mode=\"parse\")\n\nEven though I have already implemented these algorithms for you, you should study their implementation in detail: in Task 4, you will be implementing the analogous algorithms for FSTs, and a good chunk of the logic in my implementation can be reused there."
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#task-1",
    "href": "assignments/assignments-7-and-8.html#task-1",
    "title": "Assignments 7 and 8",
    "section": "Task 1",
    "text": "Task 1\nImplement FiniteStateAutomaton.union and FiniteStateAutomaton.concatenate.\nTest your implementation."
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#task-2",
    "href": "assignments/assignments-7-and-8.html#task-2",
    "title": "Assignments 7 and 8",
    "section": "Task 2",
    "text": "Task 2\nDefine an FSA that can generate English noun phrases built from the following vocabulary.\n\\[\\Sigma = \\{\\text{the}, \\text{that}, \\text{those}, \\text{lazy}, \\text{greyhound}, \\text{greyhounds}, \\text{human}, \\text{humans}, \\text{love}, \\text{loves}\\}\\]\nYou may find it useful to define a few different machines and then union or concatenate them.\nYour FSA should recognize not only simple noun phrases—such as greyhounds, the greyhound, that human, those lazy greyhounds, and the humans —but also noun phrases with relative clauses—such as the greyhound that the lazy human loves and the humans that love the lazy greyhound. Make sure, however, that your machine cannot generate any string that is not an noun phrase of English. For instance, your machine should not generate noun phrases with agreement errors in the relative clause—such as the humans that loves the greyhound.\nTo make the parses produced by a parser for your machine interpretable—i.e. the sequence of states the FSA goes through to recognize the string—use state labels that track at least part of speech information—e.g. that the is a determiner; that is some contexts that is a determiner, while in others it is a complementizer. You may want to track additional information in the states to handle agreement correctly.\nYou will not be able to build an FSA that can generate the infinite possible English noun phrases that can be built using this vocabulary. We will discuss the reasons for this when we cover context free grammars. But you should strive to capture as many noun phrases that can be built from this vocabulary as possible—including, insofar as it is possible, noun phrases of the form the humans that love the lazy greyhound that loves the humans that… with an unbounded number of relative clauses (making sure that you get the agreement correct).\nTest this FSA by attempting to parse and recognize five possible noun phrases, which should be recognizable and have at least one parse, and five possible noun phrases, which should not be recognizable and should have no parses.\nWhat kind of noun phrase cannot be generated by an FSA at all? Why?"
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#task-3",
    "href": "assignments/assignments-7-and-8.html#task-3",
    "title": "Assignments 7 and 8",
    "section": "Task 3",
    "text": "Task 3\nDefine an FSA that can generate English sentences built from the vocabulary from Task 2. You are encouraged to use the noun phrase machine you developed in that task to generate the noun phrases within these sentences. As in that task, you may find it useful to define a few different machines (in addition to the noun phrase machine) and then union and/or concatenate them.\nYour FSA should recognize not only simple transitive sentences—such as the greyhound loves the humans —but also sentences that involve clause-embedding—such as the greyhound loves that the humans love the greyhound. As before, make sure you are correctly handling agreement; and insofar as it is possible, make sure that you can recognize sentences of unbounded size—such as the humans love that the greyhound loves that the humans love that….\nTest this FSA by attempting to parse and recognize five possible sentences, which should be recognizable and have at least one parse, and five possible sentences, which should not be recognizable and should have no parses."
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#finite-state-transducers",
    "href": "assignments/assignments-7-and-8.html#finite-state-transducers",
    "title": "Assignments 7 and 8",
    "section": "Finite State Transducers",
    "text": "Finite State Transducers\n\nFSTMode = Literal[\"recognize\", \"parse\", \"transduce\"]\nFSTParse = tuple[tuple[str, str, str]]\n\nTransductionGraph = dict[tuple[str, str, str], str]\n\nclass FiniteStateTransducer(FiniteStateAutomaton):\n\n    \"\"\"A finite state transducer\n\n    Parameters\n    ----------\n    alphabet1\n    alphabet2\n    states\n    initial_state\n    final states\n    transition_graph\n    transduction_graph\n    \"\"\"\n\n    def __init__(self, alphabet1: set[str], alphabet2: set[str], states: set[str], \n                 initial_state: str, final_states: set[str],\n                 transition_graph: TransitionGraph, \n                 transduction_graph: TransductionGraph):\n        self._transduction_function = TransductionFunction(transduction_graph)\n        self._alphabet2 = alphabet2        \n        self._transduction_function.validate(alphabet1, alphabet2, states)\n\n        super().__init__(alphabet1,\n                         states,\n                         initial_state,\n                         final_states,\n                         transition_graph)\n        \n    def __next__(self):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n        \n    def __call__(self, string1: str | list[str], \n                 string2: str | list[str] | None = None, \n                 mode: FSTMode = \"recognize\") -&gt; bool | set[FSTParse] | set[str] | set[list[str]]:\n        \"\"\"\n        whether/how/what a string is accepted/parsed/transduced to by the FST\n\n        Parameters\n        ----------\n        string1\n        string2\n            only necessary if mode is \"recognize\" or \"parse\"\n        mode\n            whether to run in \"recognize\", \"parse\", or \"transduce\" mode\n        \"\"\"\n\n        if mode == 'recognize':\n            return self._recognize(string1, string2)\n        elif mode == 'parse':\n            return self._parse(string1, string2)\n        elif mode == \"transduce\":\n            return self._transduce(string1)\n        else:\n            msg = 'mode must be \"recognize\", \"parse\", or \"transduce\"'\n            raise ValueError(msg)\n\n    @lru_cache(65536)\n    def _recognize(self, string1: str | list[str], string2: str | list[str], \n                   state: str | None = None) -&gt; bool:\n        \"\"\"\n        whether a pair of string is accepted by the FST\n\n        Parameters\n        ----------\n        string1 : str\n        string2 : str\n        state : str | NoneType\n\n        Returns\n        -------\n        bool\n        \"\"\"\n        msg = \"you still need to implement FiniteStateTransducer._recognize\"\n        raise NotImplementedError(msg)\n\n    @lru_cache(65536)\n    def _parse(self, string1: str | list[str], string2: str | list[str], \n               prev_state: str | None = None) -&gt; set[FSTParse]:\n        \"\"\"\n        how a pair of strings is parsed by the FST\n\n        Parameters\n        ----------\n        string1 : str\n        state : str | NoneType\n        previous_states : list(str)\n\n        Returns\n        -------\n        list(list(str))\n        \"\"\"\n        msg = \"you still need to implement FiniteStateTransducer._parse\"\n        raise NotImplementedError(msg)\n\n    @lru_cache(65536)\n    def _transduce(self, string: str | list[str], state: str | None = None, \n                   new_string: str='') -&gt; set[str] | set[list[str]]:\n        \"\"\"\n        the strings transduced from the input by the FST\n\n        Parameters\n        ----------\n        string\n        state\n        new_string\n        \"\"\"\n        msg = \"you still need to implement FiniteStateTransducer._transduce\"\n        raise NotImplementedError(msg)\n\n    def _relabel_states(self, tag: str):\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag\n        \"\"\"\n\n        super()._relabel_states(tag)\n        \n        state_map = {s: s+'_'+tag for s in self._states}\n        self._transduction_function.relabel_states(state_map)\n\n        return self\n\n    def concatenate(self, other):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n    \n    def exponentiate(self, k):        \n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n    \n    def intersect(self, other):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n    \n    def complement(self):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n    \n    def union(other, self):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n\n    def determinize(self):\n        msg = \"we do not need this for the current assignment\"\n        raise NotImplementedError(msg)\n\nclass TransductionFunction(object):\n    \"\"\"\n    A finite state transduction function\n\n    Parameters\n    ----------\n    transduction_graph\n    \"\"\"\n\n    def __init__(self, transduction_graph: TransductionGraph):\n        self._transduction_graph = transduction_graph\n\n    def __call__(self, state1, state2, symbol):\n        try:\n            return self._transduction_graph[(state1, state2, symbol)]\n        except KeyError:\n            return set({})\n\n    def add_transductions(self, transduction_graph: TransductionGraph):\n        self._transduction_graph.update(transduction_graph)\n\n    def validate(self, alphabet1, alphabet2, states):\n        self._validate_input_values(alphabet1, states)\n        self._validate_output_values(alphabet2)\n\n    def _validate_input_values(self, alphabet, states):\n        for state1, state2, symbol in self._transduction_graph.keys():\n            try:\n                assert symbol in alphabet\n\n            except AssertionError:\n                msg = 'all input symbols in transduction function ' +\\\n                      'must be in alphabet1'\n                raise ValueError(msg)\n\n            try:\n                assert state1 in states and state2 in states\n\n            except AssertionError:\n                msg = 'all input states in transduction function ' +\\\n                      'must be in set of states'\n                raise ValueError(msg)\n\n            try:\n                assert symbol != ''\n\n            except AssertionError:\n                msg = 'epsilon transduction not currently supported'\n                raise ValueError(msg)\n\n                \n        self._istotalfunction = all([(s1, s2, a) in self._transduction_graph\n                                     for s1 in states\n                                     for s2 in states\n                                     for a in alphabet])\n\n    def _validate_output_values(self, alphabet):\n        for symb in self._transduction_graph.values():\n            try:\n                assert symb in alphabet\n            except AssertionError:\n                msg = 'all output symbols in transduction function' +\\\n                      'must be in alphabet2'\n                raise ValueError(msg)\n\n    def relabel_states(self, state_map):\n        \"\"\"\n        append tag to the input/ouput states in the transduction function\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        new_transduction_graph = {}\n\n        for (instate, outstate, insymb), outs in self._transduction_graph.items():\n            new_inp = (state_map[instate], state_map[outstate], insymb)\n            new_transduction_graph[new_inp] = outs\n\n        self._transduction_graph = new_transduction_graph\n\n    @property\n    def transduction_graph(self):\n        return self._transduction_graph\n\n\nfst = FiniteStateTransducer(alphabet1={\"a\", \"b\"},\n                            alphabet2={\"c\", \"d\"},\n                            states={\"q0\", \"q1\"},\n                            initial_state=\"q0\",\n                            final_states={\"q0\", \"q1\"},\n                            transition_graph={(\"q0\", \"a\"): {\"q0\", \"q1\"},\n                                              (\"q0\", \"b\"): {\"q0\"},\n                                              (\"q1\", \"a\"): {\"q0\"},\n                                              (\"q1\", \"b\"): {\"q0\"}},\n                            transduction_graph={(\"q0\", \"q0\", \"a\"): \"d\",\n                                                (\"q0\", \"q0\", \"b\"): \"d\",\n                                                (\"q0\", \"q1\", \"a\"): \"c\",\n                                                (\"q0\", \"q1\", \"b\"): \"d\",\n                                                (\"q1\", \"q0\", \"a\"): \"c\",\n                                                (\"q1\", \"q0\", \"b\"): \"d\"})"
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#task-4",
    "href": "assignments/assignments-7-and-8.html#task-4",
    "title": "Assignments 7 and 8",
    "section": "Task 4",
    "text": "Task 4\nImplement FiniteStateTransducer._recognize, FiniteStateTransducer._parse, and FiniteStateTransducer._transduce. If you wish, you may alter the typing of the transduction_graph so that it outputs sets of characters in the output alphabet rather the single character."
  },
  {
    "objectID": "assignments/assignments-7-and-8.html#task-5",
    "href": "assignments/assignments-7-and-8.html#task-5",
    "title": "Assignments 7 and 8",
    "section": "Task 5",
    "text": "Task 5\nFor this task, we’ll be working with the MegaAcceptability dataset, which you can read about in this paper.\n\n!pip install pandas\n\n\nimport pandas as pd\n\nmega_acceptability = pd.read_csv('http://megaattitude.io/projects/mega-acceptability/mega-acceptability-v1/mega-acceptability-v1-normalized.tsv', sep='\\t')\n\nmega_acceptability\n\nWe will specifically be interested in the frames…\n\nframes = set(mega_acceptability.frame.unique())\n\nframes\n\n…and the sentences.\n\nsentences = {frame: {s.lower() for s in mega_acceptability.query(f'frame==\"{frame}\"').sentence.unique()} \n             for frame in frames}\n\nsentences['NP Ved NP']\n\nThough for this task, you will find having access to the verb forms useful.\n\nverbs = {'root': set(mega_acceptability.verb.unique()),\n         'past': set(mega_acceptability.query('frame==\"NP Ved\"').verbform.unique()),\n         'past_participle': set(mega_acceptability.query('frame==\"NP was Ved\"').verbform.unique())}\n\nverbs['past']\n\nConstruct a finite state transducer whose input language is the set of frames and whose output language is the set of sentences. The transducer should map a frame—e.g. NP Ved NP—to all sentences that instantiate that frame—e.g. someone liked something, someone thought something, etc.\nTest whether your FST recognizes all of the sentences in MegaAcceptability.\nTest whether your transduction works correctly by testing whether you output all of the sentences corresponding to a particular frame.\nMegaAcceptability contains many items that are unacceptable, denoted by having a more negative responsenorm.\n\nmega_acceptability.sort_values('responsenorm')\n\nSuppose that, given some frame in the input language, we wanted our FST to output only sentences that are instances of that frame whose responsenorm is above a particular threshold—i.e. generate only acceptable instances of a frame. You do not need to do it, but describe how you might go about it."
  },
  {
    "objectID": "assignments/assignments-9-and-10.html",
    "href": "assignments/assignments-9-and-10.html",
    "title": "Assignments 9 and 10",
    "section": "",
    "text": "Assignment 9 consists of Tasks 1 and 2 and Assignment 10 consists of Tasks 3-5.\nIn these assignments, we will focus on (i) extracting context free grammars (CFGs) from a treebank; and (ii) implement the CKY and Earley algorithms for recognizing/parsing CFGs."
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#trees",
    "href": "assignments/assignments-9-and-10.html#trees",
    "title": "Assignments 9 and 10",
    "section": "Trees",
    "text": "Trees\nWe’ll use a slightly augmented version of the Tree class we developed earlier in the class to represent trees that should be output by the parser.\n\nimport pyparsing\n\nfrom typing import TypeVar, Optional\nfrom collections.abc import Hashable, Callable\n\nDataType = Hashable\nTreeList = list[str, Optional[list['TreeList']]]\nTreeTuple = tuple[DataType, Optional[tuple['TreeTuple', ...]]]\n\nclass Tree:\n\n    LPAR = pyparsing.Suppress('(')\n    RPAR = pyparsing.Suppress(')')\n    DATA = pyparsing.Regex(r'[^\\(\\)\\s]+')\n\n    PARSER = pyparsing.Forward()\n    SUBTREE = pyparsing.ZeroOrMore(PARSER)\n    PARSERLIST = pyparsing.Group(LPAR + DATA + SUBTREE + RPAR)\n    PARSER &lt;&lt;= DATA | PARSERLIST\n    \n    def __init__(self, data: DataType, children: list['Tree'] = []):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n  \n    def to_tuple(self) -&gt; TreeTuple:\n        return self._data, tuple(c.to_tuple() for c in self._children)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n    \n    def __eq__(self, other: 'Tree') -&gt; bool:\n        return self.to_tuple() == other.to_tuple()\n\n    def __str__(self) -&gt; str:\n        return ' '.join(self.terminals)\n        \n    def __repr__(self) -&gt; str:\n        return self.to_string()\n     \n    def to_string(self, depth=0) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n    \n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: int | tuple[int, ...]) -&gt; 'Tree':\n        if isinstance(idx, int):\n            return self._children[idx]\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        elif idx:\n            return self._children[idx[0]].__getitem__(idx[1:])\n        else:\n            return self\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n     \n    @property\n    def terminals(self) -&gt; list[str]:\n        if self._children:\n            return [w for c in self._children \n                    for w in c.terminals]\n        else:\n            return [str(self._data)]\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n            \n    def index(self, data: DataType, index_path: tuple[int, ...] = tuple()) -&gt; list[tuple[int, ...]]:\n        indices = [index_path] if self._data==data else []\n        root_path = [] if index_path == -1 else index_path\n        \n        indices += [j \n                    for i, c in enumerate(self._children) \n                    for j in c.index(data, root_path+(i,))]\n\n        return indices\n    \n    def relabel(self, label_map: Callable[[DataType], DataType], \n                nonterminals_only: bool = False, terminals_only: bool = False) -&gt; 'Tree':\n        if not nonterminals_only and not terminals_only:\n            data = label_map(self._data)\n        elif nonterminals_only and self._children:\n            data = label_map(self._data)\n        elif terminals_only and not self._children:\n            data = label_map(self._data)\n        else:\n            data = self._data\n        \n        children = [c.relabel(label_map, nonterminals_only, terminals_only) \n                    for c in self._children]\n        \n        return self.__class__(data, children)\n    \n    @classmethod\n    def from_string(cls, treestr: str) -&gt; 'Tree':\n        treelist = cls.PARSER.parseString(treestr[2:-2])[0]\n        \n        return cls.from_list(treelist)\n    \n    @classmethod\n    def from_list(cls, treelist: TreeList) -&gt; 'Tree':\n        if isinstance(treelist, str):\n            return cls(treelist[0])\n        elif isinstance(treelist[1], str):\n            return cls(treelist[0], [cls(treelist[1])])\n        else:\n            return cls(treelist[0], [cls.from_list(l) for l in treelist[1:]])"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#cfg-rules",
    "href": "assignments/assignments-9-and-10.html#cfg-rules",
    "title": "Assignments 9 and 10",
    "section": "CFG rules",
    "text": "CFG rules\nVanilla context free grammar rules (in contrast to the dotted rules we use for Earley, define below) will be represented using a fairly lightweight wrapper around pairings of a string (the left side) with a tuple of strings (the right side).\n\nfrom enum import Enum\n\nclass NormalForm(Enum):\n    CNF = 0\n    BNF = 1\n    GNF = 2\n\nclass Rule:\n    \"\"\"A context free grammar rule\n\n    Parameters\n    ----------\n    left_side\n    right_side\n    \"\"\"\n\n    def __init__(self, left_side: str, *right_side: str):\n        self._left_side = left_side\n        self._right_side = right_side\n\n    def __repr__(self) -&gt; str:\n        return self._left_side + ' -&gt; ' + ' '.join(self._right_side)\n        \n    def to_tuple(self) -&gt; tuple[str, tuple[str, ...]]:\n        return (self._left_side, self._right_side)\n        \n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n\n    def __eq__(self, other: 'Rule') -&gt; bool:\n        left_side_equal = self._left_side == other._left_side\n        right_side_equal = self._right_side == other._right_side\n\n        return left_side_equal and right_side_equal\n\n    def validate(self, alphabet: set[str], variables: set[str], normal_form: NormalForm = NormalForm.CNF):\n        \"\"\"validate the rule\n\n        Parameters\n        ----------\n        alphabet : set(str)\n        variables : set(str)\n        \"\"\"\n\n        if self._left_side not in variables:\n            msg = \"left side of rule must be a variable\"\n            raise ValueError(msg)\n\n        acceptable = alphabet | variables | {''}\n            \n        if not all([s in acceptable for s in self._right_side]):\n            msg = \"right side of rule must contain only\" +\\\n                  \"a variable, a symbol, or the empty string\"\n            raise ValueError(msg)\n\n        if normal_form == NormalForm.CNF:\n            try:\n                if len(self.right_side) == 1:\n                    assert self.right_side[0] in alphabet\n                elif len(self.right_side) == 2:\n                    assert all([s in variables for s in self.right_side])\n                else:\n                    raise AssertionError\n\n            except AssertionError:\n                raise ValueError(f\"{self} is not in CNF\")\n        \n\n    @property\n    def left_side(self) -&gt; str:\n        return self._left_side\n\n    @property\n    def right_side(self) -&gt; tuple[str, ...]:\n        return self._right_side\n\n    @property\n    def is_unary(self) -&gt; bool:\n        return len(self._right_side) == 1\n    \n    @property\n    def is_binary(self) -&gt; bool:\n        return len(self._right_side) == 2\n\nDefining a rule is straightforward.\n\nRule(\"S\", \"NP\", \"VP\")\n\nS -&gt; NP VP\n\n\nNote that these rules are hashable, so they can be members of a python set.\n\n{Rule(\"S\", \"NP\", \"VP\"), Rule(\"S\", \"NP\", \"VP\")}\n\n{S -&gt; NP VP}"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#context-free-grammar",
    "href": "assignments/assignments-9-and-10.html#context-free-grammar",
    "title": "Assignments 9 and 10",
    "section": "Context Free Grammar",
    "text": "Context Free Grammar\nAs in the previous two assignments, we will define a ContextFreeGrammar to align very closely with the formal definition we covered in class. But because the recognition and parsing algorithms are slightly more complicated than those for finite state automata, we will factor the parsing algorithms themselves out of this class. To do this, we will specify a parser class that the grammar will initialize during its own initilization. We will set it to None to start, which will mean that we won’t be able to use the ContextFreeGrammar.__call__ method until we define a parser.\n\nfrom typing import Literal\nfrom functools import lru_cache\n\nMode = Literal[\"recognize\", \"parse\"]\n\nclass ContextFreeGrammar:\n\n    \"\"\"\n    A context free grammar\n\n    Parameters\n    ----------\n    alphabet : set(str)\n    variables : set(str)\n    rules : set(Rule)\n    start_variable : str\n\n    Attributes\n    ----------\n    alphabet : set(str)\n    variables : set(str)\n    rules : set(Rule)\n    start_variable : str\n\n    Methods\n    -------\n    reduce(left_side)\n    \"\"\"\n    \n    # this will need to be filled in by the parser class, once it is defined:\n    # - CKYParser\n    # - EarleyParser\n    parser_class = None\n    \n    def __init__(self, alphabet: set[str], variables: set[str], rules: set[Rule], start_variable: str):\n        self._alphabet = alphabet\n        self._variables = variables\n        self._rules = rules\n        self._start_variable = start_variable\n        \n        self._validate_variables()\n        self._validate_rules()\n\n        if self.parser_class is not None:\n            self._parser = self.parser_class(self)\n        else:\n            self._parser = None\n\n    def __call__(self, string: str | list[str], mode: Mode = \"recognize\"):\n        if self._parser is not None:\n            return self._parser(string, mode)\n        else:\n            raise AttributeError(\"no parser is specified\")\n        \n    def _validate_variables(self):\n        if self._alphabet & self._variables:\n            raise ValueError('alphabet and variables must not share elements')\n        \n        if self._start_variable not in self._variables:\n            raise ValueError('start variable must be in set of variables')\n\n    def _validate_rules(self):\n        if self.parser_class is not None:\n            for r in self._rules:\n                r.validate(self._alphabet, self._variables,\n                           self.parser_class.normal_form)\n\n    @property            \n    def alphabet(self) -&gt; set[str]:\n        return self._alphabet\n\n    @property    \n    def variables(self) -&gt; set[str]:\n        return self._variables\n   \n    @lru_cache(2**10)\n    def rules(self, left_side: str | None = None) -&gt; set[Rule]:\n        if left_side is None:\n            return self._rules\n        else:\n            return {rule for rule in self._rules \n                    if rule.left_side == left_side}\n\n    @property\n    def start_variable(self) -&gt; str:\n        return self._start_variable\n\n    @lru_cache(2**14)\n    def parts_of_speech(self, word: str | None = None) -&gt; set[str]:\n        if word is None:\n            return {rule.left_side for rule in self._rules \n                    if rule.is_unary \n                    if rule.right_side[0] in self._alphabet}\n        \n        else:\n            return {rule.left_side for rule in self._rules \n                    if rule.is_unary \n                    if rule.right_side[0] == word}\n  \n    @property\n    def phrase_variables(self) -&gt; set[str]:\n        try:\n            return self._phrase_variables\n        except AttributeError:\n            self._phrase_variables = {rule.left_side for rule in self._rules \n                                      if not rule.is_unary or \n                                      rule.right_side[0] not in self._alphabet}\n            return self._phrase_variables\n\n    @lru_cache(2^15)\n    def reduce(self, *right_side: str) -&gt; set[str]:\n        \"\"\"\n        the nonterminals that can be rewritten as right_side\n\n        Parameters\n        ----------\n        right_side\n\n        Returns\n        -------\n        set(str)\n        \"\"\"\n        return {r.left_side for r in self._rules\n                if r.right_side == tuple(right_side)}\n\nWe can then define a context free grammar as below.\n\ngrammar = ContextFreeGrammar(alphabet={'the', 'greyhound', 'ate', 'the', 'salmon',\n                                       'with', 'a', 'fork', 'again', 'quickly'},\n                             variables={'S', 'NP', 'VP', 'PP', 'D', 'N', 'V', 'P', 'Adv'},\n                             rules={Rule('S', 'NP', 'VP'),\n                                    Rule('NP', 'D', 'N'),\n                                    Rule('NP', 'NP', 'PP'),\n                                    Rule('VP', 'V', 'NP'),\n                                    Rule('VP', 'VP', 'PP'),\n                                    Rule('VP', 'Adv', 'VP'),\n                                    Rule('VP', 'VP', 'Adv'),\n                                    Rule('PP', 'P', 'NP'),\n                                    Rule('D', 'the'),\n                                    Rule('D', 'a'),\n                                    Rule('N', 'greyhound'),\n                                    Rule('N', 'salmon'),\n                                    Rule('N', 'fork'),\n                                    Rule('V', 'fork'),\n                                    Rule('V', 'ate'),\n                                    Rule('P', 'with'),\n                                    Rule('Adv', 'again'),\n                                    Rule('Adv', 'quickly')},\n                             start_variable='S')\n\nThere are a few attributes and methods to take note of here. One allows you to easily work with parts of speech…\n\ngrammar.parts_of_speech()\n\n{'Adv', 'D', 'N', 'P', 'V'}\n\n\n…including finding all the parts of speech for a particular word.\n\ngrammar.parts_of_speech(\"fork\")\n\n{'N', 'V'}\n\n\nAnother allows easy access to rules…\n\ngrammar.rules()\n\n{Adv -&gt; again,\n Adv -&gt; quickly,\n D -&gt; a,\n D -&gt; the,\n N -&gt; fork,\n N -&gt; greyhound,\n N -&gt; salmon,\n NP -&gt; D N,\n NP -&gt; NP PP,\n P -&gt; with,\n PP -&gt; P NP,\n S -&gt; NP VP,\n V -&gt; ate,\n V -&gt; fork,\n VP -&gt; Adv VP,\n VP -&gt; V NP,\n VP -&gt; VP Adv,\n VP -&gt; VP PP}\n\n\n…including finding all rules that have a particular left side.\n\ngrammar.rules(\"VP\")\n\n{VP -&gt; Adv VP, VP -&gt; V NP, VP -&gt; VP Adv, VP -&gt; VP PP}\n\n\nMost importantly, I have implemented a ContextFreeGrammar.reduce method for finding all left sides that can be rewritten from a particular right side.\n\ngrammar.reduce(\"V\", \"NP\")\n\n{'VP'}"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#treebank",
    "href": "assignments/assignments-9-and-10.html#treebank",
    "title": "Assignments 9 and 10",
    "section": "Treebank",
    "text": "Treebank\nWe will extract a CFG from the English Web Treebank. We can use the reader developed earlier in the course to read the treebank.\n\nimport tarfile\nfrom abc import ABC\n\nclass TreeBank(ABC):\n    \n    def __iter__(self):\n        return self\n    \n    def __next__(self):\n        return next(self._tree_iter)\n        \nclass EnglishWebTreebank(TreeBank):\n    \n    def __init__(self, root='LDC2012T13.tgz'):\n        self._root = root\n\n        self._tree_iter = self._construct_tree_iter()\n        \n    def _construct_tree_iter(self):\n        with tarfile.open(self._root) as corpus:\n            for fname in corpus.getnames():\n                if '.xml.tree' in fname:\n                    with corpus.extractfile(fname) as treefile:\n                        treestr = treefile.readline().decode()\n                        yield fname, Tree.from_string(treestr)"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#task-1",
    "href": "assignments/assignments-9-and-10.html#task-1",
    "title": "Assignments 9 and 10",
    "section": "Task 1",
    "text": "Task 1\nWrite a class method ContextFreeGrammar.from_treebank that extracts all of the alphabet elements, variables, and rules implied by the trees in the treebank. For instance, the rules implied by the following tree…\n\nprint(next(EnglishWebTreebank())[1].__repr__())\n\n…are:\nVB -&gt; try\nPP-LOC -&gt; IN NP\nNP -&gt; NNP NNP\nJJ -&gt; argentinian\nUH -&gt; please\nNP -&gt; DT JJ NN\nVB -&gt; like\n-NONE- -&gt; *PRO*\nNP-SBJ-1 -&gt; PRP\nNNP -&gt; tampa\nNP-SBJ -&gt; PRP\nIN -&gt; in\nSQ -&gt; MD NP-SBJ VP\nS -&gt; NP-SBJ-1 VP\nWHADVP-9 -&gt; WRB\nVP -&gt; VB NP PP-LOC ADVP-LOC-9\nVP -&gt; VB NP INTJ\nNNP -&gt; bay\nNP -&gt; NNS\nPRP -&gt; I\nS -&gt; NP-SBJ VP\nNNS -&gt; morcillas\nVP -&gt; TO VP\nMD -&gt; will\n-NONE- -&gt; *T*\nNN -&gt; type\nWRB -&gt; where\nVP -&gt; MD S\nMD -&gt; can\nINTJ -&gt; UH\n. -&gt; ?\nCC -&gt; but\n, -&gt; ,\nNP-SBJ-1 -&gt; -NONE-\nS -&gt; S , CC S\nVB -&gt; get\nNNS -&gt; anothers\nS -&gt; SBARQ , S .\nADVP-LOC-9 -&gt; -NONE-\nVP -&gt; VB NP\nDT -&gt; the\nVP -&gt; MD VP\nTO -&gt; to\nSBARQ -&gt; WHADVP-9 SQ\nI suggest breaking extraction into extraction from a tree using a rules attribute…\n\nclass Tree(Tree):\n    \n    @property\n    def rules(self) -&gt; set[Rule]:\n        raise NotImplementedError \n\n…which you can use to implement ContextFreeGrammar.from_treebank.\n\nclass ContextFreeGrammar(ContextFreeGrammar):\n    \n    @classmethod\n    def from_treebank(cls, treebank: TreeBank) -&gt; 'ContextFreeGrammar':\n        raise NotImplementedError\n\nOne thing you will need to make sure to do is to ensure that the alphabet and variables are disjoint. This is not guaranteed using naive extraction: nine symbols that show up as nonterminals also show up as terminals. Handling this will require some relabeling, which you can do using Tree.relabel.\nAnother thing you should make sure to do is to correctly handle trees whose root node is not S: a variety of “sentences” in EWT have NP or FRAG root nodes. But there are true sentences that have a root note other than S: S-IMP, SBAR, SQ, SINV, SBARQ, and S-HLN. Assume that all and only trees with variables at the root starting with S are sentences. Doing this will entail adding some rules to the grammar.\nFinally, you should lower case all terminals. This can also be done with Tree.relabel.\nTest your Tree.rules attribute against the tree we looked at above.\n\n# write tests"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#cky-parsing",
    "href": "assignments/assignments-9-and-10.html#cky-parsing",
    "title": "Assignments 9 and 10",
    "section": "CKY parsing",
    "text": "CKY parsing\nWe will implement CKY parsing and recognition using a system of classes: CKYCharts contain CKYChartEntrys and are filled with those entries by a CKYParser, which is a kind of ContextFreeGrammarParser. The reasoning behind having an abstract base class for ContextFreeGrammarParsers when we implement the EarleyParser.\n\nfrom typing import Union\n\nSpanIndices = tuple[int, int]\nCKYBackPointer = tuple[str, SpanIndices]\n\nclass Chart(ABC):\n\n    @property\n    def parses(self):\n        raise NotImplementedError\n\nclass ChartEntry(ABC):\n\n    def __hash__(self) -&gt; int:\n        raise NotImplementedError\n\n    @property\n    def backpointers(self):\n        raise NotImplementedError\n\nclass CKYChartEntry(ChartEntry):\n    \"\"\"\n    A chart entry for a CKY chart\n\n    Parameters\n    ----------\n    symbol\n    backpointers\n\n    Attributes\n    ----------\n    symbol\n    backpointers\n    \"\"\"\n\n    def __init__(self, symbol: str, *backpointers: CKYBackPointer):\n        self._symbol = symbol\n        self._backpointers = backpointers\n\n    def to_tuple(self):\n        return (self._symbol, self._backpointers)\n        \n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n\n    def __eq__(self, other: 'CKYChartEntry') -&gt; bool:\n        return self.to_tuple() == other.to_tuple()\n    \n    def __repr__(self) -&gt; str:\n        return self._symbol + ' -&gt; ' + ' '.join(\n            f\"{bp[0]}({bp[1][0]}, {bp[1][1]})\" \n            for bp in self.backpointers\n        )\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n    \n    @property\n    def symbol(self) -&gt; str:\n        return self._symbol\n\n    @property\n    def backpointers(self) -&gt; tuple[CKYBackPointer, ...]:\n        return self._backpointers\n\nclass CKYChart(Chart):\n    \"\"\"\n    A chart for a CKY parser\n\n    Jurafsky & Martin call this a table\n\n    Parameters\n    ----------\n    input_size\n\n    Attributes\n    ----------\n    parses\n    \"\"\"\n\n    def __init__(self, input_size: int):\n        self._input_size = input_size\n        \n        self._chart: list[list[set[CKYChartEntry]]] = [\n            [set({})\n             for _ in range(input_size+1)]\n            for _ in range(input_size+1)\n        ]\n        \n    def __getitem__(self, index: SpanIndices) -&gt; set[CKYChartEntry]:\n        i, j = index\n\n        self._validate_index(i, j)\n        \n        return self._chart[i][j]\n\n    def __setitem__(self, key: SpanIndices, item: set[CKYChartEntry]):\n        i, j = key\n\n        self._chart[i][j] = item\n        \n    def _validate_index(self, i, j):\n        try:\n            assert i &lt; j\n        except AssertionError:\n            msg = \"cannot index into the lower \" +\\\n                  \"triangle of the chart\"\n            raise ValueError(msg)\n\n        try:\n            self._chart[i]\n        except IndexError:\n            msg = \"row index is too large\"\n            raise ValueError(msg)\n\n        try:\n            self._chart[i][j]\n        except IndexError:\n            msg = \"column index is too large\"\n            raise ValueError(msg)\n\n    @property\n    def parses(self) -&gt; set[Tree]:\n        try:\n            return self._parses\n        except AttributeError:\n            self._parses = self._construct_parses()\n            return self._parses\n\n    def _construct_parses(self, entry: Union['CKYChartEntry', None] = None) -&gt; set[Tree]:\n        \"\"\"Construct the parses implied by the chart\n\n        Parameters\n        ----------\n        entry\n        \"\"\"\n        raise NotImplementedError\n    \nclass ContextFreeGrammarParser(ABC):\n    \n    def __init__(self, grammar: ContextFreeGrammar):\n        self._grammar = grammar\n\n    def __call__(self, string, mode=\"recognize\"):\n        if mode == \"recognize\":\n            return self._recognize(string)\n        elif mode == \"parse\":\n            return self._parse(string)            \n        else:\n            msg = 'mode must be \"parse\" or \"recognize\"'\n            raise ValueError(msg)\n\nclass CKYParser(ContextFreeGrammarParser):\n    \"\"\"\n    A CKY parser\n\n    Parameters\n    ----------\n    grammar : ContextFreeGrammar\n    \"\"\"\n    \n    normal_form = NormalForm.CNF\n    \n    def _fill_chart(self, string: list[str]) -&gt; CKYChart:\n        raise NotImplementedError\n\n    def _parse(self, string):\n        chart = self._fill_chart(string)\n        return chart.parses\n        \n    def _recognize(self, string):\n        chart = self._fill_chart(string)\n        \n        return any([self._grammar.start_variable == entry.symbol\n                    for entry in chart[0,len(string)]])"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#task-2",
    "href": "assignments/assignments-9-and-10.html#task-2",
    "title": "Assignments 9 and 10",
    "section": "Task 2",
    "text": "Task 2\nImplement the CKYParser._fill_chart and CKYChart._construct_parses methods.\n\nclass CKYParser(CKYParser):\n    \n    def _fill_chart(self, string: list[str]) -&gt; CKYChart:\n        raise NotImplementedError \n    \nclass CKYChart(CKYChart):\n    \n    def _construct_parses(self, entry: CKYChartEntry | None = None) -&gt; set[Tree]:\n        raise NotImplementedError\n\nTest your implementation by ensuring that the following call yields the correct number of Trees. (We are not using the grammar you extracted from the treebank because we want to a relatively minimal example to test against. The grammar you extracted will be relevant for Task 4 below.)\n\nContextFreeGrammar.parser_class = CKYParser\n\ngrammar = ContextFreeGrammar(alphabet={'the', 'greyhound', 'ate', 'the', 'salmon',\n                                       'with', 'a', 'fork', 'again', 'quickly'},\n                             variables={'S', 'NP', 'VP', 'PP', 'D', 'N', 'V', 'P', 'Adv'},\n                             rules={Rule('S', 'NP', 'VP'),\n                                    Rule('NP', 'D', 'N'),\n                                    Rule('NP', 'NP', 'PP'),\n                                    Rule('VP', 'V', 'NP'),\n                                    Rule('VP', 'VP', 'PP'),\n                                    Rule('VP', 'Adv', 'VP'),\n                                    Rule('VP', 'VP', 'Adv'),\n                                    Rule('PP', 'P', 'NP'),\n                                    Rule('D', 'the'),\n                                    Rule('D', 'a'),\n                                    Rule('N', 'greyhound'),\n                                    Rule('N', 'salmon'),\n                                    Rule('N', 'fork'),\n                                    Rule('V', 'fork'),\n                                    Rule('V', 'ate'),\n                                    Rule('P', 'with'),\n                                    Rule('Adv', 'again'),\n                                    Rule('Adv', 'quickly')},\n                             start_variable='S')\n\ngrammar(['the', 'greyhound', 'again', 'ate', 'the',\n         'salmon', 'with', 'a', 'fork', 'quickly'], mode=\"parse\")"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#dotted-rules",
    "href": "assignments/assignments-9-and-10.html#dotted-rules",
    "title": "Assignments 9 and 10",
    "section": "Dotted Rules",
    "text": "Dotted Rules\nTo implement Earley parsing, we need to augment our vanilla CFG rule from above to a dotted CFG rule, which tracks which constituents we may have seen up to a particular position in a sentence. This dotted rule needs to track not only the position of the dot in the rule (what kinds of consistuents might have been seen so far), but also which substring the dotted rule is associated with.\n\nclass DottedRule(Rule):\n    \n    def __init__(self, rule: Rule, span: SpanIndices, dot: int = 0):\n        self._rule = rule\n        self._left_side = rule.left_side\n        self._right_side = rule.right_side\n        \n        self._span = span\n        self._dot = dot\n    \n    def to_tuple(self) -&gt; tuple[Rule, SpanIndices, int]:\n        return self._rule, self._span, self._dot\n    \n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n    \n    def __eq__(self, other) -&gt; bool:\n        return self.to_tuple() == other.to_tuple()\n    \n    def __repr__(self) -&gt; str:\n        return self._left_side + ' -&gt; ' +\\\n               ' '.join(self._right_side[:self._dot]) +\\\n               ' . ' +\\\n               ' '.join(self._right_side[self._dot:]) +\\\n               ' [' + str(self._span[0]) + ', ' + str(self._span[1]) + ']'\n    \n    def complete(self, new_left_edge: int) -&gt; tuple['DottedRule', int]:\n        \"\"\"Complete the next symbol in this rule\n        \n        Parameters\n        ----------\n        new_left_edge\n\n        Returns\n        -------\n        new_dotted_rule\n        completed_symbol\n        old_left_edge\n        \"\"\"\n        dot = self._dot + 1\n        span = (self._span[0], new_left_edge)\n\n        return DottedRule(self._rule, span, dot)\n\n    @property\n    def next_symbol(self) -&gt; str:\n        if self.is_complete:\n            raise AttributeError('dotted rule is already complete')\n        else:\n            return self._right_side[self._dot]\n        \n    @property\n    def dot(self) -&gt; int:\n        return self._dot\n    \n    @property\n    def span(self) -&gt; SpanIndices:\n        return self._span\n    \n    @property\n    def is_complete(self) -&gt; bool:\n        return self._dot == len(self._right_side)\n    \n    @property\n    def left_side(self) -&gt; str:\n        return self._rule.left_side\n\nTo initialize a dotted CFG rule, we pass a vanilla CFG rule along with a tuple of indices representing the span that rule has recognized. If it has recognized nothing, the left index will be equal to the right index. We don’t need to pass the dot position because we assume that, on initialization, the dot is before the first right side symbol.\n\ndotted_rule = DottedRule(Rule('S', 'NP', 'VP'), (0, 0))\n\ndotted_rule\n\nWe can increment the dot by calling DottedRule.complete with the new right edge of the span.\n\ndotted_rule.complete(2)\n\nThis procedure creates an entirely new object.\n\nid(dotted_rule), id(dotted_rule.complete(2))\n\nCalling DottedRule.complete twice will increment the dot twice.\n\ndotted_rule.complete(2).complete(10)\n\nFinally, dotted rules are hashable and behave how you would expect when hashed.\n\n{dotted_rule, DottedRule(Rule('S', 'NP', 'VP'), (0, 0))}"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#earley-parsing",
    "href": "assignments/assignments-9-and-10.html#earley-parsing",
    "title": "Assignments 9 and 10",
    "section": "Earley Parsing",
    "text": "Earley Parsing\nWe will implement Earley parsing and recognition using a similar system of classes to the ones we developed for CKY: EarleyCharts contain EarleyChartEntrys and are filled with those entries by a EarleyParser, which is a kind of ContextFreeGrammarParser.\n\nEarleyBackPointer = tuple[str, int]\n\nclass EarleyChartEntry(ChartEntry):\n    \"\"\"A chart entry for a Earley chart\n\n    Parameters\n    ----------\n    dotted_rule\n    backpointers\n    \"\"\"\n\n    def __init__(self, dotted_rule: DottedRule, *backpointers: EarleyBackPointer):\n        self._dotted_rule = dotted_rule\n        self._backpointers = backpointers\n\n    def to_tuple(self) -&gt; tuple[DottedRule, tuple[EarleyBackPointer, ...]]:\n        return self._dotted_rule, self._backpointers\n        \n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n\n    def __eq__(self, other) -&gt; bool:\n        return self.to_tuple() == other.__key()\n    \n    def __repr__(self) -&gt; str:\n        return self._dotted_rule.__repr__()\n\n    def __str__(self) -&gt; str:\n        return self.__repr__()\n\n    @property\n    def backpointers(self) -&gt; tuple[EarleyBackPointer, ...]:\n        return self._backpointers\n    \n    @property\n    def dotted_rule(self):\n        return self._dotted_rule\n    \n    @property\n    def next_symbol(self) -&gt; str:\n        return self._dotted_rule.next_symbol\n    \n    @property\n    def span(self) -&gt; tuple[int]:\n        return self._dotted_rule.span\n    \n    @property\n    def is_complete(self):\n        return self._dotted_rule.is_complete\n    \n    def complete(self, entry: 'EarleyChartEntry', new_left_edge: int) -&gt; 'EarleyChartEntry':\n        new_dotted_rule = self._dotted_rule.complete(new_left_edge)\n        \n        bp = (self._dotted_rule.next_symbol, self._dotted_rule.span[1])\n        backpointers = self._backpointers + (bp,)\n        \n        return EarleyChartEntry(new_dotted_rule, backpointers)\n    \n    def is_completion_of(self, other: 'EarleyChartEntry') -&gt; bool:\n        return self._dotted_rule.left_side == other.dotted_rule.next_symbol\n\nclass EarleyChart(Chart):\n    \"\"\"A chart for an Earley parser\n\n    Parameters\n    ----------\n    input_size\n    \"\"\"\n    def __init__(self, input_size: int, start_variable: str = 'S'):\n        self._start_variable = start_variable\n        \n        self._chart: list[set[EarleyChartEntry]] = [\n            set() for _ in range(input_size+1)\n        ]\n        \n    def __getitem__(self, index) -&gt; set[EarleyChartEntry]:\n        return self._chart[index]\n\n    def __setitem__(self, key, item) -&gt; None:\n        self._chart[key] = item\n\n    @property\n    def parses(self) -&gt; set[Tree]:\n        try:\n            return self._parses\n        except AttributeError:\n            self._parses = set()\n            \n            for entry in self._chart[-1]:\n                is_start = entry.dotted_rule.left_side == self._start_variable\n                covers_string = entry.dotted_rule.span == (0, self.input_size)\n                \n                if is_start and covers_string:\n                    self._parses.add(self._construct_parses(entry))\n            \n            return self._parses\n\n    def _construct_parses(self, entry: 'EarleyChartEntry') -&gt; Tree:\n        \"\"\"Construct the parses implied by the chart\n\n        Parameters\n        ----------\n        entry\n        \"\"\"\n        raise NotImplementedError     \n    \n    @property\n    def input_size(self) -&gt; int:\n        return len(self._chart) - 1\n    \nclass EarleyParser(ContextFreeGrammarParser):\n    \"\"\"\n    An Earley parser\n\n    Parameters\n    ----------\n    grammar : ContextFreeGrammar\n    \"\"\"\n    normal_form = None\n    \n    def _fill_chart(self, string: list[str]) -&gt; EarleyChart:\n        \"\"\"\n        a chart for the string based on a CFG\n\n        Parameters\n        ----------\n        string\n        \"\"\"\n        raise NotImplementedError\n                    \n    def _predict(self, chart: EarleyChart, entry: EarleyChartEntry, position: int):\n        for rule in self._grammar.rules(entry.next_symbol):\n            span = (position, position)\n            dotted_rule = DottedRule(rule, span)\n            entry = EarleyChartEntry(dotted_rule)\n\n            chart[position].add(entry)\n     \n    def _scan(self, chart: EarleyChart, entry: EarleyChartEntry, position: int):\n        word = self._string[position]\n        pos_for_word = self._grammar.parts_of_speech(word)\n        \n        if entry.next_symbol in pos_for_word:\n            rule = Rule(entry.next_symbol, word)\n            span = (position, position+1)\n            dotted_rule = DottedRule(rule, span, dot=1)\n            \n            unary_entry = EarleyChartEntry(dotted_rule)\n            \n            chart[position+1].add(unary_entry)\n        \n    def _complete(self, chart: EarleyChart, entry: EarleyChartEntry, position: int):\n        start, end = entry.span\n        \n        for prev_entry in chart[start]:\n            if not prev_entry.is_complete and entry.is_completion_of(prev_entry):\n                completed_entry = prev_entry.complete(entry, end)\n                \n                chart[position].add(completed_entry)\n        \n    def _parse(self, string: str | list[str]):\n        chart = self._fill_chart(string)\n        return chart.parses\n\n    def _recognize(self, string: str | list[str]):\n        chart = self._fill_chart(string)\n        \n        for entry in chart[-1]:\n            is_start = entry.dotted_rule.left_side == self._grammar.start_variable\n            covers_string = entry.dotted_rule.span == (0, chart.input_size)\n            \n            if is_start and covers_string:\n                return True\n            \n        else:\n            return False\n        \nContextFreeGrammar.parser_class = EarleyParser"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#task-3",
    "href": "assignments/assignments-9-and-10.html#task-3",
    "title": "Assignments 9 and 10",
    "section": "Task 3",
    "text": "Task 3\nImplement the EarleyParser._fill_chart and EarleyChart._construct_parses method. Note that EarleyChart._construct_parses works slightly differently than CKYChart._construct_parses in only producing a single Tree. The parses attributes collects all Trees implied by a chart into a set.\n\nclass EarleyParser(EarleyParser):\n    \n    def _fill_chart(self, string: list[str]) -&gt; EarleyChart:\n        \"\"\"\n        a chart for the string based on a CFG\n\n        Parameters\n        ----------\n        string\n        \"\"\"\n        raise NotImplementedError\n    \nclass EarleyChart(EarleyChart):\n    \n    def _construct_parses(self, entry: 'EarleyChartEntry') -&gt; Tree:\n        \"\"\"Construct the parses implied by the chart\n\n        Parameters\n        ----------\n        entry\n        \"\"\"\n        raise NotImplementedError\n\nTest your implementation using the same grammar and sentence we used to test the CKY implementation.\n\nContextFreeGrammar.parser_class = EarleyParser\n\ngrammar = ContextFreeGrammar(alphabet={'the', 'greyhound', 'ate', 'the', 'salmon',\n                                       'with', 'a', 'fork', 'again', 'quickly'},\n                             variables={'S', 'NP', 'VP', 'PP', 'D', 'N', 'V', 'P', 'Adv'},\n                             rules={Rule('S', 'NP', 'VP'),\n                                    Rule('NP', 'D', 'N'),\n                                    Rule('NP', 'NP', 'PP'),\n                                    Rule('VP', 'V', 'NP'),\n                                    Rule('VP', 'VP', 'PP'),\n                                    Rule('VP', 'Adv', 'VP'),\n                                    Rule('VP', 'VP', 'Adv'),\n                                    Rule('PP', 'P', 'NP'),\n                                    Rule('D', 'the'),\n                                    Rule('D', 'a'),\n                                    Rule('N', 'greyhound'),\n                                    Rule('N', 'salmon'),\n                                    Rule('N', 'fork'),\n                                    Rule('V', 'fork'),\n                                    Rule('V', 'ate'),\n                                    Rule('P', 'with'),\n                                    Rule('Adv', 'again'),\n                                    Rule('Adv', 'quickly')},\n                             start_variable='S')\n\ngrammar(['the', 'greyhound', 'again', 'ate', 'the',\n         'salmon', 'with', 'a', 'fork', 'quickly'], mode=\"parse\")"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#task-4",
    "href": "assignments/assignments-9-and-10.html#task-4",
    "title": "Assignments 9 and 10",
    "section": "Task 4",
    "text": "Task 4\nImplement an instance method EarleyParser._predict_next_word that is used when EarleyParser is called in \"predict\" mode. This method should take in a tokenized string—the prefix —and output a dictionary mapping from parts-of-speech that could come after the prefix to words that could come after the prefix.\n\nfrom typing import Dict\nfrom collections import defaultdict\n\nclass EarleyParser(EarleyParser):\n    \n    def __call__(self, string, mode=\"recognize\"):\n        if mode == \"recognize\":\n            return self._recognize(string)\n        elif mode == \"parse\":\n            return self._parse(string)  \n        elif mode == \"predict\":\n            return self._predict_next_word(string)  \n        else:\n            msg = 'mode must be \"parse\", \"recognize\", or \"predict\"'\n            raise ValueError(msg)\n    \n    def _predict_next_word(self, prefix: list[str]) -&gt; Dict[str, set[str]]:\n        raise NotImplementedError\n\nContextFreeGrammar.parser_class = EarleyParser\n\nTest your prediction method against the grammar we used in class.\n\ngrammar = ContextFreeGrammar(alphabet={'the', 'greyhound', 'ate', 'the', 'salmon',\n                                       'with', 'a', 'fork', 'again', 'too'},\n                             variables={'S', 'NP', 'VP', 'PP', 'D', 'N', 'V', 'P', 'Adv'},\n                             rules={Rule('S', 'NP', 'VP'),\n                                    Rule('NP', 'D', 'N'),\n                                    Rule('NP', 'NP', 'PP'),\n                                    Rule('VP', 'V', 'NP'),\n                                    Rule('VP', 'VP', 'PP'),\n                                    Rule('VP', 'Adv', 'VP'),\n                                    Rule('VP', 'VP', 'Adv'),\n                                    Rule('PP', 'P', 'NP'),\n                                    Rule('D', 'the'),\n                                    Rule('D', 'a'),\n                                    Rule('N', 'greyhound'),\n                                    Rule('N', 'salmon'),\n                                    Rule('N', 'fork'),                                        \n                                    Rule('V', 'ate'),\n                                    Rule('VP', 'ate'),\n                                    Rule('P', 'with'),\n                                    Rule('Adv', 'again'),\n                                    Rule('Adv', 'too')},\n                             start_variable='S')\n\nFor instance, with the prefix the greyhound, you should get the following dictionary:\n{'Adv': {'again', 'too'},\n 'VP': {'ate'},\n 'P': {'with'},\n 'V': {'ate'}}\n\n# write tests here"
  },
  {
    "objectID": "assignments/assignments-9-and-10.html#task-5",
    "href": "assignments/assignments-9-and-10.html#task-5",
    "title": "Assignments 9 and 10",
    "section": "Task 5",
    "text": "Task 5\nRandomly sample sentences from EWT and compute the predicted words given the first one, two, and three words in that sentence. For each such set of predictions for each sentence count the total number of parts-of-speech and the total number of words that could follow that prefix.\n\ngrammar = ContextFreeGrammar.from_treebank(EnglishWebTreebank())\n\n# sample and predict here\n\nYou’ll see a general pattern in the numbers, but if you dig down into exactly what the next word predictions are, they will look at. What gives rise to this oddness?"
  },
  {
    "objectID": "assignments/final-project.html",
    "href": "assignments/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "In the final project for the course, you will develop a package for parsing with multiple context free grammars (MCFGs). This package must implement an agenda-based parser as described in Shieber et al. 1995. You should use the inference rules laid out in Kallmeyer 2013.\nIn addition to a working agenda-based parser, the package must use the standard directory structure for a python package, including a full test suite implemented in pytest. If you have not written a python package before, I encourage you to read this tutorial and use the directory structure they discuss:"
  },
  {
    "objectID": "assignments/final-project.html#test-grammar",
    "href": "assignments/final-project.html#test-grammar",
    "title": "Final Project",
    "section": "Test Grammar",
    "text": "Test Grammar\nIn writing tests, it will be useful to have a grammar to test your parser against. Rather than have you write your own, you might find it useful to use the one below.\nS(uv) -&gt; NP(u) VP(v)\nS(uv) -&gt; NPwh(u) VP(v)\nS(vuw) -&gt; Aux(u) Swhmain(v, w)\nS(uwv) -&gt; NPdisloc(u, v) VP(w)\nS(uwv) -&gt; NPwhdisloc(u, v) VP(w)\nSbar(uv) -&gt; C(u) S(v)\nSbarwh(v, uw) -&gt; C(u) Swhemb(v, w)\nSbarwh(u, v) -&gt; NPwh(u) VP(v)\nSwhmain(v, uw) -&gt; NP(u) VPwhmain(v, w)\nSwhmain(w, uxv) -&gt; NPdisloc(u, v) VPwhmain(w, x)\nSwhemb(v, uw) -&gt; NP(u) VPwhemb(v, w)\nSwhemb(w, uxv) -&gt; NPdisloc(u, v) VPwhemb(w, x)\nSrc(v, uw) -&gt; NP(u) VPrc(v, w)\nSrc(w, uxv) -&gt; NPdisloc(u, v) VPrc(w, x)\nSrc(u, v) -&gt; N(u) VP(v)\nSwhrc(u, v) -&gt; Nwh(u) VP(v)\nSwhrc(v, uw) -&gt; NP(u) VPwhrc(v, w)\nSbarwhrc(v, uw) -&gt; C(u) Swhrc(v, w)\nVP(uv) -&gt; Vpres(u) NP(v)\nVP(uv) -&gt; Vpres(u) Sbar(v)\nVPwhmain(u, v) -&gt; NPwh(u) Vroot(v)\nVPwhmain(u, wv) -&gt; NPwhdisloc(u, v) Vroot(w)\nVPwhmain(v, uw) -&gt; Vroot(u) Sbarwh(v, w)\nVPwhemb(u, v) -&gt; NPwh(u) Vpres(v)\nVPwhemb(u, wv) -&gt; NPwhdisloc(u, v) Vpres(w)\nVPwhemb(v, uw) -&gt; Vpres(u) Sbarwh(v, w)\nVPrc(u, v) -&gt; N(u) Vpres(v)\nVPrc(v, uw) -&gt; Vpres(u) Nrc(v, w)\nVPwhrc(u, v) -&gt; Nwh(u) Vpres(v)\nVPwhrc(v, uw) -&gt; Vpres(u) Sbarwhrc(v, w)\nNP(uv) -&gt; D(u) N(v)\nNP(uvw) -&gt; D(u) Nrc(v, w)\nNPdisloc(uv, w) -&gt; D(u) Nrc(v, w)\nNPwh(uv) -&gt; Dwh(u) N(v)\nNPwh(uvw) -&gt; Dwh(u) Nrc(v, w)\nNPwhdisloc(uv, w) -&gt; Dwh(u) Nrc(v, w)\nNrc(v, uw) -&gt; C(u) Src(v, w)\nNrc(u, vw) -&gt; N(u) Swhrc(v, w)\nNrc(u, vwx) -&gt; Nrc(u, v) Swhrc(w, x)\nDwh(which)\nNwh(who)\nD(the)\nD(a)\nN(greyhound)\nN(human)\nVpres(believes)\nVroot(believe)\nAux(does)\nC(that)\nWhatever grammar you use, you’ll want to load this grammar as a pytest.fixture within your tests."
  },
  {
    "objectID": "assignments/final-project.html#rules",
    "href": "assignments/final-project.html#rules",
    "title": "Final Project",
    "section": "Rules",
    "text": "Rules\nTo get you off the ground, I’ve provided some tools for representing rules in an MCFG. Like the context free grammar Rules you worked with in Assignments 9 and 10, MCFGRules have a left side and a right side, but rather than simply being composed of a right side of type str and a left side of type tuple[str, ...], they are composed of a right side of type MCFGRuleElement and a left side of type tuple[MCFGRuleElement, ...]. The reason for this is that we need to track not only the variable—e.g. S, NP, VP, etc.—but also the string variable(s) associated with that variable.\n\nimport re\n\nStringVariables = tuple[int, ...]\n\nclass MCFGRuleElement:\n\n    \"\"\"A multiple context free grammar rule element\n\n    Parameters\n    ----------\n    variable\n    string_variables\n\n    Attributes\n    ----------\n    symbol\n    string_variables\n    \"\"\"\n\n    def __init__(self, variable: str, *string_variables: StringVariables):\n        self._variable = variable\n        self._string_variables = string_variables\n\n    def __str__(self) -&gt; str:\n        strvars = ', '.join(\n            ''.join(str(v) for v in vtup)\n            for vtup in self._string_variables\n        )\n        \n        return f\"{self._variable}({strvars})\"\n\n    def __eq__(self, other) -&gt; bool:\n        vareq = self._variable == other._variable\n        strvareq = self._string_variables == other._string_variables\n        \n        return vareq and strvareq\n        \n    def to_tuple(self) -&gt; tuple[str, tuple[StringVariables, ...]]:\n        return (self._variable, self._string_variables)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n        \n    @property\n    def variable(self) -&gt; str:\n        return self._variable\n\n    @property\n    def string_variables(self) -&gt; tuple[StringVariables, ...]:\n        return self._string_variables\n\n    @property    \n    def unique_string_variables(self) -&gt; set[int]:\n        return {\n            i\n            for tup in self.string_variables\n            for i in tup\n        }\n\nAn MCFGRuleElement is thus basically a wrapper around a variable, represented as a str, and a (possibly singleton or empty) tuple of variables, represented as tuple[int, ...]. For instance, one rule in the grammar is VPwhemb(u, v) -&gt; NPwh(u) Vpres(v), where u and v might be represented as Tuple[int]s (0,) and (1,), respectively.\n\nprint(\n    MCFGRuleElement('VPwhemb', (0,), (1,)),\n    \"-&gt;\", \n    MCFGRuleElement('NPwh', (0,)), \n    MCFGRuleElement('Vpres', (1,))\n)\n\nVPwhemb(0, 1) -&gt; NPwh(0) Vpres(1)\n\n\nThe reason string variables must be represented as tuple[int, ...]s, rather than simply int, is to ensure that we can correctly represent left sides of rules that concatenate string variables, like VPwhemb(u, wv) -&gt; NPwhdisloc(u, v) Vpres(w).\n\nprint(\n    MCFGRuleElement('VPwhemb', (0,), (2, 1)),\n    \"-&gt;\", \n    MCFGRuleElement('NPwhdisloc', (0,), (1,)),\n    MCFGRuleElement('Vpres', (2,))\n)\n\nVPwhemb(0, 21) -&gt; NPwhdisloc(0, 1) Vpres(2)\n\n\nOne complexity that MCFGs add on top of CFGs is that we now need to explicitly track which string spans correspond to which variables. For instance, in who does the greyhound believe, u and v in VPwhmain(u, v) -&gt; NPwh(u) Vroot(v) will be instantiated by who at \\((0, 1)\\) and believe at \\((4, 5)\\), respectively.\nAnd if we want to put that VPwhmain constituent together with the greyhound at \\((2, 4),\\) which instantiates NP(uv) -&gt; D(u) N(v), we need to know that the span at \\((2, 4)\\) instantiates uv to ensure that, e.g., the spans satisfy the constraints implied by some other rule. For instance, to know that Swhmain(v, uw) -&gt; NP(u) VPwhmain(v, w) can be instantiated by who and the greyhound believe, we need to know that the span instantiating u, the greyound at \\((2, 4)\\), has a right edge matching the left edge of the span instantiating w, believe at \\((4, 5)\\)—though there are no similar constraints for the left edge of the span instantiating u or the left or right edge of the span instantiating v.\nBecause it is non-trivial to implement, I have already done the work of calculating satisfaction of those constraints in methods defined in MCFGRule below. You will write docstrings for these methods in Task 1. To keep track of this sort of information, we will use MCFGRuleElementInstances, which pair a variable with a span (represented as a tuple[int, ...]).\n\nSpanIndices = tuple[int, ...]\n\nclass MCFGRuleElementInstance:\n    \"\"\"An instantiated multiple context free grammar rule element\n\n    Parameters\n    ----------\n    symbol\n    string_spans\n\n    Attributes\n    ----------\n    symbol\n    string_spans\n    \"\"\"\n    def __init__(self, variable: str, *string_spans: SpanIndices):\n        self._variable = variable\n        self._string_spans = string_spans\n\n    def __eq__(self, other: 'MCFGRuleElementInstance') -&gt; bool:\n        vareq = self._variable == other._variable\n        strspaneq = self._string_spans == other._string_spans\n        \n        return vareq and strspaneq\n        \n    def to_tuple(self) -&gt; tuple[str, tuple[SpanIndices, ...]]:\n        return (self._variable, self._string_spans)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n\n    def __str__(self):\n        strspans = ', '.join(\n            str(list(stup))\n            for stup in self._string_spans\n        )\n        \n        return f\"{self._variable}({strspans})\"\n\n    def __repr__(self) -&gt; str:\n        return self.__str__()\n    \n    @property\n    def variable(self) -&gt; str:\n        return self._variable\n\n    @property\n    def string_spans(self) -&gt; tuple[SpanIndices, ...]:\n        return self._string_spans\n\nThe full MCFGRule implementation is given below.\n\nSpanMap = dict[int, SpanIndices]\n\nclass MCFGRule:\n    \"\"\"A linear multiple context free grammar rule\n\n    Parameters\n    ----------\n    left_side \n    right_side\n\n    Attributes\n    ----------\n    left_side\n    right_side\n    \"\"\"\n\n    def __init__(self, left_side: MCFGRuleElement, *right_side: MCFGRuleElement):\n        self._left_side = left_side\n        self._right_side = right_side\n\n        self._validate()\n\n    def to_tuple(self) -&gt; tuple[MCFGRuleElement, tuple[MCFGRuleElement, ...]]:\n        return (self._left_side, self._right_side)\n\n    def __hash__(self) -&gt; int:\n        return hash(self.to_tuple())\n    \n    def __repr__(self) -&gt; str:\n        return '&lt;Rule: '+str(self)+'&gt;'\n        \n    def __str__(self) -&gt; str:\n        if self.is_epsilon:\n            return str(self._left_side)                \n\n        else:\n            return str(self._left_side) +\\\n                ' -&gt; ' +\\\n                ' '.join(str(el) for el in self._right_side)\n\n    def __eq__(self, other: 'MCFGRule') -&gt; bool:\n        left_side_equal = self._left_side == other._left_side\n        right_side_equal = self._right_side == other._right_side\n\n        return left_side_equal and right_side_equal\n\n    def _validate(self):\n        vs = [\n            el.unique_string_variables\n            for el in self.right_side\n        ]\n        sharing = any(\n            vs1.intersection(vs2)\n            for i, vs1 in enumerate(vs)\n            for j, vs2 in enumerate(vs)\n            if i &lt; j\n        )\n\n        if sharing:\n            raise ValueError(\n                'right side variables cannot share '\n                'string variables'\n            )\n\n        if not self.is_epsilon:\n            left_vars = self.left_side.unique_string_variables\n            right_vars = {\n                var for el in self.right_side\n                for var in el.unique_string_variables\n            }\n            if left_vars != right_vars:\n                raise ValueError(\n                    'number of arguments to instantiate must '\n                    'be equal to number of unique string_variables'\n                )\n        \n    @property\n    def left_side(self) -&gt; MCFGRuleElement:\n        return self._left_side\n\n    @property\n    def right_side(self) -&gt; tuple[MCFGRuleElement, ...]:\n        return self._right_side\n\n    @property\n    def is_epsilon(self) -&gt; bool:\n        return len(self._right_side) == 0\n\n    @property\n    def unique_variables(self) -&gt; set[str]:\n        return {\n            el.variable\n            for el in [self._left_side]+list(self._right_side)\n        }\n\n    def instantiate_left_side(self, *right_side: MCFGRuleElementInstance) -&gt; MCFGRuleElementInstance:\n        \"\"\"Instantiate the left side of the rule given an instantiated right side\n\n        Parameters\n        ----------\n        right_side\n            The instantiated right side of the rule.\n        \"\"\"\n        \n        if self.is_epsilon:\n            strvars = tuple(v[0] for v in self._left_side.string_variables)\n            strconst = tuple(el.variable for el in right_side)\n            \n            if strconst == strvars:\n                return MCFGRuleElementInstance(\n                    self._left_side.variable,\n                    *[s for el in right_side for s in el.string_spans]\n                )\n\n        new_spans = []\n        span_map = self._build_span_map(right_side)\n        \n        for vs in self._left_side.string_variables:\n            for i in range(1,len(vs)):\n                end_prev = span_map[vs[i-1]][1]\n                begin_curr = span_map[vs[i]][0]\n\n                if end_prev != begin_curr:\n                    raise ValueError(\n                        f\"Spans {span_map[vs[i-1]]} and {span_map[vs[i]]} \"\n                        f\"must be adjacent according to {self} but they \"\n                        \"are not.\"\n                    )\n                \n            begin_span = span_map[vs[0]][0]\n            end_span = span_map[vs[-1]][1]\n\n            new_spans.append((begin_span, end_span))\n\n        return MCFGRuleElementInstance(\n            self._left_side.variable, *new_spans\n        )\n\n    \n    def _build_span_map(self, right_side: tuple[MCFGRuleElementInstance, ...]) -&gt; SpanMap:\n        \"\"\"Construct a mapping from string variables to string spans\"\"\"\n        \n        if self._right_side_aligns(right_side):\n            return {\n                strvar[0]: strspan\n                for elem, eleminst in zip(\n                    self._right_side,\n                    right_side\n                )\n                for strvar, strspan in zip(\n                    elem.string_variables,\n                    eleminst.string_spans\n                )\n            }\n        else:\n            raise ValueError(\n                f\"Instantiated right side {right_side} do not \"\n                f\"align with rule's right side {self._right_side}\"\n            )\n\n    def _right_side_aligns(self, right_side: tuple[MCFGRuleElementInstance, ...]) -&gt; bool:\n        \"\"\"Check whether the right side aligns\"\"\"\n\n        if len(right_side) == len(self._right_side):\n            vars_match = all(\n                elem.variable == eleminst.variable\n                for elem, eleminst in zip(self._right_side, right_side)\n            )\n            strvars_match = all(\n                len(elem.string_variables) == len(eleminst.string_spans)\n                for elem, eleminst in zip(self._right_side, right_side)\n            )\n\n            return vars_match and strvars_match\n        else:\n            return False \n\n    @classmethod\n    def from_string(cls, rule_string) -&gt; 'MCFGRule':\n        elem_strs = re.findall('(\\w+)\\(((?:\\w+,? ?)+?)\\)', rule_string)\n\n        elem_tuples = [(var, [v.strip()\n                              for v in svs.split(',')])\n                       for var, svs in elem_strs]\n\n        if len(elem_tuples) == 1:\n            return cls(MCFGRuleElement(elem_tuples[0][0],\n                                   tuple(w for w in elem_tuples[0][1])))\n\n        else:\n            strvars = [v for _, sv in elem_tuples[1:] for v in sv]\n\n            # no duplicate string variables\n            try:\n                assert len(strvars) == len(set(strvars))\n            except AssertionError:\n                msg = 'variables duplicated on right side of '+rule_string\n                raise ValueError(msg)\n\n            \n            elem_left = MCFGRuleElement(elem_tuples[0][0],\n                                    *[tuple([strvars.index(v)\n                                             for v in re.findall('('+'|'.join(strvars)+')', vs)])\n                                      for vs in elem_tuples[0][1]])\n\n            elems_right = [MCFGRuleElement(var, *[(strvars.index(sv),)\n                                              for sv in svs])\n                           for var, svs in elem_tuples[1:]]\n\n            return cls(elem_left, *elems_right)\n        \n    def string_yield(self):\n        if self.is_epsilon:\n            return self._left_side.variable\n        else:\n            raise ValueError(\n                'string_yield is only implemented for epsilon rules'\n            )\n\nTo make grammar loading easier, I have provided a class method for reading rules from strings.\n\nrule = MCFGRule.from_string('A(w1u, x1v) -&gt; B(w1, x1) C(u, v)')\n\nrule\n\n&lt;Rule: A(02, 13) -&gt; B(0, 1) C(2, 3)&gt;\n\n\nOne very important instance method to note in this implementation is MCFGRule.instantiate_left_side. This method will be crucial in your implementations–particularly, in implementing the inference rules of the parser. Before you do anything else, you should make sure to understand how it works. An example usage can be found below.\n\nrule.instantiate_left_side(\n    MCFGRuleElementInstance(\"B\", (1, 2), (5, 7)),\n    MCFGRuleElementInstance(\"C\", (2, 4), (7, 8))\n)\n\nA([1, 4], [5, 8])\n\n\nThis functionality is the main reason I’m providing you an implementation of MCFGRule in the first place: it is nontrivial but mostly because it is tedious, rather than difficult to understand conceptually. I’m more concerned with you demonstrating understanding of deductive parsing in general and agenda-based parsing specifically."
  },
  {
    "objectID": "assignments/final-project.html#structuring-your-package",
    "href": "assignments/final-project.html#structuring-your-package",
    "title": "Final Project",
    "section": "Structuring your package",
    "text": "Structuring your package\nI suggest structuring your package into three separate modules:\n\ngrammar: classes for representing MCFGs, including the classes for representing rules given above and a class for representing a grammar as a collection of such rules that provides an interface to your agenda-based parser.\nparser: classes for representing your parser and any data structures that your parser manipulates.\ntree: a class for representing parse trees produced by your parser.\n\nYou may draw on any code I have provided you in the course. The code from Assignments 9 and 10 may be particularly useful for you.\nThe structure of your test suite should mirror the structure of your package–with each module having its own test module and each test module being structured analogously to the module it tests. For instance, if you are testing a class, you should have a corresponding test class in your test module that bundles together all of the tests for methods in that class.\nYou must have tests every class and every method within that class, even the most trivial. This requirement includes the code I give you above: you must provide tests of MCFGRuleElement, MCFGRuleElementInstance, and MCFGRule. Indeed, I would suggest that the first thing you do is to create a grammar module that contains these classes and begin writing your test suite with a test_grammar module in tests/.\nFinally, all of your code must contain docstrings in numpy style for every module, every class, and every public method implemented in a class. (Ideally, you would provide docstrings for the private methods as well, but I will not require that.) I suggest you also type-hint every method."
  }
]