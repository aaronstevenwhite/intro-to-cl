[
  {
    "objectID": "finite-state-models/formal-definition/index.html",
    "href": "finite-state-models/formal-definition/index.html",
    "title": "Overview",
    "section": "",
    "text": "We’ll start with an application of FSAs to modeling licit English syllables to get a sense for what they look like, then we’ll formalize them. To visualize FSAs, we’ll use pynini (We’ll also use pynini later when we define finite state transducers.)"
  },
  {
    "objectID": "finite-state-models/generative-capacity/index.html",
    "href": "finite-state-models/generative-capacity/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 20, 2024."
  },
  {
    "objectID": "finite-state-models/recognition-and-parsing/index.html",
    "href": "finite-state-models/recognition-and-parsing/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 18, 2024."
  },
  {
    "objectID": "finite-state-models/phonological-rules-as-fsas/index.html",
    "href": "finite-state-models/phonological-rules-as-fsas/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 25, 2024."
  },
  {
    "objectID": "context-free-models/index.html",
    "href": "context-free-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around March 27, 2024."
  },
  {
    "objectID": "mildly-context-sensitive-models/index.html",
    "href": "mildly-context-sensitive-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "Availability\n\n\n\nThis module will be available around April 15, 2024."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html",
    "href": "assignments/assignments-1-and-2.html",
    "title": "Assignments 1 and 2",
    "section": "",
    "text": "Assignment 1 will consist of Tasks 1-3 and Assignment 2 will consist of Tasks 4-8.\nIn these assignments, you will be implementing and testing a vowel harmony rule system for Turkish. Vowel harmony rule systems are intended to explain the fact that, in some languages, vowels in a word must have the same value on certain phonological features. Your job in this assignment will not be to derive the rule system itself. Rather, I’m going to give you a rule system to implement that works reasonably well, and we’ll ask where it fails."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#mathematical-objects",
    "href": "assignments/assignments-1-and-2.html#mathematical-objects",
    "title": "Assignments 1 and 2",
    "section": "Mathematical objects",
    "text": "Mathematical objects\nThroughout the assignments, I will be asking you to say what kind of mathematical object you are implementing in a particular task. The kind of answers you might give here are relation and function. If your response is function, it should be as specific as possible–e.g. the function may be partial or total. In addition to specifying partiality and totality, I’d also like you to specify whether a function is injective and/or surjective. An injective function is one where, if \\(f(x) = f(y)\\), then \\(x = y\\) for all \\(x\\) and \\(y\\). A surjective function is one where, if \\(f: X \\rightarrow Y\\), then \\(f(X) = Y\\)—i.e. the range of \\(f\\) is the same as its codomain; or said another way, the image of \\(X\\) under \\(f: X \\rightarrow Y\\) is \\(Y\\)."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#data",
    "href": "assignments/assignments-1-and-2.html#data",
    "title": "Assignments 1 and 2",
    "section": "Data",
    "text": "Data\nThis assignment uses Bruce Hayes’ phonological features spreadsheet—his FeaturesDoulosSIL.xls sheet, which I have converted into a UTF-8 encoded CSV for easier processing in Python. This file contains the equivalent of the IPA charts familiar to you from LIN110.\nYou do not need the full chart for this assignment, since we will only need access to four features–SYLLABIC, HIGH, FRONT, and ROUND–and the phones that Turkish has. We’ll work with the slightly altered version of the chart below, which only contains the features for these phones and maps 0 to -.\n\nfeatures = '''phone,syllabic,high,front,round\nɑ,+,-,-,-\nb,-,-,-,-\nd͡ʒ,-,-,-,-\nt͡ʃ,-,-,-,-\nd,-,-,-,-\ne,+,-,+,-\nf,-,-,-,-\nɟ,-,+,+,-\nj,-,+,+,-\nh,-,-,-,-\nɯ,+,+,-,-\ni,+,+,+,-\nʒ,-,-,-,-\nc,-,+,+,-\nl,-,-,-,-\nm,-,-,-,-\nn,-,-,-,-\no,+,-,-,+\nø,+,-,+,+\np,-,-,-,-\nɾ,-,-,-,-\ns,-,-,-,-\nʃ,-,-,-,-\nt,-,-,-,-\nu,+,+,-,+\ny,+,+,+,+\nv,-,-,-,-\nj,-,+,+,-\nz,-,-,-,-'''\n\nwith open('features.csv', 'w') as fout:\n    fout.write(features)\n\n\n%%bash\ncat features.csv\n\nphone,syllabic,high,front,round\nɑ,+,-,-,-\nb,-,-,-,-\nd͡ʒ,-,-,-,-\nt͡ʃ,-,-,-,-\nd,-,-,-,-\ne,+,-,+,-\nf,-,-,-,-\nɟ,-,+,+,-\nj,-,+,+,-\nh,-,-,-,-\nɯ,+,+,-,-\ni,+,+,+,-\nʒ,-,-,-,-\nc,-,+,+,-\nl,-,-,-,-\nm,-,-,-,-\nn,-,-,-,-\no,+,-,-,+\nø,+,-,+,+\np,-,-,-,-\nɾ,-,-,-,-\ns,-,-,-,-\nʃ,-,-,-,-\nt,-,-,-,-\nu,+,+,-,+\ny,+,+,+,+\nv,-,-,-,-\nj,-,+,+,-\nz,-,-,-,-\n\n\nIf you are interested in doing further work in computational phonology, you might also check out the panphon package, which provides various tools for working with featurizations of phones."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#definition",
    "href": "assignments/assignments-1-and-2.html#definition",
    "title": "Assignments 1 and 2",
    "section": "Definition",
    "text": "Definition\nTo represent (e.g. FRONT, ROUND, etc.) and feature values (+, -), we will use two Enum classes: Feature and FeatureValue. Using Enums here allows us to define the set of possible feature names and feature values and thereby constrain the values that can appear in feature valuations. This functionality is useful as an additional check on the correctness of our code–e.g. in the case that we get invalid feature names or feature values.\n\nfrom enum import Enum\n\nclass Feature(Enum):\n    SYLLABIC = \"syllabic\"\n    HIGH = \"high\"\n    FRONT = \"front\"\n    ROUND = \"round\"\n\n    def __repr__(self):\n        return self.value\n\n    def __str__(self):\n        return self.__repr__()\n\nclass FeatureValue(Enum):\n    PLUS = \"+\"\n    MINUS = \"-\"\n\n    def __repr__(self):\n        return self.value\n\n    def __str__(self):\n        return self.__repr__()\n\nTo represent the relationship between feature names and feature values—encoded in the rows of features.csv—we’ll be using FeatureValuation objects, which are just thin wrappers around a dictionary with feature names (e.g. FRONT, ROUND, etc.) as keys and feature values (+, -) as values.\nImportantly, note that, unlike dictionaries, FeatureValuations are hashable, since they implement the __hash__ magic method. Usually, we want hashables to be immutable–e.g. lists and sets are mutable and not hashable while tuples and frozensets are immutable and hashable–though python does not enforce this. In this case, I will demarcate that we want the core data of the feature valuation to be a private instance attribute FeatureValuation._valuation by prepending an underscore to the attribute name: when you see an underscore prepended like this, it is a convention that you should not modify its value from outside the object it is an attribute of. If you need to access the raw dictionary (and you will need to), you should use the FeatureValuation.valuation property.\nThe __hash__ magic method more specifically determines what the hash function from the python standard library outputs when applied to a FeatureValuation object. This output will be an integer that is used in determining how to identify when to instances of the class are the same for the purposes of uniquely identifying them within a collection—e.g. when an element of a set or a dict key.\nThe upshot for our purposes is that, if a class implements __hash__, its objects can be used as dictionary keys. The class also implements comparison between feature valuations: == (__eq__), &gt; (__gt__), &lt; (__lt__), &gt;= (__ge__), and &lt;= (__le__). This behavior will be very useful for some tasks.\n\nclass FeatureValuation:\n    '''A mapping from feature names to feature values\n    \n    Parameters\n    ----------\n    valuation\n        the feature valuation as a dictionary\n    '''\n    \n    def __init__(self, valuation: dict[str, str]):\n        self._valuation = {\n            Feature(f): FeatureValue(v) \n            for f, v in valuation.items()\n        }\n    \n    def __hash__(self) -&gt; int:\n        return hash(tuple(self._valuation.items()))\n    \n    def __getitem__(self, key: Feature) -&gt; FeatureValue:\n        return self._valuation[key]\n    \n    def __eq__(self, other: 'FeatureValuation') -&gt; bool:\n        self.__class__._check_type(other)\n        \n        return self._valuation == other._valuation\n    \n    def __lt__(self, other: 'FeatureValuation') -&gt; bool:\n        self.__class__._check_type(other)\n        \n        if set(self._valuation) &lt; set(other._valuation):\n            return all(other._valuation[k] == v \n                       for k, v in self._valuation.items())\n        else:\n            return False\n    \n    def __gt__(self, other: 'FeatureValuation') -&gt; bool:        \n        return other &lt; self\n\n    def __le__(self, other: 'FeatureValuation') -&gt; bool:\n        return self == other or self &lt; other\n    \n    def __ge__(self, other: 'FeatureValuation') -&gt; bool:\n        return self == other or self &gt; other\n\n    def __repr__(self):\n        return self._valuation.__repr__()\n\n    def __str__(self):\n        return self._valuation.__str__()\n    \n    @property\n    def valuation(self) -&gt; dict[Feature, FeatureValue]:\n        return dict(self._valuation) # makes a copy\n\n    @classmethod\n    def _check_type(cls, obj):\n        try:\n            assert isinstance(obj, cls)\n        except AssertionError:\n            raise ValueError(\n                'can only compute equality between'\n                ' two FeatureValuation objects'\n            )\n\nWe can construct a FeatureValuation by calling its __init__ magic method on a Dict[str, str].\n\nfv1 = FeatureValuation({'syllabic': '+', 'round': '+'})\nfv2 = FeatureValuation({'syllabic': '+', 'round': '+', 'high': '+'})\n\nAnd note that because FeatureValuations are hashable, we can use them as dictionary keys.\n\nv1 = {fv1: {'o', 'ø', 'u', 'y'}}\nv2 = {fv2: {'u', 'y'}}\n\nAnd because we have defined __eq__, __lt__, and __gt__, we can compare FeatureValuations. Make sure you understand what each comparison does. You will need at least one of these operations for the tasks below.\n\nfv1 == fv1, fv1 &lt; fv2, fv1 &gt; fv2\n\n(True, True, False)\n\n\nFinally, to show you that hash works and returns an integer:\n\nhash(fv2)\n\n-2436770590250344338"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-1",
    "href": "assignments/assignments-1-and-2.html#task-1",
    "title": "Assignments 1 and 2",
    "section": "Task 1",
    "text": "Task 1\nLines: 5\nDefine a class method from_csv in the PhonologicalFeatureChart1 class defined below. This method should take as input a string representation of the directory path fpath to features.csv and return a PhonologicalFeatureChart1 object. This object should have a dictionary-valued private attribute _phone_to_features with phones as keys and FeatureValuation objects as values.\n(Note: I’m calling this class PhonologicalFeatureChart1 so that we can subclass it later without a bunch of copying and pasting. This isn’t strictly necessary for subclassing purposes, since you could simply subclass an new version of PhonologicalFeatureChart with an old version; but it’s useful here so that, if you run the cells out of order, you know exactly which version of the class you’re working with.) I’ll do this for other classes below without comment.)\n\nclass PhonologicalFeatureChart1:\n    '''The phonological features of different phones'''\n\n    def __init__(self, phone_to_features: Dict[str, FeatureValuation]):\n        self._phone_to_features = phone_to_features\n\n    def __repr__(self):\n        return self._phone_to_features.__repr__()\n\n    def __str__(self):\n        return self._phone_to_features.__str__()\n\n    @classmethod\n    def from_csv(cls, fpath: str='features.csv') -&gt; 'PhonologicalFeatureChart1':\n        '''Load Hayes' phonological feature chart\n\n        Parameters\n        ----------\n        fpath\n            path to phonological feature chart as a csv\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\n    def phone_to_features(self, phone: str) -&gt; FeatureValuation:\n        return self._phone_to_features[phone]\n\nWrite a test that checks for the correctness of from_csv by calling phone_to_features on some phone and making sure that it returns the correct feature valuation. (The fact that feature valuations implement __eq__ will be useful for this.) This (and all future) test should use standard Python exception handling facilities (try-except).\n\ntry:\n    phonological_feature_chart = PhonologicalFeatureChart1.from_csv()\nexcept NotImplementedError:\n    print(\"You still need to implement PhonologicalFeatureChart1.from_csv.\")\n\n# WRITE TESTS HERE\n\nYou still need to implement PhonologicalFeatureChart1.from_csv\n\n\nReferring to the set of feature as \\(F = \\{\\text{FRONT}, \\text{ROUND}, \\text{HIGH}, \\text{SYLLABIC}\\}\\) and the set of feature values as \\(V = \\{+, -\\}\\), explain what kind of mathematical object the feature valuations you just constructed are. If they are functions, say whether they are injective and/or surjective. Note that I am not asking about all possible feature valuations—just the ones constructed in from_csv.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-2",
    "href": "assignments/assignments-1-and-2.html#task-2",
    "title": "Assignments 1 and 2",
    "section": "Task 2",
    "text": "Task 2\nLines: 2\nDefine an instance method phone_from_features in the PhonologicalFeatureChart2 class that takes as input a FeatureValuation object and returns the set of phones that match that feature valuation. Assume that feature valuations need not specify a feature value for all feature names—e.g. the following should still return something (namely, all the high vowels).\n\ntry:\n    chart = PhonologicalFeatureChart2.from_csv('features.csv')\n    valuation = FeatureValuation({'syllabic': '+', 'high': '+'})\n    chart.phone_from_features(valuation)\nexcept NameError:\n    print(\"You still need to define PhonologicalFeatureChart2.\")\n\nYou still need to define PhonologicalFeatureChart2\n\n\nWe will refer to valuations like this as partial feature valuations.\nNote that you need to return a set because some phones are not uniquely determined by the features in features.csv—e.g. all consonants (besides the semivowels) will be - on these features. Further, it may return an empty set, since some feature combinations do not show up in features.csv—e.g. [-SYLLABIC, +ROUND].\n\nclass PhonologicalFeatureChart2(PhonologicalFeatureChart1):\n    '''The phonological features of different phones'''\n\n    def phone_from_features(self, features: FeatureValuation) -&gt; set[str]:\n        '''The phones that have a particular feature valuation\n\n        Parameters\n        ----------\n        features\n            the feature valuation\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of phone_from_features. This test should check at least five cases: (i) one where a singleton set should be returned when a total feature valuation is input; (ii) one where an empty set should be returned when a total feature valuation is input; (iii) one where a non-empty, non-singleton set should be returned when a total feature valuation is input; (iv) one where an empty set should be returned when a partial feature valuation is input; and (v) one where a non-empty, non-singleton set should be returned when a partial feature valuation is input.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object phone_from_features implements and what kind of object a partial feature valuation is, referring to the set of phones as \\(P\\). There are two possible answers here depending on what you take the right side of the relation/function to be.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-3",
    "href": "assignments/assignments-1-and-2.html#task-3",
    "title": "Assignments 1 and 2",
    "section": "Task 3",
    "text": "Task 3\nLines: 2\nUsing your phone_from_features method, define an instance method alter_features_of_phone in PhonologicalFeatureChart (our final version, so no number) that takes as input a phone and a (partial) feature valuation like valuation above. This function should return the set of phones that correspond to setting that phone’s features to the values listed in the feature valuation. For instance, if I passed this function the phone /u/ and the (partial) feature valuation [-ROUND], the function should return {/ɯ/}, but if I passed it /u/ and the feature valuation [-SYLLABIC, -HIGH, -LOW, -ROUND], it should return the set of consonants.\n\nclass PhonologicalFeatureChart(PhonologicalFeatureChart2):\n    '''The phonological features of different phones'''\n\n    def alter_features_of_phone(\n        self, phone: str, \n        features: FeatureValuation\n    ) -&gt; Set[str]:\n        '''The phones with features altered\n\n        Parameters\n        ----------\n        phone\n            the phone whose features we want to alter\n        features\n            the feature to alter\n        '''\n\n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of alter_features_of_phone. This test should check the same five kinds of cases that your test for Task 2 checked.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object alter_features_of_phone implements. There are two possible answers here depending on what you take the right side of the relation/function to be. Note that the left side of the relation is a tuple.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#data-1",
    "href": "assignments/assignments-1-and-2.html#data-1",
    "title": "Assignments 1 and 2",
    "section": "Data",
    "text": "Data\nThe remainder of this assignment is based on data from the UniMorph project – specifically, Turkish UniMorph. The UniMorph project provides a schema for annotating word forms with their root form and the morphological features they express across languages, as well as annotated data for (currently) 168 languages. Take a look at the Turkish dataset. You’ll notice that it consists of three columns.\n    hamsi          hamsiler          N;NOM;PL\n    hamsi          hamsilere         N;DAT;PL\n    hamsi          hamsilerden       N;ABL;PL\n    hamsi          hamsinin          N;GEN;SG\n    hamsi          hamsiye           N;DAT;SG\n    hamsi          hamsiyi           N;ACC;SG\n    hamsi          hamsilerin        N;GEN;PL\n    hamsi          hamsileri         N;ACC;PL\n    hamsi          hamsiden          N;ABL;SG\n    hamsi          hamsilerde        N;LOC;PL\n    hamsi          hamside           N;LOC;SG\n    hamsi          hamsi             N;NOM;SG\nThe second column contains word forms; the first contains the root corresponding to that form; and the third corresponds to the part of speech of and morphological features expressed by that form, separated by ;.\nI have included some code below that should make working with these data easier by loading Turkish Unimorph as an instance of my custom Unimorph class, defined below. Before moving forward, read through this code to make sure you understand what turkish_unimorph is.\n\nfrom collections import defaultdict\n\nclass Unimorph:\n\n    def __init__(self, fpath, pos_filter=lambda x: True, root_filter=lambda x: True,\n                 word_filter=lambda x: True, feature_filter=lambda x: True,\n                 graph_to_phone_map=None):\n\n        self._graph_to_phone_map = graph_to_phone_map\n\n        self._pos_filter = pos_filter\n        self._root_filter = root_filter\n        self._word_filter = word_filter\n        self._feature_filter = feature_filter\n        \n        self._load_unimorph(fpath)\n\n    def __getitem__(self, key):\n        return self._pos_to_word_to_features[key]\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        try:\n            return next(self._gen)\n        except StopIteration:\n            self._initialize_gen()\n            raise\n\n    def _load_unimorph(self, fpath):\n        '''load unimorph file and convert graphs to ipa\n\n        Parameters\n        ----------\n        fpath : str\n            path to unimorph data\n        \n\n        Returns\n        -------\n        tuple(dict)\n        '''\n\n        pos_to_word_to_features = defaultdict(lambda:\n                                              defaultdict(lambda:\n                                                          defaultdict(set)))\n\n        with open(fpath) as f:\n            for line in f:\n                line_split = line.strip().split('\\t')\n\n                if len(line_split) != 3:\n                    continue\n\n                root, word, pos_features = line_split\n\n                pos_features_split = pos_features.split(';')\n\n                pos = pos_features_split[0]\n                features = set(pos_features_split[1:])\n\n                if self._graph_to_phone_map is not None:\n                    try:\n                        root = self._convert_graph_to_phone(root)\n                        word = self._convert_graph_to_phone(word)\n                    except KeyError:\n                        continue\n                else:\n                    root = tuple(root)\n                    word = tuple(word)\n                        \n\n                keep = self._pos_filter(pos)\n                keep &= self._root_filter(root)\n                keep &= self._word_filter(word)\n                keep &= self._feature_filter(features)\n\n                if keep:\n                    pos_to_word_to_features[pos][root][word] = features\n\n        # freeze dict so it is no longer a defaultdict\n        self._pos_to_word_to_features = dict(pos_to_word_to_features)\n\n        self._initialize_gen()\n\n    def _initialize_gen(self):\n        self._gen = ((pos, root, word, features)\n                     for pos, d1 in self._pos_to_word_to_features.items()\n                     for root, d2 in d1.items()\n                     for word, features in d2.items())\n        \n    def _convert_graph_to_phone(self, word):\n        '''map graphs to phones\n\n        Parameters\n        ----------\n        word : str\n            the word as a string of graphs\n\n        Returns\n        -------\n        str\n        '''\n\n        # this takes the last phone in the list\n        # it should maybe create a set of possible words\n        return tuple([self._graph_to_phone_map[graph][-1]\n                      for graph in word])\n\n\ngraph_to_phone_map = {'a': ['ɑ'],\n                      'b': ['b'],\n                      'c': ['d͡ʒ'],\n                      'ç': ['t͡ʃ'],\n                      'd': ['d'],\n                      'e': ['e'],\n                      'f': ['f'],\n                      'g': ['ɡ̟', 'ɟ'],\n                      'ğ': ['ː', '‿', 'j'],\n                      'h': ['h'],\n                      'ı': ['ɯ'],\n                      'i': ['i'],\n                      'j': ['ʒ'],\n                      'k': ['k', 'c'],\n                      'l': ['ɫ', 'l'],\n                      'm': ['m'],\n                      'n': ['n'],\n                      'o': ['o'],\n                      'ö': ['ø'],\n                      'p': ['p'],\n                      'r': ['ɾ'],\n                      's': ['s'],\n                      'ş': ['ʃ'],\n                      't': ['t'],\n                      'u': ['u'],\n                      'ü': ['y'],\n                      'v': ['v'],\n                      'y': ['j'],\n                      'z': ['z'],\n                      ' ': [' ']}\n\n\nimport requests\nfrom io import BytesIO\nfrom zipfile import ZipFile\n\nturkish_unimorph_url = 'https://github.com/unimorph/tur/archive/master.zip'\nturkish_unimorph_zip = requests.get(turkish_unimorph_url).content\n\nwith ZipFile(BytesIO(turkish_unimorph_zip)) as zf:\n    with zf.open('tur-master/tur') as f_in:\n        with open('tur.txt', 'w') as f_out:\n            f_out.write(f_in.read().decode())\n\n\nturkish_unimorph = Unimorph('tur.txt',\n                            pos_filter=lambda x: x == 'N',\n                            root_filter=lambda x: ' ' not in x,\n                            word_filter=lambda x: ' ' not in x,\n                            feature_filter=lambda x: x.issubset({'PL', 'GEN'}),\n                            graph_to_phone_map=graph_to_phone_map)\n\nThere are two important things to notice. First, words and roots are represented as tuples of strings, instead of strings. The reason for this is that (i) I map each root and word in Turkish Unimorph to a phonetic/phonemic representation using a fixed mapping from graphs to phones; and (ii) some phones are represented as digraphs or trigraphs in unicode (e.g. t͡ʃ), so if we mapped from strings of graphs to strings of phones, it would be difficult to recover which characters in a string are a single phone and which are part of a phone that unicode represents with multiple symbols. Second, my Unimorph class allows the user to pass filters to the constructor __init__. In the current case, I have set these filters so our Unimorph instance only contains plural and/or genitive nouns."
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-4",
    "href": "assignments/assignments-1-and-2.html#task-4",
    "title": "Assignments 1 and 2",
    "section": "Task 4",
    "text": "Task 4\nLines: 24\nIn standard descriptions of Turkish, the vowel harmony rule system plays out on three features: height [+/-HIGH], frontness [+/-FRONT], and roundedness [+/-ROUND]. Roughly, if a vowel is high, it must match with the immediately previous vowel on both frontness and roundedness; and if it is not high and not round, it must match with the immediately previous vowel on frontness.\nUsing your alter_features_of_phone method, define a class TurkishVowelHarmony1 whose instances take as input a word and applies the vowel harmony rule system to it (implemented using the __call__ magic method). Pay special attention to the fact that this system only looks at the immediately previous vowel.\n\nString = tuple[str]\n\nclass TurkishVowelHarmony1:\n    '''The Turkish vowel harmony system'''\n    \n    def __call__(self, word: String) -&gt; String:\n        '''Apply the vowel harmony rule\n        \n        Parameters\n        ----------\n        word\n            the word to apply the vowel harmony rule to\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of __call__. It should check at least six cases: (i) three randomly selected words found in Turkish Unimorph where the result of applying a TurkishVowelHarmony1 object to those words returns the same word back; and (ii) three randomly selected words found in Turkish Unimorph where it doesn’t.\n\n# WRITE TESTS HERE\n\nExplain what kind of mathematical object turkish_vowel_harmony implements, referring to the set of Turkish phones as \\(\\Sigma\\) and the set of strings over those phones as \\(\\Sigma^*\\). (Remember that \\(\\Sigma^* \\equiv \\bigcup_i^\\infty \\Sigma^i\\).)\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-5",
    "href": "assignments/assignments-1-and-2.html#task-5",
    "title": "Assignments 1 and 2",
    "section": "Task 5",
    "text": "Task 5\nLines: 1\nA disharmonic form is a root/word that does not obey the vowel harmony rule. Write an instance method disharmonic in TurkishVowelHarmony that maps a root or word to a boolean indicating whether or not it that root or word is disharmonic.\n\nclass TurkishVowelHarmony2(TurkishVowelHarmony1):\n    '''The Turkish vowel harmony system'''\n    \n    def disharmonic(self, word: Tuple[str]) -&gt; bool:\n        '''Whether the word is disharmonic\n        \n        Parameters\n        ----------\n        word\n            the word to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nWrite a test that checks for the correctness of disharmonic. It should check the same six cases you used to test __call__.\n\n# WRITE TESTS HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-6",
    "href": "assignments/assignments-1-and-2.html#task-6",
    "title": "Assignments 1 and 2",
    "section": "Task 6",
    "text": "Task 6\nLines: 2\nUsing your disharmonic method, write another instance method proportion_disharmonic_roots to compute the proportion of roots that are disharmonic in Turkish Unimorph.\n\nclass TurkishVowelHarmony3(TurkishVowelHarmony2):\n    '''The Turkish vowel harmony system'''\n    \n    def proportion_disharmonic_roots(self, lexicon: Unimorph) -&gt; float:\n        '''The proportion of words that are disharmonic in the lexicon\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-7",
    "href": "assignments/assignments-1-and-2.html#task-7",
    "title": "Assignments 1 and 2",
    "section": "Task 7",
    "text": "Task 7\nLines: 7\nUsing your disharmonic method, write an instance method xtab_root_word_harmony to cross-tabulate the proportion of words that are disharmonic against whether those words’ roots are disharmonic. The method should print that cross-tabulation as a \\(2 \\times 2\\) table with root (dis)harmony along the rows and word (dis)harmony along the columns.\n\nclass TurkishVowelHarmony4(TurkishVowelHarmony3):\n    '''The Turkish vowel harmony system'''\n    \n    def xtab_root_word_harmony(self, lexicon: Unimorph) -&gt; None:\n        '''Cross-tabulate word disharmony against root disharmony\n        \n        This should print (not return) a table represented as a list of lists:\n        \n                         | harmonic word | disharmonic word |\n                         ------------------------------------\n           harmonic root |               |                  |\n        disharmonic root |               |                  |\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nExplain the pattern that you see in this table.\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-1-and-2.html#task-8",
    "href": "assignments/assignments-1-and-2.html#task-8",
    "title": "Assignments 1 and 2",
    "section": "Task 8",
    "text": "Task 8\nLines: 1\nUsing your disharmonic function, write an instance method get_disharmonic to find all of the words of some category (e.g. N, V, etc.) with a particular set of features (e.g. {plural, genitive}, etc.). Use that method to find all the plural and/or genitive nouns with disharmonic roots. Note that I’ve prefiltered Turkish Unimorph to just the plural and genitive nouns, but this method should still work for arbitrary categories and morphological features.\n\nclass TurkishVowelHarmony(TurkishVowelHarmony4):\n    '''The Turkish vowel harmony system'''\n    \n    def get_disharmonic(self, \n                        lexicon: Unimorph, \n                        category: str,\n                        features: Set[str]) -&gt; Set[Tuple[str]]:\n        '''Find all of the words of some category with a particular set of features\n        \n        Parameters\n        ----------\n        lexicon\n            the Unimorph lexicon to check for disharmony\n        category\n            some category (e.g. \"N\", \"V\", etc.)\n        features\n            some set of features (e.g. {\"PL\", \"GEN\"}, etc.)\n        '''\n        \n        # remove after implementing\n        raise NotImplementedError\n\nExplain what pattern you see in the vowels of the plural and genitive affixes. (A prerequisite for answering this question is figuring out what the plural and genitive affixes are.)\nWRITE YOUR ANSWER HERE"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html",
    "href": "assignments/assignments-3-and-4.html",
    "title": "Assignments 3 and 4",
    "section": "",
    "text": "Like Assignments 1 and 2, Assignments 3 and 4 are bundled together. You only need to do Task 1 for Assignment 3 and Tasks 2 and 3 for Assignment 4.\nThese assignments focus on implementing fuzzy tree search. In class, we developed various tree search algorithms that look for an exact match between a query and the data contained in particular trees. I’ve copied in the relevant class below as TreeOld.\nimport pyparsing\nfrom typing import TypeVar, Union\nfrom rdflib import Graph, URIRef\n\nDataType = TypeVar(\"DataType\")\nTreeList = list[str | tuple[Union[str, 'TreeList']]]\n\nclass TreeOld:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n    children\n    \"\"\"\n    \n    RDF_TYPES = {}\n    RDF_EDGES = {'is': URIRef('is-a'),\n                 'parent': URIRef('is-the-parent-of'),\n                 'child': URIRef('is-a-child-of'),\n                 'sister': URIRef('is-a-sister-of')}\n    \n    LPAR = pyparsing.Suppress('(')\n    RPAR = pyparsing.Suppress(')')\n    DATA = pyparsing.Regex(r'[^\\(\\)\\s]+')\n\n    PARSER = pyparsing.Forward()\n    SUBTREE = pyparsing.ZeroOrMore(PARSER)\n    PARSERLIST = pyparsing.Group(LPAR + DATA + SUBTREE + RPAR)\n    PARSER &lt;&lt;= DATA | PARSERLIST\n    \n    def __init__(self, data: DataType, children: list['TreeOld'] = []):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n  \n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string()\n     \n    def to_string(self, depth: int = 0) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; DataType:\n        if isinstance(idx, int):\n            return self._children[idx]\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        elif idx:\n            return self._children[idx[0]].__getitem__(idx[1:])\n        else:\n            return self\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['TreeOld']:\n        return self._children\n        \n    def _validate(self):\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n            \n    def index(self, data: DataType, index_path: tuple[int] = tuple()):\n        indices = [index_path] if self._data==data else []\n        root_path = [] if index_path == -1 else index_path\n        \n        indices += [j \n                    for i, c in enumerate(self._children) \n                    for j in c.index(data, root_path+(i,))]\n\n        return indices\n            \n    def to_rdf(\n        self, \n        graph: Graph | None=None, \n        nodes: dict[int, URIRef] = {}, \n        idx: tuple[int] = tuple()\n    ) -&gt; Graph: \n        graph = Graph() if graph is None else graph\n        \n        idxstr = '_'.join(str(i) for i in idx)\n        nodes[idx] = URIRef(idxstr)\n            \n        if self._data not in self.RDF_TYPES:\n            self.RDF_TYPES[self._data] = URIRef(self._data)\n\n        typetriple = (nodes[idx], \n                      self.RDF_EDGES['is'],\n                      self.RDF_TYPES[self.data])\n\n        graph.add(typetriple)\n\n        for i, child in enumerate(self._children):\n            childidx = idx+(i,)\n            child.to_rdf(graph, nodes, childidx)\n                \n            partriple = (nodes[idx], \n                         self.RDF_EDGES['parent'],\n                         nodes[childidx])\n            chitriple = (nodes[childidx], \n                         self.RDF_EDGES['child'],\n                         nodes[idx])\n            \n            graph.add(partriple)\n            graph.add(chitriple)\n            \n        for i, child1 in enumerate(self._children):\n            for j, child2 in enumerate(self._children):\n                child1idx = idx+(i,)\n                child2idx = idx+(j,)\n                sistriple = (nodes[child1idx], \n                             Tree.RDF_EDGES['sister'],\n                             nodes[child2idx])\n                \n                graph.add(sistriple)\n        \n        self._rdf_nodes = nodes\n        \n        return graph\n    \n    @property\n    def rdf(self) -&gt; Graph:\n        return self.to_rdf()\n    \n    def find(self, query):\n        return [tuple([int(i) \n                       for i in str(res[0]).split('_')]) \n                for res in self.rdf.query(query)]\n    \n    @classmethod\n    def from_string(cls, treestr: str) -&gt; 'TreeOld':\n        treelist = cls.PARSER.parseString(treestr[2:-2])[0]\n        \n        return cls.from_list(treelist)\n    \n    @classmethod\n    def from_list(cls, treelist: TreeList):\n        if isinstance(treelist, str):\n            return cls(treelist[0])\n        elif isinstance(treelist[1], str):\n            return cls(treelist[0], [cls(treelist[1])])\n        else:\n            return cls(treelist[0], [cls.from_list(l) for l in treelist[1:]])\nIn fuzzy search, we allow this exact matching restriction to be loosened by instead allowing that matches be (i) within some fixed edit distance; and/or (ii) closest (in terms of edit distance) to the query among all pieces of data. I’ve copied the relevant edit distance class that we developed in class below as EditDistance.\nimport numpy as np\n\nclass EditDistance:\n    '''Distance between strings\n\n\n    Parameters\n    ----------\n    insertion_cost\n    deletion_cost\n    substitution_cost\n    '''\n    \n    def __init__(self, insertion_cost: float = 1., \n                 deletion_cost: float = 1., \n                 substitution_cost: float | None = None):\n        self._insertion_cost = insertion_cost\n        self._deletion_cost = deletion_cost\n\n        if substitution_cost is None:\n            self._substitution_cost = insertion_cost + deletion_cost\n        else:\n            self._substitution_cost = substitution_cost\n\n    def __call__(self, source: str | list[str], target: str | list[str]) -&gt;  float:\n        '''The edit distance between the source and target\n        \n        The use of lists enables digraphs to be identified\n        \n        Parameters\n        ----------\n        source\n        target\n        '''\n        \n        # coerce to list if not a list\n        if isinstance(source, str):\n            source = list(source)\n            \n        if isinstance(target, str):\n            target = list(target)\n        \n        n, m = len(source), len(target)\n        source, target = ['#']+source, ['#']+target\n\n        distance = np.zeros([n+1, m+1], dtype=float)\n        \n        for i in range(1,n+1):\n            distance[i,0] = distance[i-1,0]+self._deletion_cost\n\n        for j in range(1,m+1):\n            distance[0,j] = distance[0,j-1]+self._insertion_cost\n            \n        for i in range(1,n+1):\n            for j in range(1,m+1):\n                if source[i] == target[j]:\n                    substitution_cost = 0.\n                else:\n                    substitution_cost = self._substitution_cost\n                    \n                costs = np.array([distance[i-1,j]+self._deletion_cost,\n                                  distance[i-1,j-1]+substitution_cost,\n                                  distance[i,j-1]+self._insertion_cost])\n                    \n                distance[i,j] = costs.min()\n                \n        return distance[n,m]"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-1",
    "href": "assignments/assignments-3-and-4.html#task-1",
    "title": "Assignments 3 and 4",
    "section": "Task 1",
    "text": "Task 1\nLines: 14\nDefine an instance method fuzzy_find. This method should take a piece of query data and optionally a distance and return all of the nodes that have data that is within edit distance distance from data; and/or if , closest is True, it should return all of the nodes closest to data among all nodes in the tree.\nFor instance:\n\nfuzzy_find('review', distance=3., closest=False) will return a tuple of every piece of data in the tree within edit distance 3 from review (e.g. view, reviewer, reviews, etc.), its distance to review, and its index; if there is nothing within that edit distance, an empty list will be returned\nfuzzy_find('review', distance=3., closest=True) will return a tuple of the closest pieces of data in the tree that are also within edit distance 3 from review (e.g. view, reviewer, reviews, etc.), its distance to review, and its index; if there is nothing within that edit distance, an empty list will be returned\nfuzzy_find('review', distance=np.inf, closest=True) will return a tuple of the closest pieces of data in the tree to review (e.g. view, reviewer, reviews, etc.), regardless of edit distance, its distance to review, and its index; this will always return something\nfuzzy_find('review', distance=np.inf, closest=False) will return a tuple of every piece of data in the tree to review (e.g. view, reviewer, reviews, etc.), regardless of edit distance, its distance to review, and its index; this will always return a list with as many elements as there are nodes in the tree\n\nThis method should also support only searching the terminal nodes (leaves) of the tree with the flag terminals_only.\nHint: you should look back at the methods we defined for searching and indexing the tree above. Specifically, to understand why you might want something like index_path defaulting to the empty tuple, look at the index method of TreeOld.\n\nFuzzyFindResult = tuple[tuple, str | list[str], float]\n\nclass Tree(TreeOld):\n    \n    DIST = EditDistance(1., 1.)\n    \n    def fuzzy_find(self, data: Union[str, List[str]], \n                   closest: bool = True, \n                   distance: float = np.inf,\n                   case_fold: bool = True,\n                   terminals_only: bool = True,\n                   index_path: tuple = tuple()) -&gt; list[FuzzyFindResult]:\n        \n        '''Find the (closest) strings within a certain distance\n        \n        Defaults to computing the closest strings among the terminals and ignoring case\n        \n        The format of the things returned is [((0,1,0), \"view\", 2.0), ...]. Note that \n        edit distance can be computed on either a `str` or `List[str]`; that's why\n        the middle element of each tuple might be either.\n        \n        Parameters\n        ----------\n        data\n            the data to match against\n        closest\n            whether to return only the closest strings or all strings within distance\n        distance\n            the distance within which a string must be\n        case_fold\n            whether to lower-case the data\n        terminals_only\n            whether to only search the terminals\n        index_path\n        '''\n        raise NotImplementedError\n\nWrite tests that use the following tree as input data.\n\ntreestr = '( (SBARQ (WHNP-1 (WP What)) (SQ (NP-SBJ-1 (-NONE- *T*)) (VP (VBZ is) (NP-PRD (NP (DT the) (JJS best) (NN place)) (SBAR (WHADVP-2 (-NONE- *0*)) (S (NP-SBJ (-NONE- *PRO*)) (VP (TO to) (VP (VB get) (NP (NP (NNS discounts)) (PP (IN for) (NP (NML (NNP San) (NNP Francisco)) (NNS restaurants)))) (ADVP-LOC-2 (-NONE- *T*))))))))) (. ?)) )'\n\ntesttree = Tree.from_string(treestr)\n\ntesttree\n\nSBARQ\n--WHNP-1\n  --WP\n    --What\n--SQ\n  --NP-SBJ-1\n    ---NONE-\n      --*T*\n  --VP\n    --VBZ\n      --is\n    --NP-PRD\n      --NP\n        --DT\n          --the\n        --JJS\n          --best\n        --NN\n          --place\n      --SBAR\n        --WHADVP-2\n          ---NONE-\n            --*0*\n        --S\n          --NP-SBJ\n            ---NONE-\n              --*PRO*\n          --VP\n            --TO\n              --to\n            --VP\n              --VB\n                --get\n              --NP\n                --NP\n                  --NNS\n                    --discounts\n                --PP\n                  --IN\n                    --for\n                  --NP\n                    --NML\n                      --NNP\n                        --San\n                      --NNP\n                        --Francisco\n                    --NNS\n                      --restaurants\n              --ADVP-LOC-2\n                ---NONE-\n                  --*T*\n--.\n  --?\n\n\nThe tests should test the four combinations of distance and closest listed above with the same query data, both with and without terminals_only=True and terminals_only=False (eight tests in total). Two further tests should test distance=np.inf, closest=True, terminals_only=True for a case where only a single element should be returned and a case where multiple elements in the tree should be returned.\n\n# write tests here\n\nRemember that we talked in class about what it would mean to take the distance between a string and a collection of strings: basically, the minimum of the edit distances between the string and each string in that set.\nWe can use this concept in two ways here. The first is to view the tree as a container for some data and to compute the minimum distance between a query and any data contained in the tree. Alternatively, we can think of the query itself as determining a set and compute the minimum distance of each piece of data in the tree to that set. Task 2 will implement the former and Task 3 the latter.\nI’ve copied the corpus reader we developed for the English Web Treebank in class below. We’ll make use of this for Task 2. (You’ll need to grab LDC2012T13.tgz from the course Google drive.)\n\n!wget --no-check-certificate 'https://drive.google.com/uc?export=download&id=1ygMIl1w6wz6A24oxkLwirunSKXb9EW12' -O 'LDC2012T13.tgz'\n\n\nimport tarfile\nfrom collections import defaultdict\n\nclass EnglishWebTreebankOld:\n    \n    def __init__(self, root='LDC2012T13.tgz'):\n        \n        def trees():\n            with tarfile.open(root) as corpus:\n                for fname in corpus.getnames():\n                    if '.xml.tree' in fname:\n                        with corpus.extractfile(fname) as treefile:\n                            treestr = treefile.readline().decode()\n                            yield fname, Tree.from_string(treestr)\n                        \n        self._trees = trees()\n                        \n    def items(self):\n        for fn, tlist in self._trees:\n              yield fn, tlist"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-2",
    "href": "assignments/assignments-3-and-4.html#task-2",
    "title": "Assignments 3 and 4",
    "section": "Task 2",
    "text": "Task 2\nLines: 3\nDefine an instance method fuzzy_find for the corpus reader class that computes the minimum distance between a query and a tree for all trees in the corpus. It should return a list of tuples with the first element a tree ID, the second an index in that tree, the third the data at that index and the fourth the distance between the query and that index. A tuple should be included in the returned list only if the distance is equal to the minimum across trees in the corpus.\nHint: this should be very straightforward using a particular parameterization for Tree1.fuzzy_find. Which one?\n\nclass EnglishWebTreebank(EnglishWebTreebankOld):\n    \n    def fuzzy_find(self, data: str | list[str]) -&gt; list[FuzzyFindResult]:\n        '''Find the trees in the corpus closest to the query data\n        \n        Parameters\n        ----------\n        data\n        '''\n        raise NotImplementedError\n\nNow, load this corpus.\n\newt = EnglishWebTreebank()\n\nWrite a single test for a piece of data you know exists in some tree in the corpus. (Determiners or auxiliary verbs are good candidates.) Thus, the minimum distance will be zero and your method should return only trees that contain that element. Note that this test should use some already existing method to produce the correct set of trees.\nHint: such a method already exists in the TreeOld class.\n\n# write test here\n\nThe next task will look at computing distance between the elements of a tree and a query set defined by a regular expression. Here is a regular expression class based on the formal definition of regular expressions I gave you in class.\n\nfrom itertools import product\n\nclass Regex:\n    \"\"\"A regular expression\n    \n    Parameters\n    ----------\n    regex_parsed\n    maxlength\n    \"\"\"\n\n    CHAR = pyparsing.Word(pyparsing.alphas, exact=1).setName(\"character\") # &lt;- use 'exact', not 'max'\n\n    LPAR = pyparsing.Suppress('(')\n    RPAR = pyparsing.Suppress(')')\n\n    PARSER = pyparsing.Forward()\n    GROUP = pyparsing.Group(LPAR + PARSER + RPAR)\n    QUANT = pyparsing.oneOf(\"* ? +\")\n    DSJ = '|'\n\n    ITEM = pyparsing.Group((CHAR | GROUP) + QUANT) | pyparsing.Group(CHAR + DSJ + CHAR) | CHAR | GROUP\n    ITEMSEQ = pyparsing.OneOrMore(ITEM)\n\n    PARSER &lt;&lt;= pyparsing.delimitedList(ITEMSEQ, pyparsing.Empty())\n    \n    def __init__(self, regex_parsed: List[Union[str, List]], maxlength: int):\n        self._regex_parsed = regex_parsed\n        self._maxlength = maxlength\n    \n    @classmethod\n    def from_string(cls, regexstr: str, maxlength: int = 30):\n        if regexstr[0] != '(':\n            regexstr = '(' + regexstr\n            \n        if regexstr[-1] != ')':\n            regexstr = regexstr +')'\n            \n        regex_parsed = cls.PARSER.parseString(regexstr)[0]\n        \n        return cls(regex_parsed, maxlength)\n    \n    def __iter__(self):\n        self._gen = self._construct_regex_generator()\n        return self\n    \n    def __next__(self):\n        return next(self._gen)\n        \n    def _construct_regex_generator(self, regex=None):\n        if regex is None:\n            regex = self._regex_parsed\n        \n        if isinstance(regex, str):\n            if len(regex) &gt; self._maxlength:\n                raise StopIteration\n\n            yield regex\n        \n        elif regex[1] in ['*', '+']:\n            i = 0 if regex[1] == '*' else 1\n            while True:\n                for s in self._construct_regex_generator(regex[0]):\n                    yield s*i\n\n                i += 1\n                \n                if i &gt; self._maxlength:\n                    break\n                    \n        elif regex[1] == '?':\n            yield ''\n            yield regex[0]\n\n        elif regex[1] == '|':\n            left = self._construct_regex_generator(regex[0])\n            right = self._construct_regex_generator(regex[2])\n            \n            s1 = s2 = ''\n            \n            while True:\n                if len(s1) &lt;= self._maxlength:                \n                    s1 = next(left)\n                    yield s1\n                \n                if len(s2) &lt;= self._maxlength:\n                    s2 = next(right)\n                    yield s2\n\n                if len(s1) &gt; self._maxlength and len(s2) &gt; self._maxlength:\n                    break\n        \n        else:\n            evaluated = [self._construct_regex_generator(r) for r in regex]\n            for p in product(*evaluated):\n                c = ''.join(p)\n                \n                if len(c) &lt;= self._maxlength:\n                    yield c\n\nThe way to use this class to generate the set of strings associated with a regular expression is tree an instance of the Regex class as a generator.\nImportantly, I’ve include a way of only generating strings of less than some length threshold maxlength in the case that your regular expression evaluates to an infinite set.\n\nfor s in Regex.from_string('co+lou?r', 20):\n    print(s)"
  },
  {
    "objectID": "assignments/assignments-3-and-4.html#task-3",
    "href": "assignments/assignments-3-and-4.html#task-3",
    "title": "Assignments 3 and 4",
    "section": "Task 3",
    "text": "Task 3\nLines: 15\nDefine a new version of fuzzy_find that behaves exactly the same as your version from Task 1 except that it allows the query data to be a regular expression parsable by Regex.from_string. Make sure that you correctly handle infinite sets.\nHint: your new fuzzy_find will be nearly identical to the old one. My implementation only has a single additional line.\n\nclass Tree(TreeOld):\n    \n    DIST = EditDistance(1., 1., 1.)\n    \n    def fuzzy_find(self, data: str | list[str], \n                   closest: bool = True, \n                   distance: float = np.inf,\n                   case_fold: bool = True,\n                   terminals_only: bool = True,\n                   index_path: tuple[int] = tuple()) -&gt; list[FuzzyFindResult]:\n        \n        '''Find the (closest) strings within a distance of the set defined by a regex\n        \n        Defaults to computing the closest strings among the terminals and ignoring case\n        \n        Parameters\n        ----------\n        data\n            the regex to match against\n        closest\n            whether to return only the closest strings or all strings within distance\n        distance\n            the distance within which a string must be\n        case_fold\n            whether to lower-case the data\n        terminals_only\n            whether to only search the terminals\n        index_path\n        '''\n        raise NotImplementedError\n\nWrite tests analogous to the ones you wrote for Task 1.\n\n# write tests here"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html",
    "title": "Finding data with tree pattern matching",
    "section": "",
    "text": "Definition of Tree up to this point\nfrom typing import TypeVar\n\nDataType = TypeVar(\"DataType\")\n\nclass Tree:\n    \"\"\"A tree\n    \n    Parameters\n    ----------\n    data\n        The data contained in this tree\n    children\n        The subtrees of this tree\n    \"\"\"\n    def __init__(self, data: DataType, children: list['Tree']=[]):\n        self._data = data\n        self._children = children\n        \n        self._validate()\n        \n    def _validate(self) -&gt; None:\n        try:\n            assert all(isinstance(c, Tree)\n                       for c in self._children)\n        except AssertionError:\n            msg = 'all children must be trees'\n            raise TypeError(msg)\n        \n    @property\n    def data(self) -&gt; DataType:\n        return self._data \n    \n    @property\n    def children(self) -&gt; list['Tree']:\n        return self._children\n\n    def __str__(self):\n        if self._children:\n            return ' '.join(c.__str__() for c in self._children)\n        else:\n            return str(self._data)\n        \n    def __repr__(self):\n        return self.to_string(0)\n     \n    def to_string(self, depth: int) -&gt; str:\n        s = (depth - 1) * '  ' +\\\n            int(depth &gt; 0) * '--' +\\\n            self._data + '\\n'\n        s += ''.join(c.to_string(depth+1)\n                     for c in self._children)\n        \n        return s\n\n    def __contains__(self, data: DataType) -&gt; bool:\n        # pre-order depth-first search\n        if self._data == data:\n            return True\n        else:\n            for child in self._children:\n                if data in child:\n                    return True\n                \n            return False\n        \n    def __getitem__(self, idx: tuple[int]) -&gt; 'Tree':\n        idx = (idx,) if isinstance(idx, int) else idx\n        \n        try:\n            assert all(isinstance(i, int) for i in idx)\n            assert all(i &gt;= 0 for i in idx)\n        except AssertionError:\n            errmsg = 'index must be a positive int or tuple of positive ints'\n            raise IndexError(errmsg)\n        \n        if not idx:\n            return self\n        elif len(idx) == 1:\n            return self._children[idx[0]]\n        else:\n            return self._children[idx[0]][idx[1:]]\nWe can get from indices to trees, but how would we go from data to indices? Similar to a list, we can implement an index() method.\nclass Tree(Tree):\n     \n    def index(self, data, index_path=tuple()):\n        indices = [index_path] if self._data==data else []\n        root_path = [] if index_path == -1 else index_path\n        \n        indices += [j \n                    for i, c in enumerate(self._children) \n                    for j in c.index(data, root_path+(i,))]\n\n        return indices\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\ndeterminer_indices = tree1.index('D')\n\ndeterminer_indices\n\n[(0, 0), (1, 1, 0)]\ntree1[determiner_indices[0]]\n\nD\n--a\ntree1[determiner_indices[1]]\n\nD\n--the"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#searching-on-tree-patterns",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#searching-on-tree-patterns",
    "title": "Finding data with tree pattern matching",
    "section": "Searching on tree patterns",
    "text": "Searching on tree patterns\nWhat if instead we wanted to find where a piece of data was based on an entire tree pattern?\n\ntree_pattern = Tree('S', \n                    [Tree('NP',\n                          [Tree('D', \n                                [Tree('the')])]),\n                     Tree('VP')])\n\ntree_pattern\n\nS\n--NP\n  --D\n    --the\n--VP\n\n\nWe could implement a find() method.\n\nclass Tree(Tree):\n    \n    def find(self, pattern: 'Tree', \n             subtree_idx: tuple=tuple()) -&gt; list[tuple]:\n        '''The subtrees matching the pattern\n        \n        Parameters\n        ----------\n        pattern\n            the tree pattern to match against\n        subtree_idx\n            the index of the subtree within the tree pattern to return\n            defaults to the entire match\n        '''\n        \n        #raise NotImplementedError\n        \n        match_indices = [i + subtree_idx\n                         for i in self.index(pattern.data) \n                         if self[i].match(pattern)]\n            \n        return match_indices\n   \n    def match(self, pattern: 'Tree') -&gt; bool:\n        if self._data != pattern.data:\n            return False\n        \n        for child1, child2 in zip(self._children, pattern.children):\n            if not child1.match(child2):\n                return False\n                \n        return True\n\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree2 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree3 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree4 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree2.find(tree_pattern, (0,0))\n\n[(0, 0)]\n\n\n\ntree_pattern = Tree('VP', \n                    [Tree('V'),\n                     Tree('NP', \n                          [Tree('D', \n                                [Tree('the')])])])\n\ntree_pattern\n\nVP\n--V\n--NP\n  --D\n    --the\n\n\n\ntree1.find(tree_pattern, subtree_idx=(1,))\n\n[]\n\n\n\ntree2.find(tree_pattern, subtree_idx=(1,))\n\n[]\n\n\n\ntree3.find(tree_pattern, subtree_idx=(1,))\n\n[(1, 1)]\n\n\n\ntree4.find(tree_pattern, subtree_idx=(1,))\n\n[(1, 1)]\n\n\nThis sort of treelet-based matching is somewhat weak as it stands. What if we wanted:\n\n…nodes to be allowed to have some value from a set?\n…arbitrary distance between the nodes we are matching on?\n…arbitrary boolean conditions on node matches?"
  },
  {
    "objectID": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#expanding-pattern-based-search-with-sparql",
    "href": "formal-and-practical-preliminaries/working-with-annotated-corpora/finding-data-with-tree-pattern-matching.html#expanding-pattern-based-search-with-sparql",
    "title": "Finding data with tree pattern matching",
    "section": "Expanding pattern-based search with SPARQL",
    "text": "Expanding pattern-based search with SPARQL\nTo handle this, we need both a domain-specific language (DSL) for specifying such queries and an interpeter for that language. We can use SPARQL for our DSL. To intepret SPARQL, we will use the existing interpreter in rdflib.\nTo use rdflib’s interpreter, we need to map our Tree objects into an in-memory format for which a SPARQL interpreter is already implemented. We will use Resource Description Format as implemented in rdflib.\n\nfrom rdflib import Graph, URIRef\n\nclass Tree(Tree):\n    \n    RDF_TYPES = {}\n    RDF_EDGES = {'is': URIRef('is-a'),\n                 'parent': URIRef('is-the-parent-of'),\n                 'child': URIRef('is-a-child-of'),\n                 'sister': URIRef('is-a-sister-of')}\n            \n    def to_rdf(self, graph=None, nodes={}, idx=tuple()) -&gt; Graph: \n        graph = Graph() if graph is None else graph\n        \n        idxstr = '_'.join(str(i) for i in idx)\n        nodes[idx] = URIRef(idxstr)\n            \n        if self._data not in Tree.RDF_TYPES:\n            Tree.RDF_TYPES[self._data] = URIRef(self._data)\n\n        typetriple = (nodes[idx], \n                      Tree.RDF_EDGES['is'],\n                      Tree.RDF_TYPES[self.data])\n\n        graph.add(typetriple)\n\n        for i, child in enumerate(self._children):\n            childidx = idx+(i,)\n            child.to_rdf(graph, nodes, childidx)\n                \n            partriple = (nodes[idx], \n                         Tree.RDF_EDGES['parent'],\n                         nodes[childidx])\n            chitriple = (nodes[childidx], \n                         Tree.RDF_EDGES['child'],\n                         nodes[idx])\n            \n            graph.add(partriple)\n            graph.add(chitriple)\n            \n        for i, child1 in enumerate(self._children):\n            for j, child2 in enumerate(self._children):\n                child1idx = idx+(i,)\n                child2idx = idx+(j,)\n                sistriple = (nodes[child1idx], \n                             Tree.RDF_EDGES['sister'],\n                             nodes[child2idx])\n                \n                graph.add(sistriple)\n        \n        self._rdf_nodes = nodes\n        \n        return graph\n    \n    @property\n    def rdf(self) -&gt; Graph:\n        if not hasattr(self, \"_rdf\"):\n            self._rdf = self.to_rdf()\n\n        return self._rdf\n    \n    def find(self, query: str) -&gt; list[tuple[int]]:\n        return [tuple([int(i) \n                       for i in str(res[0]).split('_')]) \n                for res in self.rdf.query(query)]\n\n\ntree1 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree2 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('a')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree3 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('a')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\ntree4 = Tree('S', \n             [Tree('NP', \n                   [Tree('D', \n                         [Tree('the')]),\n                    Tree('N', \n                         [Tree('greyhound')])]),\n             Tree('VP', \n                   [Tree('V', \n                         [Tree('loves')]),\n                    Tree('NP',\n                         [Tree('D',\n                               [Tree('the')]),\n                          Tree('N',\n                               [Tree('greyhound')])])])])\n\n\ntree1.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;.\n                      ?node &lt;is-the-parent-of&gt;* ?child.\n                      ?node &lt;is-a-child-of&gt;* ?parent.\n                      ?parent &lt;is-a&gt; &lt;S&gt;.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?node &lt;is-a-sister-of&gt; ?sister.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[]\n\n\n\ntree2.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;.\n                      ?node &lt;is-the-parent-of&gt;* ?child.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?node &lt;is-a-sister-of&gt; ?sister.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[(0,)]\n\n\n\ntree2.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[(0,)]\n\n\n\ntree3.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;VP&gt;.\n                    }''')\n\n[]\n\n\n\ntree4.find('''SELECT ?node\n              WHERE { ?node &lt;is-a&gt; &lt;NP&gt;;\n                            &lt;is-the-parent-of&gt;* ?child;\n                            &lt;is-a-sister-of&gt; ?sister.\n                      ?child &lt;is-a&gt; &lt;the&gt;.\n                      ?sister &lt;is-a&gt; &lt;V&gt;.\n                    }''')\n\n[(1, 1)]"
  },
  {
    "objectID": "finite-state-models/index.html",
    "href": "finite-state-models/index.html",
    "title": "Overview",
    "section": "",
    "text": "In the last module, we mainly focused on setting up the foundations for analyzing languages as formal objects. We defined a language \\(L\\) on an alphabet \\(\\Sigma\\) as a subset of the set of strings \\(\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i\\) on \\(\\Sigma\\) (i.e. \\(L \\in 2^{\\Sigma^*}\\)). We explored one way of describing languages in the form of regular expressions on \\(\\Sigma\\) and we discussed one way of describing a relation between strings in the form of minimum edit distance.1"
  },
  {
    "objectID": "finite-state-models/index.html#footnotes",
    "href": "finite-state-models/index.html#footnotes",
    "title": "Overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact, we decribed minimum edit distance as a relation from tuples of strings to distances; but as we explored in Assignment 3, we can also think of a distance, in the context of a particular weighting on edit operations, as a relation on strings: namely, a relation on strings that are {at most, exactly, at least, …} that distance apart. This idea extends the notion of a ball or sphere, depending on the constraint, to strings.↩︎"
  },
  {
    "objectID": "finite-state-models/levels-of-abstraction.html",
    "href": "finite-state-models/levels-of-abstraction.html",
    "title": "Levels of Abstraction",
    "section": "",
    "text": "What we will now begin to do is to deploy these tools for describing and expressing generalizations about possible languages. To do this, we are going to need to start thinking in terms of sets of sets of languages. That is, we will begin to think about subsets \\(\\mathcal{L}\\) of the set of languages \\(2^{\\Sigma^*}\\) on \\(\\Sigma\\). This approach will yield four important levels of abstraction:\n\nPrimitive (unanalyzed) elements \\(\\sigma \\in \\Sigma\\)\nCollections of primitive elements: sets \\(\\Sigma\\) or sequences \\(\\boldsymbol\\sigma \\in \\Sigma^*\\)\nLanguages: collections of sequences of primitive elements \\(L \\in 2^{\\Sigma^*}\\)\nCollections of languages \\(\\mathcal{L} \\subseteq 2^{\\Sigma^*}\\)\n\nWe are going to be particularly interested in collections of languages that share some interesting properties. We will call such collections classes of languages (or families of languages), and we will be interested in ways of compactly describing those classes that leverage the property shared by languages in the class. We will call compact descriptions of a particular language grammars and collections thereof classes of grammars \\(\\mathcal{G}\\)."
  },
  {
    "objectID": "finite-state-models/generation.html",
    "href": "finite-state-models/generation.html",
    "title": "Generation",
    "section": "",
    "text": "We will characterize the relationship between grammars and languages as one of generation: a grammars \\(G\\) generates a language \\(L\\) if \\(G\\) is a description of that language. To express that \\(G\\) generates \\(L\\), we will say that \\(\\mathbb{L}(G) = L\\), where \\(\\mathbb{L}\\) is some function from grammars to languages. (It is important to note that generation sounds like a procedural concept, but it is really declarative. Know that \\(G\\) generates a language \\(L\\) does not require us to know how to build \\(L\\) from \\(G\\).)"
  },
  {
    "objectID": "finite-state-models/generation.html#example-regular-expressions",
    "href": "finite-state-models/generation.html#example-regular-expressions",
    "title": "Generation",
    "section": "Example: regular expressions",
    "text": "Example: regular expressions\nWe’ve already seen one kind of grammar under this definition: regulars expressions. Remember that regular expressions \\(R(\\Sigma)\\) on \\(\\Sigma\\) themselves are strings of a language on \\(\\Sigma \\cup\\{\\epsilon, \\emptyset, \\cup, \\circ, (, ), *\\}\\). We formally define these strings recursively.\n\\(\\rho\\) is a regular expression if and only if:\n\n\\(\\rho \\in \\Sigma \\cup \\{\\epsilon, \\emptyset\\}\\)\n\\(\\rho\\) is \\((\\rho_1 \\cup \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\((\\rho_1 \\circ \\rho_2)\\) for some regular expressions \\(\\rho_1\\) and \\(\\rho_2\\)\n\\(\\rho\\) is \\(\\rho_1^*\\) for some regular expression \\(\\rho_1\\)\n\nWe can generate all the regular expressions given an alphabet.\nRegular expressions (so defined) evaluate to sets of strings on \\(\\Sigma\\)–i.e. languages on \\(\\Sigma\\). Another way of thinking about this is that a regular expression on \\(\\Sigma\\) describes a language on \\(\\Sigma\\).\nWe can define this evaluation procedure formally as a function \\(\\text{eval}: R(\\Sigma) \\rightarrow 2^{\\Sigma^*}\\), where \\(R(\\Sigma)\\) is the set of regular expressions on \\(\\Sigma\\).\n\\(\\text{eval}(\\rho) = \\begin{cases}\\{\\} & \\text{if } \\rho = \\emptyset \\\\\\{\\_\\} & \\text{if } \\rho = \\epsilon \\\\ \\{\\rho\\} & \\text{if } \\rho \\in \\Sigma\\\\ \\text{eval}(\\rho_1) \\times \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\circ \\rho_2) \\\\ \\text{eval}(\\rho_1) \\cup \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\cup \\rho_2)\\\\ \\bigcup_{i = 0}^\\infty \\text{eval}(\\rho_1)^i & \\text{if } \\rho = \\rho_1^*\\\\ \\end{cases}\\)\nEach regular expression is thus a grammar; the set of all regular expressions is a class of grammars; and \\(\\text{eval}\\) is an implementation of \\(\\mathbb{L}\\). We will call the class of languages \\(\\mathcal{R}\\) generated by the regular expressions the regular languages: \\[\\mathcal{R} \\equiv \\mathbb{L}(R(\\Sigma)) = \\{\\mathbb{L}(r) \\mid r \\in R(\\Sigma)\\} = \\{\\text{eval}(r) \\mid r \\in R(\\Sigma)\\}\\]\nWe will be studying these languages in depth, because as I’ve mentiond before and as you will read about in Heinz 2018, it seems very likely that all phonological grammars of natural languages are a subset of this class. To study these languages, it will be useful to introduce a class of grammars that are equivalent to the regular expressions in that they generate exactly the same set of languages–i.e. the regular languages. These grammars are known as finite state automata.\nWe will refer to this sort of equivalence–i.e. that \\(\\mathbb{L}(\\mathcal{G}_1) = \\mathbb{L}(\\mathcal{G}_2)\\) for classes \\(\\mathcal{G}_1\\) and \\(\\mathcal{G}_2\\)–as weak equivalence or equivalence in weak generative capacity. We will discuss another notion–strong equivalence–later. First, though we should discuss the broader context."
  },
  {
    "objectID": "finite-state-models/the-generativist-conceit.html",
    "href": "finite-state-models/the-generativist-conceit.html",
    "title": "The Generativist Conceit",
    "section": "",
    "text": "Why would we want to do all this? In the first lecture, I discussed what I called the Generativist Conceit: that we can understand natural language by studying the class of grammars that generate all + only the classes of natural languages. Though certain strong ideas associated with the Generativist Conceit are controversial, as I have stated it (relatively weakly) here, it is relatively uncontroversial: if our aim is to provide a scientific theory that characterizes natural language (whatever that might be: a colection of actual utterances, an aspect of human psychology or some abstract object with an existence independent of human minds), we at least need some method for encoding that theory in a way that makes testable predictions. Grammars are definitionally such a method as I’ve described them.\nAnother relatively uncontroversial observation that drives the Generativist Conceit is that children must be able to start without a language and come to know one, and the one they come to know is determined in some important way from their environment (rather than their genetics) in a finite amount of time on the basis of a finite number of observations of actual language use. Why this observation is important is that it means that natural languages must be learnable from a finite number of examples. Grammars are usually finite objects that (may) describe countably infinite languages, and so they might be a useful tool for describing the process by which a child comes to know a language.\nMore formally, we may say that children have some method \\(c\\) for mapping sequences of strings (or texts) \\(\\mathbf{t} \\in \\Sigma^{**}\\) along with some extralinguistic experience \\(E \\in \\mathcal{E}\\) into a grammar \\(G \\in \\mathcal{G}\\): \\[c: \\Sigma^{**} \\times \\mathcal{E} \\rightarrow \\mathcal{G}\\]. A child’s extralinguistic experience is nontrivial to represent, so to make the problem more tractable, we will often ignore that part.\nTo demonstrate that the problem of learning a grammar from a text, consider the following formal model, known as language identification in the limit, due to Gold 1964.\nWe start from the idea that a learner should be modeled as a function \\(f: \\Sigma^{**} \\rightarrow 2^{\\Sigma^*}\\) that maps an environment \\(E = \\langle \\boldsymbol\\sigma_1, \\boldsymbol\\sigma_2, \\ldots \\rangle \\in \\Sigma^{**}\\) to a language \\(L\\), where an environment is a sequence of strings in \\(L\\). We will say that a learner \\(f\\) learns a language \\(L\\) given an environment (or text) \\(E \\subset L^*\\) if the learner outputs \\(L\\) after seeing enough examples from the environment and that it learns a language \\(L\\) if it learns that language in any environment. That is, \\(f\\) learns \\(L\\) if it doesn’t matter which sequence you give it: \\(f \\text{ learns } L \\leftrightarrow \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L\\). A language family \\(\\mathcal{L}\\) is learnable if there exists a language learner that can learn all languages in the family: \\(f \\text{ learns } \\mathcal{L} \\leftrightarrow \\forall L \\in \\mathcal{L}: \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L\\).\nGold (1967) showed that, if a language family \\(\\mathcal{L}\\) contains languages \\(L_1, L_2, \\ldots, L_\\infty\\), such that \\(L_1 \\subset L_2 \\subset \\ldots \\subset L_\\infty \\bigcup _{i=1}^\\infty L_i\\), then it is not learnable. Here’s the idea behind the proof: suppose \\(f\\) is a learner that can learn \\(L_1, L_2, \\ldots, L_\\infty\\); we’ll show that it cannot learn \\(L_\\infty\\), by constructing an environment for \\(L_\\infty\\) that “tricks” \\(f\\).\nFirst, we construct environments \\(E_1, E_2, \\ldots\\) such that \\(f(E_i) = L_i\\). Next, we construct environment \\(E_\\infty\\) for \\(L_\\infty\\) inductively as follows:\n\nPresent \\(f\\) with \\(E_1\\) until it outputs \\(L_{1}\\).\nSwitch to presenting \\(f\\) with an environment that alternates the rest of \\(E_1\\) and the entirety of \\(E_2\\). Since \\(L_1 \\subset L_2\\), this concatenated environment is still an environment for \\(L_2\\), so \\(f\\) must eventually output \\(L_2\\).\nSwitch to presenting the rest of \\(E_1 \\circ E_2\\) and the entirety of \\(E_3\\) alternatively. And so on for all \\(i\\).\n\nBy construction, the resulting environment \\(E\\) contains the entirety of \\(E_{1},E_{2},\\ldots\\), thus it contains \\(L_\\infty\\), so it is an environment for \\(L_\\infty\\). But since the learner always switches to \\(L_{i}\\) for some finite \\(i\\), it never converges to \\(L_{\\infty}\\).\nThe point here is that we need to be careful about the class of languages we pick out as possible ones the learner might select from. We’ll explore this idea in the context of finite state automata, which are equivalent to regular expressions. Importantly, the full class of finite state automata have the problematic containment structure above, but the hope is that if we constrain these grammars in particular ways, we can avoid these sorts of issues while retaining coverage of natural languages."
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html",
    "href": "finite-state-models/formal-definition/the-regular-operations.html",
    "title": "The regular operations",
    "section": "",
    "text": "Given NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{union}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A \\cup B = \\mathbb{L}(M) = \\mathbb{L}(\\text{union}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive and do not contain \\(q_0\\); \\(Q = Q'_1 \\cup Q'_2 \\cup \\{q_0\\}\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\{q_1, q_2\\} & \\text{if } q=q_0 \\land \\sigma=\\epsilon \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\\\\n\\delta_2(q, \\sigma) & \\text{if } q\\in Q_2 \\\\\n\\text{undefined} & \\text{otherwise} \\\\\n\\end{cases}\\]\nDefine \\(F = F_1 \\cup F_2\\)\n\n\nclass FiniteStateAutomaton:\n    pass\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def _deepcopy(self):\n        fsa = copy(self)\n        del fsa._generator\n\n        return deepcopy(fsa)\n        \n    def _relabel_fsas(self, other):\n        \"\"\"\n        append tag to the input/ouput states throughout two FSAs\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n        \"\"\"\n\n        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n\n        return fsa1, fsa2\n\n    def _relabel_states(self, tag):\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        state_map = {s: s+'_'+tag for s in self._states}\n        \n        self._states = {state_map[s] for s in self._states}    \n        self._initial_state += '_'+tag\n        self._final_states = {state_map[s] for s in self._final_states}\n\n        self._transition_function = self._transition_function\n        self._transition_function.relabel_states(state_map)\n        \n        return self\n\n    def union(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        union this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.union\"\n        raise NotImplementedError(msg)"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#union",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#union",
    "title": "The regular operations",
    "section": "",
    "text": "Given NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{union}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A \\cup B = \\mathbb{L}(M) = \\mathbb{L}(\\text{union}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive and do not contain \\(q_0\\); \\(Q = Q'_1 \\cup Q'_2 \\cup \\{q_0\\}\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\{q_1, q_2\\} & \\text{if } q=q_0 \\land \\sigma=\\epsilon \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\\\\n\\delta_2(q, \\sigma) & \\text{if } q\\in Q_2 \\\\\n\\text{undefined} & \\text{otherwise} \\\\\n\\end{cases}\\]\nDefine \\(F = F_1 \\cup F_2\\)\n\n\nclass FiniteStateAutomaton:\n    pass\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __or__(self, other):\n        return self.union(other)\n\n    def _deepcopy(self):\n        fsa = copy(self)\n        del fsa._generator\n\n        return deepcopy(fsa)\n        \n    def _relabel_fsas(self, other):\n        \"\"\"\n        append tag to the input/ouput states throughout two FSAs\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n        \"\"\"\n\n        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n\n        return fsa1, fsa2\n\n    def _relabel_states(self, tag):\n        \"\"\"\n        append tag to the input/ouput states throughout the FSA\n\n        Parameters\n        ----------\n        tag : str\n        \"\"\"\n\n        state_map = {s: s+'_'+tag for s in self._states}\n        \n        self._states = {state_map[s] for s in self._states}    \n        self._initial_state += '_'+tag\n        self._final_states = {state_map[s] for s in self._final_states}\n\n        self._transition_function = self._transition_function\n        self._transition_function.relabel_states(state_map)\n        \n        return self\n\n    def union(self, other: 'FiniteStateAutomaton') -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        union this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.union\"\n        raise NotImplementedError(msg)"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#concatenation",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#concatenation",
    "title": "The regular operations",
    "section": "Concatenation",
    "text": "Concatenation\nGiven NFAs \\(M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle\\) recognizing \\(A = \\mathbb{L}(M_1)\\) and \\(M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle\\) recognizing \\(B = \\mathbb{L}(M_2)\\), we construct \\(M = \\text{concatenate}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_1, F_2 \\rangle\\) recognizing \\(A \\circ B = \\mathbb{L}(M) = \\mathbb{L}(\\text{concatenate}(M_1, M_2))\\):\n\nRelabel \\(Q_1\\) and \\(Q_2\\) so they are mutually exclusive; \\(Q = Q'_1 \\cup Q'_2\\) and \\(\\Sigma = \\Sigma_1 \\cup \\Sigma_2\\)\nDefine \\[\\delta(q, \\sigma) = \\begin{cases}\n\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\land q\\not\\in F_1 \\\\\n\\delta_1(q, \\sigma) & \\text{if } q\\in F_1 \\land \\sigma \\neq \\epsilon \\\\\n\\delta_1(q, \\sigma) \\cup \\{q_2\\} & \\text{if } q\\in F_1 \\land \\sigma = \\epsilon \\\\\n\\delta_2(q, \\sigma)& \\text{if } q\\in Q_2 \\\\\n\\end{cases}\\]\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n        \n    def __add__(self, other):\n        return self.concatenate(other)\n\n    def __pow__(self, k):\n        return self.exponentiate(k)\n\n    def concatenate(self, other: 'FiniteStateAutomaton'):\n        \"\"\"\n        concatenate this FSA with another\n\n        Parameters\n        ----------\n        other : FiniteStateAutomaton\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        msg = \"you still need to implement FiniteStateAutomaton.concatenate\"\n        raise NotImplementedError(msg)\n\n    def exponentiate(self, k: int) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        concatenate this FSA k times\n\n        Parameters\n        ----------\n        k : int\n            the number of times to repeat; must be &gt;1\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        if k &lt;= 1:\n            raise ValueError(\"must be &gt;1\")\n\n        new = self\n\n        for i in range(1,k):\n            new += self\n\n        return new"
  },
  {
    "objectID": "finite-state-models/formal-definition/the-regular-operations.html#kleene-closure",
    "href": "finite-state-models/formal-definition/the-regular-operations.html#kleene-closure",
    "title": "The regular operations",
    "section": "Kleene Closure",
    "text": "Kleene Closure\nGiven an NFA \\(M = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) recognizing \\(A = \\mathbb{L}(M)\\), the NFA \\(N = \\text{kleene}(M) = \\langle Q \\cup \\{q'_0\\}, \\Sigma, \\delta', q'_0\\not\\in Q, F' \\rangle\\) recognizing \\(A^* = \\mathbb{L}(N) = \\mathbb{L}(\\text{kleene}(M))\\):\n\nDefine \\[\\delta'(q, \\sigma) = \\begin{cases}\nq_0 & \\text{if } (q = q'_0 \\lor q \\in F) \\land \\sigma = \\epsilon \\\\\n\\delta(q, \\sigma) & \\text{otherwise} \\\\\n\\end{cases}\\]\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def kleene_star(self)  -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        construct the kleene closure machine\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        fsa = self._deepcopy()\n        \n        new_transition = fsa._transition_function\n        new_transition.add_transitions({(s, ''): fsa._initial_state\n                                        for s in fsa._final_states})\n\n        return FiniteStateAutomaton(fsa._alphabet, fsa._states,\n                                    fsa._initial_state, fsa._final_states,\n                                    new_transition.transition_graph)"
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html",
    "title": "Two equivalent definitions",
    "section": "",
    "text": "A Deterministic Finite State Automaton (DFA) is a grammar with 5 components:\nA Nondeterministic Finite State Automaton (NFA) is also a grammar with 5 components:\nfrom copy import copy, deepcopy\nfrom typing import Union, Optional, Dict, List\nfrom functools import lru_cache\n\nclass TransitionFunction:\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def __init__(self, transition_graph: dict[tuple[str, str], set[str]]):\n        self._transition_graph = transition_graph\n\n    def __call__(self, state: str, symbol: str) -&gt; set[str]:\n        try:\n            return self._transition_graph[(state, symbol)]\n        except KeyError:\n            return set({})\n\n    def __or__(self, other):\n        return TransitionFunction(dict(self._transition_graph, \n                                       **other._transition_graph))\n        \n    def add_transitions(self, transition_graph):\n        self._transition_graph.update(transition_graph)\n\n    def validate(self, alphabet, states):\n        self._validate_input_values(alphabet, states)\n        self._validate_output_types()\n        self._homogenize_output_types()\n        self._validate_output_values(states)\n\n    def _validate_input_values(self, alphabet, states):\n        for state, symbol in self._transition_graph.keys():\n            try:\n                assert symbol in alphabet\n\n            except AssertionError:\n                msg = 'all input symbols in transition function ' +\\\n                      'must be in alphabet'\n                raise ValueError(msg)\n\n            try:\n                assert state in states\n\n            except AssertionError:\n                msg = 'all input states in transition function ' +\\\n                      'must be in set of states'\n                raise ValueError(msg)\n\n    def _validate_output_types(self):\n        for states in self._transition_graph.values():\n            try:\n                t = type(states)\n                assert t is str or t is set\n\n            except AssertionError:\n                msg = 'all outputs in transition function' +\\\n                      'must be specified via str or set'\n                raise ValueError(msg)            \n\n    def _homogenize_output_types(self):\n        outputs = self._transition_graph.values()\n\n        for inp, out in self._transition_graph.items():\n            if type(out) is str:\n                self._transition_graph[inp] = {out}\n\n    def _validate_output_values(self, states):\n        for out in self._transition_graph.values():\n            try:\n                assert all([state in states for state in out])\n            except AssertionError:\n                msg = 'all output symbols in transition function' +\\\n                      'must be in states'\n                raise ValueError(msg)\n\n    @property\n    def transition_graph(self):\n        return self._transition_graph\n\nclass FiniteStateAutomaton:\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    def __init__(self, alphabet: set[str], states: set[str], \n                 initial_state: str, final_states: set[str], \n                 transition_graph: 'TransitionFunction'):\n        self._alphabet = {''} | alphabet\n        self._states = states\n        self._initial_state = initial_state\n        self._final_states = final_states\n        self._transition_function = TransitionFunction(transition_graph)\n\n        self._validate_initial_state()\n        self._validate_final_states()\n        self._transition_function.validate(self._alphabet, states)\n\n        self._generator = self._build_generator()\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        return next(self._generator)\n\n    def _build_generator(self):\n        string_buffer = [(self._initial_state, '')]\n        \n        if self._initial_state in self._final_states:\n            stack = ['']\n        else:\n            stack = []\n\n        while string_buffer:\n            if stack:\n                yield stack.pop()\n\n            else:\n                # this is very inefficient when we have total\n                # transition functions with many transitions to the\n                # sink state; could be optimized by removing a string\n                # if it's looping in a sink state, but we don't know\n                # in general what the sink state is\n                new_buffer = []\n                for symb in self._alphabet:\n                    for old_state, string in string_buffer:\n                        new_states = self._transition_function(old_state, symb)\n                        for st in new_states:\n                            new_elem = (st, string+symb)\n                            new_buffer.append(new_elem)\n\n                stack += [string\n                          for state, string in new_buffer\n                          if state in self._final_states]\n\n                string_buffer = new_buffer\nWe often define (and draw) FSAs s.t. their transition functions are partial. We can assume all FSA transition functions \\(\\delta\\) are in fact total by adding a sink state \\(q_\\text{sink} \\not\\in F\\) to the FSA and mapping all \\(\\langle q, \\sigma \\rangle\\) pairs for which \\(\\delta\\) is undefined to \\(q_\\text{sink}\\).\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def istotalfunction(self, states, alphabet):\n        return all((s, a) in self._transition_graph\n                    for s in states\n                    for a in alphabet)\n\n    def totalize(self, states, alphabet):\n        if not self.istotalfunction(states, alphabet):\n            domain = {(s, a) for s in states for a in alphabet}\n\n            sink_state = 'qsink'\n\n            while sink_state in states:\n                sink_state += 'sink'\n\n            for inp in domain:\n                if inp not in self._transition_graph:\n                    self._transition_graph[inp] = {sink_state}\nIt turns out that these two ways of defining FSAs are at least weakly equivalent. That is, the class of languages generated by NFAs is the same as the class generated by DFAs; therefore, both generate exactly the regular languages."
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html#dfas-are-strongly-equivalent-to-nfas",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html#dfas-are-strongly-equivalent-to-nfas",
    "title": "Two equivalent definitions",
    "section": "DFAs are (strongly) equivalent to NFAs",
    "text": "DFAs are (strongly) equivalent to NFAs\nEvery DFA can be converted to a strongly equivalent NFA. A DFA \\(G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) can be converted to an NFA \\(G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle\\) by defining the \\(G'\\) transition function \\(\\delta'(q,\\sigma) = \\{\\delta(q,\\sigma)\\}\\)\n\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    @property\n    def isdeterministic(self):\n        return all(len(v) &lt; 2 for v in self._transition_graph.values())\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n\n    @property\n    def isdeterministic(self) -&gt; bool:\n        return self._transition_function.isdeterministic"
  },
  {
    "objectID": "finite-state-models/formal-definition/two-equivalent-definitions.html#nfas-are-at-least-weakly-equivalent-to-dfas",
    "href": "finite-state-models/formal-definition/two-equivalent-definitions.html#nfas-are-at-least-weakly-equivalent-to-dfas",
    "title": "Two equivalent definitions",
    "section": "NFAs are (at least weakly) equivalent to DFAs",
    "text": "NFAs are (at least weakly) equivalent to DFAs\nEvery NFA can be converted to a weakly equivalent DFA\nThe \\(\\epsilon\\)-closure of \\(\\delta\\) is \\[\\delta_\\epsilon(q, \\sigma) =\n\\begin{cases}\n\\emptyset & \\text{if }  \\delta(q, \\sigma) \\text{ is undefined}\\\\\n\\delta(q, \\sigma) \\cup \\bigcup_{r \\in \\delta(q, \\sigma)} \\delta_\\epsilon(r, \\epsilon) & \\text{otherwise} \\\\\n\\end{cases}\\]\nAn NFA \\(G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle\\) can be converted to a DFA \\(G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle\\) by defining:\n\n\\(Q' = \\mathcal{P}(Q)\\)\n\\(\\delta'(R, \\sigma) = \\bigcup_{r\\in R}\\delta_\\epsilon(r, \\sigma)\\)\n\\(q'_0 = \\{q_0\\} \\cup \\delta_\\epsilon(q_0, \\epsilon)\\)\n\\(F' = \\{R\\;|\\; \\exists r \\in R: r \\in F\\}\\)\n\n\nfrom functools import reduce\nfrom itertools import chain, combinations\n\ndef powerset(iterable):\n    \"\"\"https://docs.python.org/3/library/itertools.html#recipes\"\"\"\n    s = list(iterable)\n    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n\ndef transitive_closure(edges: set[tuple[str]]) -&gt; set[tuple[str]]:\n    \"\"\"\n    the transitive closure of a graph\n\n    Parameters\n    ----------\n    edges\n        the graph to compute the closure of\n\n    Returns\n    ----------\n    set(tuple)\n    \"\"\"\n    while True:\n        new_edges = {(x, w)\n                     for x, y in edges\n                     for q, w in edges\n                     if q == y}\n\n        all_edges = edges | new_edges\n\n        if all_edges == edges:\n            return edges\n\n        edges = all_edges\n\nclass TransitionFunction(TransitionFunction):\n    \"\"\"A finite state machine transition function\n\n    Parameters\n    ----------\n    transition_graph\n\n    Attributes\n    ----------\n    isdeterministic\n    istotalfunction\n    transition_graph\n\n    Methods\n    -------\n    validate(alphabet, states)\n    relable_states(state_map)\n    totalize()\n    \"\"\"\n\n    def validate(self, alphabet, states):\n        self._validate_input_values(alphabet, states)\n        self._validate_output_types()\n        self._homogenize_output_types()\n        self._validate_output_values(states)\n\n        self._add_epsilon_transitive_closure()\n\n    def _add_epsilon_transitive_closure(self):\n        # get the state graph of all epsilon transitions\n        transitions = {(instate, outstate)\n                       for (instate, insymb), outs in self._transition_graph.items()\n                       for outstate in outs if not insymb}\n        \n        # compute the transitive closure of the epsilon transition\n        # state graph; requires homogenization beforehand\n        for instate, outstate in transitive_closure(transitions):\n            self._transition_graph[(instate, '')] |= {outstate}\n\n        new_graph = dict(self._transition_graph)\n\n        # add alphabet transitions from all states q_i that exit\n        # with an alphabet transition (q_i, a) and enter states q_j\n        # with epsilon transitions to states q_k\n        for (instate1, insymb1), outs1 in self._transition_graph.items():\n            for (instate2, insymb2), outs2 in self._transition_graph.items():\n                if instate2 in outs1 and not insymb2:\n                    # vacuously adds the already added epsilon\n                    # transitions as well\n                    try:\n                        new_graph[(instate1,insymb1)] |= outs2\n                    except KeyError:\n                        new_graph[(instate1,insymb1)] = outs2\n\n        self._transition_graph = new_graph\n\n    def determinize(self, initial_state):\n        new_initial_state = {initial_state} | self(initial_state, \"\")\n        new_transition_graph = {(instate, insymb): outstates\n                                for (instate, insymb), outstates \n                                in self._transition_graph.items()\n                                if insymb}\n        \n        return new_initial_state, TransitionFunction(new_transition_graph)\n\n\nclass FiniteStateAutomaton(FiniteStateAutomaton):\n    \"\"\"A finite state automaton\n\n    Parameters\n    ----------\n    alphabet\n    states\n    initial_state\n    final states\n    transition_graph\n    \"\"\"\n        \n    def determinize(self) -&gt; 'FiniteStateAutomaton':\n        \"\"\"\n        if nondeterministic, determinize the FSA\n\n        Returns\n        -------\n        FiniteStateAutomaton\n        \"\"\"\n        new_states = {'_'.join(sorted(s)) for s in powerset(self._states) if s}\n        # doesn't handle epsilons coming from the start state\n        new_init, new_trans = self._transition_function.determinize(self._initial_state)\n        new_final = {'_'.join(sorted(s)) for s in powerset(self._states)\n                     if any([t in s for t in self._final_states])\n                     if s}\n        \n        new_transition = {('_'.join(sorted(s)), a): {'_'.join(sorted(t))}\n                          for s in powerset(self._states)\n                          for t in powerset(self._states)\n                          for a in self._alphabet\n                          if any([tprime in new_trans(sprime,a)\n                                  for sprime in s for tprime in t])\n                          if s and t}\n\n        return FiniteStateAutomaton(self._alphabet-{''}, new_states,\n                                    '_'.join(sorted(new_init)), new_final,\n                                    new_transition)\n\n    @property\n    def isdeterministic(self) -&gt; bool:\n        return self._transition_function.isdeterministic"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "About",
    "section": "",
    "text": "This site contains materials for Introduction to Computational Linguistics–a course given by Aaron Steven White at the University of Rochester."
  },
  {
    "objectID": "index.html#about-the-course",
    "href": "index.html#about-the-course",
    "title": "About",
    "section": "About the course",
    "text": "About the course\nThis course covers foundational concepts in computational linguistics. Major focus is placed on the use of formal languages as a tool for understanding natural language as well as on developing students’ ability to implement foundational algorithms pertaining to those formal languages. Topics include basic formal language theory, finite state phonological and morphological parsing, and syntactic parsing for context free grammars and mildly context sensitive formalisms."
  },
  {
    "objectID": "index.html#prerequisites",
    "href": "index.html#prerequisites",
    "title": "About",
    "section": "Prerequisites",
    "text": "Prerequisites\nThis course relies on concepts covered in an introductory linguistics course and an introductory programming course. With respect to the latter, it specifically assumes that you can competently write scripts that do non-trivial things and can work competently with Python’s object-oriented programming facilities but maybe not develop a package on your own."
  },
  {
    "objectID": "index.html#about-the-instructor",
    "href": "index.html#about-the-instructor",
    "title": "About",
    "section": "About the instructor",
    "text": "About the instructor\nAaron Steven White is an Associate Professor of Linguistics and Computer Science at the University of Rochester, where he directs the Formal and Computational Semantics lab (FACTS.lab). His research investigates the relationship between linguistic expressions and conceptual categories that undergird the human ability to convey information about possible past, present, and future configurations of things in the world.\nIn addition to being a principal investigator on numerous federally funded grants and contracts, White is the recipient of a National Science Foundation Faculty Early Career Development (CAREER) award. His work has appeared in a variety linguistics, cognitive science, and natural language processing venues, including Semantics & Pragmatics, Glossa, Language Acquisition, Cognitive Science, Cognitive Psychology, Transactions of the Association for Computational Linguistics, and Empirical Methods in Natural Language Processing."
  },
  {
    "objectID": "index.html#about-the-site",
    "href": "index.html#about-the-site",
    "title": "About",
    "section": "About the site",
    "text": "About the site\nThe site itself is built using Quarto. The source files for this site are available on github at aaronstevenwhite/intro-to-cl. See Installation for information on how to run the code documented here."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "About",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThe development of these materials was supported by the University of Rochester and a National Science Foundation grant: CAREER: Logical Form Induction (BCS/IIS-2237175)."
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "About",
    "section": "License ",
    "text": "License \nIntroduction to Computational Linguistics by Aaron Steven White is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License. Based on a work at https://github.com/aaronstevenwhite/intro-to-cl."
  }
]