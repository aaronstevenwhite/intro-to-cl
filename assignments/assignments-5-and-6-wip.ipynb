{
 "cells": [
  {
   "cell_type": "raw",
   "id": "9b7d51ee",
   "metadata": {},
   "source": [
    "---\n",
    "title: Assignments 5 and 6\n",
    "highlight-style: oblivion\n",
    "format:\n",
    "    html:\n",
    "        code-tools: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VNf-2eQ0E632",
   "metadata": {
    "id": "VNf-2eQ0E632"
   },
   "source": [
    "*Like Assignments 1-4, Assignments 5 and 6 are bundled together. You only need to do Tasks 1 and 2 for Assignment 5 and Task 3 for Assignment 6. **There is additionally a Task 4 that you are not required to even attempt because of its difficulty.** I have left that task in in case you'd like to take a crack at it. Whether or not you actually attempt Task 4, please at least read through the entire notebook to see what the task entails and how you might handle it.*\n",
    "\n",
    "These assignments focus on implementing a natural language inference system. In natural language inference, we receive a _premise_ sentence and a _hypothesis_ sentence and we must say whether we can infer the premise from the hypothesis. For instance, if (1) were our premise and (2) were our hypothesis, our system should respond _yes_.\n",
    "\n",
    "1. Every firm polled saw costs grow more than expected, even after adjusting for inflation.\n",
    "2. Every big company in the poll reported cost increases.\n",
    "\n",
    "In MacCartney & Manning 2009 (henceforth, M&M), you read about one sort of system for doing this: a _natural logic_ system. This system works by (i) obtaining an _edit path_ from the premise and the hypothesis; (ii) mapping that edit path into an _inference path_; (iii) computing the _join_ of the inferences in this path to obtain a relation between the premise and the hypothesis; and (iv) checking whether there is a _forward entailment_ relation between the premise and the hypothesis.\n",
    "\n",
    "The definition of the relations is given in Table 2 of the paper.\n",
    "\n",
    "| Symbol           | Names                | Example                     | Set theoretic definition   |\n",
    "|:----------------:|:--------------------:|:---------------------------:|:--------------------------:|\n",
    "|$x \\equiv y$      | equivalence          | couch $\\equiv$ sofa         | $x = y$                    |\n",
    "|$x \\sqsubset y$   | forward entailment   | crow $\\sqsubset$ bird       | $x \\subset y$              |\n",
    "|$x \\sqsupset y$   | reverse entailment   | European $\\sqsupset$ French | $x \\supset y$              |\n",
    "|$x \\land y$       | negation             | human $\\land$ nonhuman      | $x \\cap y = \\emptyset$ & $x \\cup y = U$ |\n",
    "|$x \\mid y$        | alternation          | cat $\\mid$ dog              | $x \\cap y = \\emptyset$ & $x \\cup y \\neq U$ |a\n",
    "|$x \\smile y$      | cover                | animal $\\smile$ nonhuman    | $x \\cap y \\neq \\emptyset$ & $x \\cup y = U$ |\n",
    "|$x\\;\\#\\;y$        | independence         | animal $\\;\\#\\;$ nonhuman    | otherwise                  |\n",
    "\n",
    "The table of joins is given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "gFcUO95oE636",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1677253193975,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 300
    },
    "id": "gFcUO95oE636",
    "outputId": "3e7135af-05e6-4a35-fdb8-c540653ca947"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t≡\t[\t]\t^\t|\tu\t#\n",
      "≡\t≡\t[\t]\t^\t|\tu\t#\n",
      "\n",
      "[\t[\t[\t≡[]|#\t|\t|\t[^|u#\t[|#\n",
      "\n",
      "]\t]\t≡[]u#\t]\tu\t]^|u#\tu\t]u#\n",
      "\n",
      "^\t^\tu\t|\t≡\t]\t[\t#\n",
      "\n",
      "|\t|\t[^|u#\t|\t[\t≡[]|#\t[\t[|#\n",
      "\n",
      "u\tu\tu\t]^|u#\t]\t]\t≡[]u#\t]u#\n",
      "\n",
      "#\t#\t[u#\t]|#\t#\t]|#\t[u#\t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relations = ['≡', '[', ']', '^', '|', 'u', '#']\n",
    "\n",
    "join_table = {('≡', '≡'): {'≡'},\n",
    "              ('≡', '['): {'['},\n",
    "              ('≡', ']'): {']'},\n",
    "              ('≡', '^'): {'^'},\n",
    "              ('≡', '|'): {'|'},\n",
    "              ('≡', 'u'): {'u'},\n",
    "              ('≡', '#'): {'#'},\n",
    "              ('[', '≡'): {'['},\n",
    "              ('[', '['): {'['},\n",
    "              ('[', ']'): {'#', '|', '≡', '[', ']'},\n",
    "              ('[', '^'): {'|'},\n",
    "              ('[', '|'): {'|'},\n",
    "              ('[', 'u'): {'#', '^', 'u', '|', '['},\n",
    "              ('[', '#'): {'#', '|', '['},\n",
    "              (']', '≡'): {']'},\n",
    "              (']', '['): {'#', 'u', '≡', '[', ']'},\n",
    "              (']', ']'): {']'},\n",
    "              (']', '^'): {'u'},\n",
    "              (']', '|'): {'#', '^', 'u', '|', ']'},\n",
    "              (']', 'u'): {'u'},\n",
    "              (']', '#'): {'#', 'u', ']'},\n",
    "              ('^', '≡'): {'^'},\n",
    "              ('^', '['): {'u'},\n",
    "              ('^', ']'): {'|'},\n",
    "              ('^', '^'): {'≡'},\n",
    "              ('^', '|'): {']'},\n",
    "              ('^', 'u'): {'['},\n",
    "              ('^', '#'): {'#'},\n",
    "              ('|', '≡'): {'|'},\n",
    "              ('|', '['): {'[', '^', '|', 'u', '#'},\n",
    "              ('|', ']'): {'|'},\n",
    "              ('|', '^'): {'['},\n",
    "              ('|', '|'): {'#', '|', '≡', '[', ']'},\n",
    "              ('|', 'u'): {'['},\n",
    "              ('|', '#'): {'#', '|', '['},\n",
    "              ('u', '≡'): {'u'},\n",
    "              ('u', '['): {'u'},\n",
    "              ('u', ']'): {'#', '^', 'u', '|', ']'},\n",
    "              ('u', '^'): {']'},\n",
    "              ('u', '|'): {']'},\n",
    "              ('u', 'u'): {'#', 'u', '≡', '[', ']'},\n",
    "              ('u', '#'): {'#', 'u', ']'},\n",
    "              ('#', '≡'): {'#'},\n",
    "              ('#', '['): {'#', 'u', '['},\n",
    "              ('#', ']'): {']', '|', '#'},\n",
    "              ('#', '^'): {'#'},\n",
    "              ('#', '|'): {'#', '|', ']'},\n",
    "              ('#', 'u'): {'#', 'u', '['},\n",
    "              ('#', '#'): set()}\n",
    "\n",
    "print('\\t'.join(['']+relations))\n",
    "for r1 in relations:\n",
    "    row = '\\t'.join(''.join([r3 for r3 in relations \n",
    "                             if r3 in join_table[r1, r2]]) \n",
    "                    for r2 in relations)\n",
    "    print(f'{r1}\\t{row}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "y1BVat1GE638",
   "metadata": {
    "id": "y1BVat1GE638"
   },
   "source": [
    "In Tasks 1 and 2, you will be developing the core of this system, using the minimum edit distance-based edit paths we developed in class and assuming default inference relations associated with each atomic edit operation (as discussed in Section 4 of M&M). In Tasks 3 and 4, you will enrich this system with lexical relation information from WordNet (Task 3) and with more intelligent handling of inferences associated with certain environments (as discussed in Section 5 of M&M). You will then test the system on the classic FraCaS dataset. \n",
    "\n",
    "## Task 1\n",
    "\n",
    "*Lines:* 4\n",
    "\n",
    "Define the `__add__` magic method for the `Inference` class below. This method should use `join_table` (defined above) to produce a set of `Inference`s by joining two inferences—e.g. animal $\\sqsupset$ dog $\\bowtie$ dog $\\sqsupset$ greyhound = {animal $\\sqsupset$ greyhound}. `__add__` must return a set because, as M&M discussed, the result of joining two relations can result in indeterminacy. (In their implementation, M&M actually treat all such indetrminate joins as #. We will not do that here, since it is useful to see _why_ they do that.)\n",
    "\n",
    "Importantly, note that `__add__` should **not** be symmetric for the same reason joins are not: animal $\\sqsupset$ dog $\\bowtie$ dog $\\sqsubset$ mammal = {animal $\\equiv$ mammal, animal $\\sqsupset$ mammal, animal $\\sqsubset$ mammal, animal $\\smile$ mammal, animal $\\#$ mammal}, but dog $\\sqsubset$ mammal $\\bowtie$ animal $\\sqsupset$ dog isn't even a licit join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "WIdwn0IuE638",
   "metadata": {
    "id": "WIdwn0IuE638"
   },
   "outputs": [],
   "source": [
    "class Inference:\n",
    "    '''An inference from one linguistic expression to another\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    premise\n",
    "        The premise in the relation\n",
    "    hypothesis\n",
    "        The hypothesis in the relation\n",
    "    relation\n",
    "        The relation\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, premise: list[str], hypothesis: list[str], relation: str):\n",
    "        if relation not in relations:\n",
    "            raise ValueError(f'relation must be in {relations}')\n",
    "        \n",
    "        self.premise = premise\n",
    "        self.hypothesis = hypothesis\n",
    "        self.relation = relation\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return ' '.join(self.premise) + ' ' + self.relation + ' ' + ' '.join(self.hypothesis)\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return hash((tuple(self.premise), tuple(self.hypothesis), self.relation))\n",
    "        \n",
    "    def __add__(self, other: 'Inference') -> set['Inference']:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def __eq__(self, other: 'Inference') -> bool:\n",
    "        return self.premise == other.premise &\\\n",
    "               self.hypothesis == other.hypothesis &\\\n",
    "               self.relation == other.relation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zEM79G7qE638",
   "metadata": {
    "id": "zEM79G7qE638"
   },
   "source": [
    "Test your implementation of `Inference.__add__` using the `Editor` subclasses below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cPaqdsxDE639",
   "metadata": {
    "id": "cPaqdsxDE639"
   },
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "class Editor(ABC):\n",
    "    \n",
    "    def __init__(self, *args):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __call__(self, input: list[str], idx: int) -> Inference:\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def input(self):\n",
    "        return self._input\n",
    "    \n",
    "    @property\n",
    "    def output(self):\n",
    "        return self._output\n",
    "        \n",
    "\n",
    "class Substitution(Editor):\n",
    "    \"\"\"A substitution editor\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input\n",
    "        The string in the input to replace\n",
    "    output\n",
    "        The string to replace the input string with\n",
    "    relation\n",
    "        The inference relation that results\n",
    "    \"\"\"\n",
    "    \n",
    "    default_relation = None\n",
    "    \n",
    "    def __init__(self, input: str, output: str, relation: str):\n",
    "        self._input = input\n",
    "        self._output = output\n",
    "        self._relation = relation\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'<SUB \"{self._output}\" for \"{self._input}\" resulting in {self._relation}>'\n",
    "    \n",
    "    def __call__(self, input: list[str], idx: int) -> Inference:\n",
    "        \"\"\"Substitute input for output at location idx\"\"\"\n",
    "        if input[idx] != self._input:\n",
    "            raise ValueError(f'SUB \"{self._input}\" -> \"{self._output}\" at {idx} '\n",
    "                             f'cannot be applied to {input}')\n",
    "        \n",
    "        output = input[:idx] + [self._output] + input[(idx+1):]\n",
    "        \n",
    "        return Inference(input, output, self._relation)\n",
    "        \n",
    "class Deletion(Editor):\n",
    "    \"\"\"A deletion editor\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input\n",
    "        The string in the input to delete\n",
    "    relation\n",
    "        The inference relation that results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input: str, relation: str='['):\n",
    "        self._input = input\n",
    "        self._relation = relation\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return f'<DEL \"{self._input}\" resulting in {self._relation}>'\n",
    "        \n",
    "    def __call__(self, input: list[str], idx: int) -> Inference:\n",
    "        \"\"\"Substitute input for output at location idx\"\"\"\n",
    "        if input[idx] != self._input:\n",
    "            raise ValueError(f'DEL \"{self._input}\" at {idx} '\n",
    "                             f'cannot be applied to {input}')\n",
    "        \n",
    "        output = input[:idx] + input[(idx+1):]\n",
    "        \n",
    "        return Inference(input, output, self._relation)\n",
    "        \n",
    "class Insertion(Editor):\n",
    "    \"\"\"An insertion editor\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input\n",
    "        The string to insert into the output\n",
    "    relation\n",
    "        The inference relation that results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,  output: str, relation: str=']'):\n",
    "        self._output = output\n",
    "        self._relation = relation\n",
    "      \n",
    "    def __repr__(self):\n",
    "        return f'<INS \"{self._output}\" resulting in {self._relation}>'\n",
    "    \n",
    "    def __call__(self, input: list[str], idx: int) -> Inference:\n",
    "        \"\"\"Substitute input for output at location idx\"\"\"\n",
    "        output = input[:idx] + [self._output] + input[idx:]\n",
    "        \n",
    "        return Inference(input, output, self._relation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hicxSa6bE639",
   "metadata": {
    "id": "hicxSa6bE639"
   },
   "source": [
    "These subclasses are initialized with input and/or output strings and a relation. For instance, \"brindle\" and \"fawn\" are two different colorings of greyhounds—no greyhound is both brindle and fawn—and so they are in the | relation. Each is at least a [subsective modifier](https://en.wikipedia.org/wiki/Subsective_modifier) (all brindle greyhounds are greyhounds), so if we delete one, we obtain a $\\sqsubset$ relation, and if we insert one, we get a $\\sqsupset$ relation (the default relations for deletion and insertion, as discussed in M&M). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "iOu7miaTE639",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1677253193976,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 300
    },
    "id": "iOu7miaTE639",
    "outputId": "e38bcde0-387e-4b1f-9291-d00a6cb64c59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<SUB \"fawn\" for \"brindle\" resulting in |>,\n",
       " <DEL \"brindle\" resulting in [>,\n",
       " <INS \"brindle\" resulting in ]>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "substitute_fawn_for_brindle = Substitution('brindle', 'fawn', '|')\n",
    "delete_brindle = Deletion('brindle')\n",
    "insert_brindle = Insertion('brindle')\n",
    "\n",
    "substitute_fawn_for_brindle, delete_brindle, insert_brindle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Tme8qLxiE63-",
   "metadata": {
    "id": "Tme8qLxiE63-"
   },
   "source": [
    "Note that not all insertions or deletions of adjectives will be associated with $\\sqsubset$ or $\\sqsupset$: privative adjectives like \"fake\" will introduce a $|$: fake greyhounds are not greyhounds (fake greyhound $|$ greyhound) and greyhounds are not fake greyhounds (greyhound $|$ fake greyhound)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "SD8NogjoE63-",
   "metadata": {
    "id": "SD8NogjoE63-"
   },
   "outputs": [],
   "source": [
    "delete_fake = Deletion('fake', relation='|')\n",
    "insert_fake = Insertion('fake', relation='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CRcvSj5YE63-",
   "metadata": {
    "id": "CRcvSj5YE63-"
   },
   "source": [
    "Indeed, most substitutions involving \"fake\" will also yield a $|$ relation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "PPT3U9OME63_",
   "metadata": {
    "id": "PPT3U9OME63_"
   },
   "outputs": [],
   "source": [
    "substitute_fake_for_virtuosic = Substitution('virtuosic', 'fake', '|')\n",
    "substitute_virtuosic_for_fake = Substitution('fake', 'virtuosic', '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1H2GqVYcE63_",
   "metadata": {
    "id": "1H2GqVYcE63_"
   },
   "source": [
    "But insertion and deletion edits involving \"virtuosic\" should act like \"brindle\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "jJge0uv8E63_",
   "metadata": {
    "id": "jJge0uv8E63_"
   },
   "outputs": [],
   "source": [
    "delete_virtuosic = Deletion('virtuosic')\n",
    "insert_virtuosic = Insertion('virtuosic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "luWno6WGE63_",
   "metadata": {
    "id": "luWno6WGE63_"
   },
   "source": [
    "Use the following four sentences to write your tests. These tests should involve applying an edit $e_1$ to sentence $s_i$ to yield sentence $e_1(s_i)$, then applying an edit $e_2$ to $e_1(s_i)$ to yield sentence. You should then combine the inferences associated with $e_1$ and $e_2$ using your `Inference.__add__` and check that it is correct. Make sure to test at least one case where the result should be a non-singleton set of inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "oV5NW9KGE63_",
   "metadata": {
    "id": "oV5NW9KGE63_"
   },
   "outputs": [],
   "source": [
    "test_sentence1 = ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
    "test_sentence2 = ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
    "test_sentence3 = ['a', 'fake', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
    "test_sentence4 = ['a', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "X7jQ1wr0E63_",
   "metadata": {
    "id": "X7jQ1wr0E63_"
   },
   "outputs": [],
   "source": [
    "# write tests here  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2579506f",
   "metadata": {},
   "source": [
    "## Editor Libraries\n",
    "\n",
    "We'll need a way to store collections of editors and, crucially, make new default ones when needed. The `EditorLibrary` class provides a convenient way to store and retrieve editors (like substitutions, deletions, and insertions). When we try to get an editor that isn't in the library, it automatically creates a default one - substitutions get a `#` relation (since we don't have a default for them), while deletions and insertions get the default behavior defined by MacCartney & Manning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ac298b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Union\n",
    "\n",
    "EditorType = Literal['substitute', 'delete', 'insert']\n",
    "EditorInputOutput = Union[str, tuple[str, str]]\n",
    "\n",
    "empty_library = {'substitute': {}, 'delete': {}, 'insert': {}}\n",
    "\n",
    "class EditorLibrary:\n",
    "    def __init__(self, library: dict[EditorType, dict[EditorInputOutput, Editor]] = empty_library):\n",
    "        self._library = library\n",
    "        \n",
    "    def __getitem__(self, key: tuple[EditorType, EditorInputOutput]) -> Editor:\n",
    "        editor_type, edit = key\n",
    "        if edit not in self._library[editor_type]:\n",
    "            self._add_default_editor(editor_type, edit)\n",
    "        return self._library[editor_type][edit]\n",
    "    \n",
    "    def add_editor(self, editor: Editor):\n",
    "        if isinstance(editor, Substitution):\n",
    "            self._library['substitute'][(editor.input, editor.output)] = editor\n",
    "            \n",
    "        if isinstance(editor, Insertion):\n",
    "            self._library['insert'][editor.output] = editor\n",
    "            \n",
    "        if isinstance(editor, Deletion):\n",
    "            self._library['delete'][editor.input] = editor\n",
    "            \n",
    "    def _add_default_editor(self, editor_type: str, edit: EditorInputOutput):\n",
    "        if editor_type == 'substitute':\n",
    "            self.add_editor(Substitution(input=edit[0], output=edit[1], relation='#'))\n",
    "        \n",
    "        elif editor_type == 'insert':\n",
    "            self.add_editor(Insertion(output=edit))\n",
    "            \n",
    "        elif editor_type == 'delete':\n",
    "            self.add_editor(Deletion(input=edit))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f'{editor_type} is not a recognized edit type')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "obB8OzQDE63_",
   "metadata": {
    "id": "obB8OzQDE63_"
   },
   "source": [
    "## Task 2\n",
    "\n",
    "*Lines:* 20\n",
    "\n",
    "We don't want to have to hand-compute the edits that are required to convert one sentence into another. Instead, we will use a modified form of the `StringEdit` class we developed in class. What we need in particular are the edit paths that that class produces.\n",
    "\n",
    "First, we'll define a class for representing and manipulating edit paths. One important thing we want this class to do is to convert the edit path into a list of editors, for which we need to have a way to look up the editor for a given edit type and parameters given an `EditorLibrary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "8aa4a08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EditLocation = int\n",
    "EditorParameters = tuple[EditorInputOutput, EditLocation]\n",
    "\n",
    "class EditPath:\n",
    "    \"\"\"Class for representing and manipulating sequences of text edits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    edits\n",
    "        List of tuples containing edit type and parameters for each edit operation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, edits: list[tuple[EditorType, EditorParameters]]):\n",
    "        self.edits_unshifted = edits\n",
    "        self.edits = self._shift_indices(edits)\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.edits)\n",
    "\n",
    "    def __call__(self, input_text: list[str]) -> list[str]:\n",
    "        \"\"\"Apply the edit path to transform the input text.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        input_text : list[str]\n",
    "            The input text to transform\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        list[str]\n",
    "            The transformed text after applying all edits\n",
    "        \"\"\"\n",
    "        current_text = input_text.copy()\n",
    "        \n",
    "        for edit_type, (edit, idx) in self.edits:\n",
    "            if edit_type == 'substitute':\n",
    "                old_word, new_word = edit\n",
    "\n",
    "                if old_word != current_text[idx]:\n",
    "                    raise ValueError(\n",
    "                        f'Substitution {old_word} -> {new_word} at {idx} '\n",
    "                        f'cannot be applied to {current_text}'\n",
    "                    )\n",
    "\n",
    "                current_text[idx] = new_word\n",
    "                \n",
    "            elif edit_type == 'delete':\n",
    "                current_text.pop(idx)\n",
    "                \n",
    "            elif edit_type == 'insert':\n",
    "                current_text.insert(idx, edit)\n",
    "                \n",
    "        return current_text\n",
    "\n",
    "    def _shift_indices(self, edit_path: list[tuple[EditorType, EditorParameters]]) -> list[tuple[EditorType, EditorParameters]]:\n",
    "        \"\"\"Adjust indices of edits to account for previous insertions and deletions.\n",
    "\n",
    "        The edit sequence output by the `StringEdit` class is always relativized to \n",
    "        the original string. But if we are applying the edits in sequence, the string\n",
    "        that we apply subsequent edits to will differ from the original string, so we\n",
    "        need to shift the indices of the subsequent edits to account for the previous\n",
    "        edits.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        edit_path : list[tuple[EditorType, EditorParameters]]\n",
    "            Original list of edits with unadjusted indices.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[tuple[EditorType, EditorParameters]]\n",
    "            List of edits with indices shifted to account for previous edits.\n",
    "        \"\"\"\n",
    "        edit_path_shifted = []\n",
    "\n",
    "        # track cumulative index shifts from insertions/deletions\n",
    "        # we always need to shift back by 1 to account for the fact that\n",
    "        # the original edit sequence is 1-indexed (due to the sentinel)\n",
    "        shifts = [(0, -1)]\n",
    "\n",
    "        for i, (edit_type, (edit, idx)) in enumerate(edit_path):\n",
    "            original_idx = idx\n",
    "\n",
    "            # apply all previous shifts to current index\n",
    "            for j, s in shifts:\n",
    "                idx = idx + s if idx >= j else idx \n",
    "\n",
    "            if edit_type == 'substitute':\n",
    "                # if the substitution is the same element, we don't need to\n",
    "                # record the edit at all because it doesn't change the string\n",
    "                if edit[0] == edit[1]:\n",
    "                    continue\n",
    "                else:\n",
    "                    # substitution does not shift the index because \n",
    "                    # we're simply replacing one element with another\n",
    "                    shifts.append((idx, 0))\n",
    "    \n",
    "            elif edit_type == 'delete':\n",
    "                # deletion shifts the index back by 1 because \n",
    "                # they remove an element\n",
    "                shifts.append((idx, -1))\n",
    "\n",
    "            elif edit_type == 'insert':\n",
    "                # insertion shifts the index forward by 1 because \n",
    "                # they add an element\n",
    "                shifts.append((idx, 1))\n",
    "\n",
    "                next_edit_type, (_, next_idx) = edit_path[i+1]\n",
    "                \n",
    "                # ensures that the inserted element does not get inserted out of \n",
    "                # order with an immediately following substitution \n",
    "                if next_edit_type != 'substitute' or next_idx != original_idx:\n",
    "                    idx += 1\n",
    "\n",
    "            edit_path_shifted.append((edit_type, (edit, idx)))\n",
    "\n",
    "        return edit_path_shifted\n",
    "    \n",
    "    def to_editors(self, library: EditorLibrary) -> list[Editor]:\n",
    "        \"\"\"Convert edit path to a sequence of editors using the provided library.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        library : EditorLibrary\n",
    "            Dictionary mapping edit types and parameters to Editor instances.\n",
    "            The outer dictionary maps edit types (e.g. 'substitute', 'delete', 'insert')\n",
    "            to inner dictionaries that map edit parameters to Editor instances.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list[Editor]\n",
    "            List of Editor instances corresponding to the edits in this path.\n",
    "            Only includes edits that have matching editors in the library.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            library[edit_type][edit] \n",
    "            for edit_type, (edit, idx) in self.edits\n",
    "            if edit in library[edit_type]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a592d6",
   "metadata": {},
   "source": [
    "Next, we'll define a class for computing edit distances, alignments, and edit paths between strings we used in class, with some slight updates for the current assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "Xm85fPY5E64A",
   "metadata": {
    "id": "Xm85fPY5E64A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "Alignment = list[tuple[int, int]]\n",
    "\n",
    "class StringEdit:\n",
    "    \"\"\"Class for computing edit distances, alignments, and edit paths between strings.\n",
    "\n",
    "    This class implements the Wagner-Fisher algorithm for computing minimum edit\n",
    "    distance between sequences, along with the corresponding alignments and edit paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    insertion_cost : float, default=1.0\n",
    "        Cost of inserting a character\n",
    "    deletion_cost : float, default=1.0\n",
    "        Cost of deleting a character\n",
    "    substitution_cost : float or None, default=None\n",
    "        Cost of substituting a character. If None, defaults to insertion_cost + deletion_cost\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, insertion_cost: float = 1., deletion_cost: float = 1., substitution_cost: float | None = None):\n",
    "        self._insertion_cost = insertion_cost\n",
    "        self._deletion_cost = deletion_cost\n",
    "\n",
    "        if substitution_cost is None:\n",
    "            self._substitution_cost = insertion_cost + deletion_cost\n",
    "        else:\n",
    "            self._substitution_cost = substitution_cost\n",
    "\n",
    "    def __call__(self, source: list[str], target: list[str], only_distance: bool = False) ->  float | tuple[float, Alignment, list[EditPath]]:\n",
    "        return self._wagner_fisher(source, target, only_distance)\n",
    "            \n",
    "    def _wagner_fisher(self, source: list[str], target: list[str], only_distance: bool) ->  float | tuple[float, Alignment, list[EditPath]]:\n",
    "        \"\"\"Compute minimum edit distance, alignment, and edit sequence using Wagner-Fisher algorithm.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source : list[str]\n",
    "            Source sequence\n",
    "        target : list[str]\n",
    "            Target sequence\n",
    "        only_distance : bool\n",
    "            If True, return only the edit distance\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        float | tuple[float, Alignment, list[EditPath]]\n",
    "            If only_distance is True, returns just the edit distance.\n",
    "            Otherwise returns a tuple of (distance, alignment, edit_paths)\n",
    "        \"\"\"\n",
    "        n, m = len(source), len(target)\n",
    "        source, target = self._add_sentinel(source, target)\n",
    "\n",
    "        # initialize matrices for dynamic programming\n",
    "        distance = np.zeros([n+1, m+1], dtype=float)\n",
    "        pointers = np.zeros([n+1, m+1], dtype=list)\n",
    "        edits = np.zeros([n+1, m+1], dtype=list)\n",
    "\n",
    "        pointers[0,0] = []\n",
    "        edits[0,0] = []\n",
    "        \n",
    "        # initialize first column (deletions)\n",
    "        for i in range(1,n+1):\n",
    "            distance[i,0] = distance[i-1,0]+self._deletion_cost\n",
    "            pointers[i,0] = [(i-1,0)]\n",
    "            edits[i,0] = [('delete', (source[i], i))]\n",
    "\n",
    "        # initialize first row (insertions)\n",
    "        for j in range(1,m+1):\n",
    "            distance[0,j] = distance[0,j-1]+self._insertion_cost\n",
    "            pointers[0,j] = [(0,j-1)]\n",
    "            edits[0,j] = [('insert', (target[j], j))]\n",
    "            \n",
    "        # fill in the rest of the matrices\n",
    "        for i in range(1,n+1):\n",
    "            for j in range(1,m+1):\n",
    "                if source[i] == target[j]:\n",
    "                    substitution_cost = 0.\n",
    "                else:\n",
    "                    substitution_cost = self._substitution_cost\n",
    "                    \n",
    "                costs = np.array([distance[i-1,j]+self._deletion_cost,\n",
    "                                  distance[i-1,j-1]+substitution_cost,\n",
    "                                  distance[i,j-1]+self._insertion_cost])\n",
    "                    \n",
    "                distance[i,j] = costs.min()\n",
    "\n",
    "                best_edits = np.where(costs==distance[i,j])[0]\n",
    "\n",
    "                indices = [(i-1,j), (i-1,j-1), (i,j-1)]\n",
    "                pointers[i,j] = [indices[k] for k in best_edits]\n",
    " \n",
    "                edit_types = list(zip([\"delete\", \"substitute\", \"insert\"],\n",
    "                                      [(source[i], i), \n",
    "                                       ((source[i], target[j]), i), \n",
    "                                       (target[j], i)]))\n",
    "                edits[i,j] = [edit_types[k] for k in best_edits]\n",
    "\n",
    "        if only_distance:\n",
    "            return distance[n,m]\n",
    "\n",
    "        pointer_backtrace, edit_backtrace = self._construct_backtrace(pointers, edits, n, m)\n",
    "\n",
    "        edit_paths = [EditPath(bt) for bt in edit_backtrace]\n",
    "\n",
    "        return distance[n,m], pointer_backtrace, edit_paths\n",
    "\n",
    "    def _construct_backtrace(self, pointers: np.ndarray, edits: np.ndarray, n: int, m: int) -> tuple[list[list[tuple[int, int]]], list[list[tuple[str, tuple[str, int]]]]]:\n",
    "        \"\"\"Construct all possible backtraces through the dynamic programming matrix.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        pointers : np.ndarray\n",
    "            Matrix of pointers to previous cells\n",
    "        edits : np.ndarray\n",
    "            Matrix of edit operations\n",
    "        n : int\n",
    "            Length of source sequence\n",
    "        m : int\n",
    "            Length of target sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[list[list[tuple[int, int]]], list[list[tuple[str, tuple[str, int]]]]]\n",
    "            Returns (pointer_backtraces, edit_backtraces)\n",
    "        \"\"\"\n",
    "        stack = [([(n,m)], [])]\n",
    "        complete_pointer_backtraces = []\n",
    "        complete_edit_backtraces = []\n",
    "        \n",
    "        while stack:\n",
    "            current_pointer_path, current_edit_path = stack.pop()\n",
    "            current_pos = current_pointer_path[-1]\n",
    "            \n",
    "            if current_pos == (0,0):\n",
    "                complete_pointer_backtraces.append(current_pointer_path[::-1])\n",
    "                complete_edit_backtraces.append(current_edit_path[::-1])\n",
    "                continue\n",
    "                \n",
    "            for next_pos, edit in zip(pointers[current_pos], edits[current_pos]):\n",
    "                new_pointer_path = current_pointer_path + [next_pos]\n",
    "                new_edit_path = current_edit_path + [edit]\n",
    "                stack.append((new_pointer_path, new_edit_path))\n",
    "                \n",
    "        return complete_pointer_backtraces, complete_edit_backtraces\n",
    "        \n",
    "    def _add_sentinel(\n",
    "        self, \n",
    "        source: str | list | tuple, \n",
    "        target: str | list | tuple\n",
    "    ) -> tuple[str | list | tuple, str | list | tuple]:\n",
    "        \"\"\"Add sentinel symbols to beginning of sequences.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        source\n",
    "            Source sequence\n",
    "        target\n",
    "            Target sequence\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple[str | list | tuple, str | list | tuple]\n",
    "            Source and target with added sentinels\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If source or target are not str, list, or tuple\n",
    "        \"\"\"\n",
    "        if isinstance(source, str):\n",
    "            source = '#'+source\n",
    "        elif isinstance(source, list):\n",
    "            source = ['#'] + source\n",
    "        elif isinstance(source, tuple):\n",
    "            source = ('#',) + source\n",
    "        else:\n",
    "            raise ValueError('source must be str, list, or tuple')\n",
    "            \n",
    "        if isinstance(target, str):\n",
    "            target = '#' + target\n",
    "        elif isinstance(target, list):\n",
    "            target = ['#'] + target\n",
    "        elif isinstance(target, tuple):\n",
    "            target = ('#',) + target\n",
    "        else:\n",
    "            raise ValueError('target must be str, list, or tuple')\n",
    "            \n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tKFxyYhiE64A",
   "metadata": {
    "id": "tKFxyYhiE64A"
   },
   "source": [
    "In the original implementation, the edit path indexed into the source string. This made sense at the time because we wanted to know which words, relative to their original position in the string, are operated on by an edit. It's problematic for current purposes, because once we compute insertions and deletions, the position of later insertions or deletions change. The implementation below now corrects for this, but just make sure you're taking into account that the order of edits matters for this reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "vxfhbuCAE64A",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1677253193978,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 300
    },
    "id": "vxfhbuCAE64A",
    "outputId": "f6292874-34a5-4b61-a83e-e14bbc391d33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
      "Target:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)]]\n",
      "Edit path: [[('delete', ('virtuosic', 1)), ('delete', ('brindle', 5))]]\n",
      "Edited: [['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "editdist = StringEdit(1, 1, 1)\n",
    "\n",
    "dist, align, edits = editdist(test_sentence1, test_sentence2)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence1) == test_sentence2 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence1)\n",
    "print('Target:   ', test_sentence2)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edited:\", [e(test_sentence1) for e in edits])\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "4uCuEVluE64B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1677253193978,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 300
    },
    "id": "4uCuEVluE64B",
    "outputId": "fb1de2b0-5140-48a3-9cbc-7b53aecab378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Target:    ['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)]]\n",
      "Edit path: [[('insert', ('virtuosic', 1)), ('insert', ('brindle', 6))]]\n",
      "Edited: [['a', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "dist, align, edits = editdist(test_sentence2, test_sentence1)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence2) == test_sentence1 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence2)\n",
    "print('Target:   ', test_sentence1)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edited:\", [e(test_sentence2) for e in edits])\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "40845765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Target:    ['some', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)], [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)]]\n",
      "Edit path: [[('substitute', (('a', 'some'), 0)), ('insert', ('virtuosic', 1)), ('insert', ('brindle', 6))], [('insert', ('some', 0)), ('substitute', (('a', 'virtuosic'), 1)), ('insert', ('brindle', 6))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "test_sentence1_prime = [\"some\"] + test_sentence1[1:]\n",
    "\n",
    "dist, align, edits = editdist(test_sentence2, test_sentence1_prime)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence2) == test_sentence1_prime for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence2)\n",
    "print('Target:   ', test_sentence1_prime)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "eaf3c8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['some', 'virtuosic', 'synthesist', 'loves', 'a', 'happy', 'brindle', 'greyhound']\n",
      "Target:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 0), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)], [(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)]]\n",
      "Edit path: [[('delete', ('some', 0)), ('substitute', (('virtuosic', 'a'), 0)), ('delete', ('brindle', 5))], [('substitute', (('some', 'a'), 0)), ('delete', ('virtuosic', 1)), ('delete', ('brindle', 5))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "dist, align, edits = editdist(test_sentence1_prime, test_sentence2)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence1_prime) == test_sentence2 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence1_prime)\n",
    "print('Target:   ', test_sentence2)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "3659e79f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Target:    ['a', 'virtuosic', 'synthesist', 'loves', 'some', 'happy', 'brindle', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)]]\n",
      "Edit path: [[('insert', ('virtuosic', 1)), ('substitute', (('a', 'some'), 4)), ('insert', ('brindle', 6))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "test_sentence1_prime2 = test_sentence1[:4] + [\"some\"] + test_sentence1[5:]\n",
    "\n",
    "dist, align, edits = editdist(\n",
    "    test_sentence2, \n",
    "    test_sentence1_prime2\n",
    ")\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence2) == test_sentence1_prime2 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence2)\n",
    "print('Target:   ', test_sentence1_prime2)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "fc352da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'virtuosic', 'synthesist', 'loves', 'some', 'happy', 'brindle', 'greyhound']\n",
      "Target:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)]]\n",
      "Edit path: [[('delete', ('virtuosic', 1)), ('substitute', (('some', 'a'), 3)), ('delete', ('brindle', 5))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "dist, align, edits = editdist(test_sentence1_prime2, test_sentence2)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence1_prime2) == test_sentence2 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence1_prime2)\n",
    "print('Target:   ', test_sentence2)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", all(e(test_sentence1_prime2) == test_sentence2 for e in edits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "bfd81ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['some', 'virtuosic', 'synthesist', 'loves', 'some', 'happy', 'brindle', 'greyhound']\n",
      "Target:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 0), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)], [(0, 0), (1, 1), (2, 1), (3, 2), (4, 3), (5, 4), (6, 5), (7, 5), (8, 6)]]\n",
      "Edit path: [[('delete', ('some', 0)), ('substitute', (('virtuosic', 'a'), 0)), ('substitute', (('some', 'a'), 3)), ('delete', ('brindle', 5))], [('substitute', (('some', 'a'), 0)), ('delete', ('virtuosic', 1)), ('substitute', (('some', 'a'), 3)), ('delete', ('brindle', 5))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_sentence1_prime3 = [\"some\"] + test_sentence1[1:4] + [\"some\"] + test_sentence1[5:]\n",
    "dist, align, edits = editdist(test_sentence1_prime3, test_sentence2)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence1_prime3) == test_sentence2 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence1_prime3)\n",
    "print('Target:   ', test_sentence2)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "ca9fd871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source:    ['a', 'synthesist', 'loves', 'a', 'happy', 'greyhound']\n",
      "Target:    ['some', 'virtuosic', 'synthesist', 'loves', 'some', 'happy', 'brindle', 'greyhound']\n",
      "Pointer path: [[(0, 0), (1, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)], [(0, 0), (0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (5, 7), (6, 8)]]\n",
      "Edit path: [[('substitute', (('a', 'some'), 0)), ('insert', ('virtuosic', 1)), ('substitute', (('a', 'some'), 4)), ('insert', ('brindle', 6))], [('insert', ('some', 0)), ('substitute', (('a', 'virtuosic'), 1)), ('substitute', (('a', 'some'), 4)), ('insert', ('brindle', 6))]]\n",
      "Edit path is correct: True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dist, align, edits = editdist(test_sentence2, test_sentence1_prime3)\n",
    "\n",
    "edit_path_is_correct = all(\n",
    "    e(test_sentence2) == test_sentence1_prime3 for e in edits\n",
    ")\n",
    "\n",
    "print('Source:   ', test_sentence2)\n",
    "print('Target:   ', test_sentence1_prime3)\n",
    "print('Pointer path:', align)\n",
    "print('Edit path:', edits)\n",
    "print(\"Edit path is correct:\", edit_path_is_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_xVZ2SibE64B",
   "metadata": {
    "id": "_xVZ2SibE64B"
   },
   "source": [
    "Implement the `__call__` method for the `NaturalLogic` class. This should take a premise sentence and a hypothesis sentence, and it should produce the paths of inferences (computed from the paths of edits) that take you from premise to hypothesis. \n",
    "\n",
    "Each path should be a list of inferences that result from cumulatively composing the inferences associated with each edit in the path. It should **not** be a path of local inferences. That is, it should not be a list of inferences that result from each edit in an edit path, but rather a list of inferences that result from composing those edits using `Inference.__add__`.\n",
    "\n",
    "You will **not** be using `EditPath.__call__` in any way. That method is implemented to demonstrate how we should apply edit paths to strings. You should instead be using `EditPath.to_editors` to get the list of editors that result from the edit path, and then you should use those editors to compute the local inferences (again, the inferences that result from applying each editor in sequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F9yzAtCHE64B",
   "metadata": {
    "id": "F9yzAtCHE64B"
   },
   "outputs": [],
   "source": [
    "InferencePath = list[Inference]\n",
    "\n",
    "class NaturalLogic:\n",
    "    \"\"\"Class for performing natural logic inference between sentences.\n",
    "\n",
    "    This class implements natural logic inference by finding edit paths between sentences\n",
    "    and composing the inferences associated with each edit. It uses an editor library\n",
    "    that maps edit operations (substitutions, deletions, insertions) to Editor instances\n",
    "    that specify the inference relations for those edits.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    editor_library : EditorLibrary, optional\n",
    "        Dictionary mapping edit types and parameters to Editor instances. The outer \n",
    "        dictionary maps edit types (e.g. 'substitute', 'delete', 'insert') to inner \n",
    "        dictionaries that map edit parameters to Editor instances. Defaults to an empty\n",
    "        EditorLibrary.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    EDIT : StringEdit\n",
    "        StringEdit instance used for computing edit distances and paths between strings,\n",
    "        with default costs of 1 for substitution, deletion, and insertion.\n",
    "    _editor_library : EditorLibrary\n",
    "        The editor library containing the inference rules for different edits.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    EDIT = StringEdit(1, 1, 1)\n",
    "    \n",
    "    def __init__(self, editor_library: EditorLibrary=EditorLibrary()):\n",
    "        self._editor_library = editor_library\n",
    "    \n",
    "    def __getitem__(self, key: tuple[EditorType, EditorInputOutput]):\n",
    "        return self._editor_library[key]\n",
    "    \n",
    "    def __call__(self, premise: list[str], hypothesis: list[str]) -> list[InferencePath]:\n",
    "        \"\"\"Perform natural logic inference between a premise and hypothesis sentence.\n",
    "\n",
    "        This method computes the possible edit paths between the premise and \n",
    "        hypothesis, and then composes the inferences associated with each edit \n",
    "        to yield all possible inference paths implied by the edit paths.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        premise : list[str]\n",
    "            The premise sentence to infer from.\n",
    "        hypothesis : list[str]\n",
    "            The hypothesis sentence to infer to.\n",
    "\n",
    "        Returns \n",
    "        -------\n",
    "        global_inference_paths : list[InferencePath]\n",
    "            A list of inference paths representing the paths of cumulatively \n",
    "            composed inferences from the premise to the hypothesis implied by \n",
    "            the edit path.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "keeRonENE64B",
   "metadata": {
    "id": "keeRonENE64B"
   },
   "source": [
    "Implement tests using the four test sentences above. (Ignore my modified versions of these sentences.) For now, you can just assume that the editor library contains the editors defined for Task 1. (We don't need to explicitly specify any insertions that result in $\\sqsupset$ or deletions that result in $\\sqsubset$, since those are added by default by `NaturalLogic.add_editor`.) In Task 3, we will expand the library using [WordNet](https://wordnet.princeton.edu/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HVM5wCSOE64B",
   "metadata": {
    "id": "HVM5wCSOE64B"
   },
   "outputs": [],
   "source": [
    "library = {'substitute': {('virtuosic', 'fake'): substitute_fake_for_virtuosic,\n",
    "                          ('fake', 'virtuosic'): substitute_virtuosic_for_fake,\n",
    "                          ('brindle', 'fawn'): substitute_fawn_for_brindle}, \n",
    "           'delete': {\"fake\": delete_fake}, \n",
    "           'insert': {\"fake\": insert_fake}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fzq33LxxE64B",
   "metadata": {
    "id": "Fzq33LxxE64B"
   },
   "outputs": [],
   "source": [
    "# write tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IzlawFvyE64B",
   "metadata": {
    "id": "IzlawFvyE64B"
   },
   "source": [
    "## Evaluating against FraCaS\n",
    "\n",
    "For the remainder of the assignment (Tasks 3 and 4), we will evaluate our `NaturalLogic` implementation using the [FraCaS textual inference test suite](https://nlp.stanford.edu/~wcmac/downloads/fracas.xml). FraCaS is shipped as XML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HDjqdHS_E64B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1486,
     "status": "ok",
     "timestamp": 1678725791472,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 240
    },
    "id": "HDjqdHS_E64B",
    "outputId": "5472734a-05a1-43a0-ab6c-591e8237bb1b"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "wget https://nlp.stanford.edu/~wcmac/downloads/fracas.xml\n",
    "cat fracas.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kxqlq6J7E64B",
   "metadata": {
    "id": "Kxqlq6J7E64B"
   },
   "source": [
    "I've included a simple corpus reader below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P_0OIUnwE64B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8901,
     "status": "ok",
     "timestamp": 1678725812401,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 240
    },
    "id": "P_0OIUnwE64B",
    "outputId": "6d0dc6c5-1c26-4fea-8e2f-05a54e304b0e"
   },
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4\n",
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GjtI0AgLE64C",
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1678725812596,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 240
    },
    "id": "GjtI0AgLE64C"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup, Tag\n",
    "\n",
    "class Fracas:\n",
    "    \"\"\"Corpus reader for the FraCaS textual inference problem set\"\"\"\n",
    "    \n",
    "    def __init__(self, root: str=\"fracas.xml\"):\n",
    "        with open(root) as fp:\n",
    "            self._data = BeautifulSoup(fp, 'lxml')\n",
    "            \n",
    "        self._construct_problem_generator()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        return next(self._problem_generator)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._data.comment.string\n",
    "     \n",
    "    def _construct_problem_generator(self):\n",
    "        for problem in self.problems:\n",
    "            yield problem\n",
    "    \n",
    "    @property\n",
    "    def problems(self):\n",
    "        return [FracasProblem(problem) \n",
    "                for problem in self._data.find_all('problem')]\n",
    "\n",
    "class FracasProblem:\n",
    "    \"\"\"A FraCaS problem\"\"\"\n",
    "    \n",
    "    problem_type_map = {'001'}\n",
    "    \n",
    "    def __init__(self, problem: Tag):\n",
    "        self.id = problem.get('id')\n",
    "        self.answer = problem.get('fracas_answer')\n",
    "        \n",
    "        self.premise = problem.p.string.strip()\n",
    "        self.question = problem.q.string.strip()\n",
    "        self.hypothesis = problem.h.string.strip()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return (f\"id: {self.id}\"\n",
    "                f\"\\n\\npremise: {self.premise}\"\n",
    "                f\"\\nquestion: {self.question}\"\n",
    "                f\"\\nhypothesis: {self.hypothesis}\"\n",
    "                f\"\\n\\nanswer: {self.answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P95G2O6CE64C",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1678725812975,
     "user": {
      "displayName": "Aaron Steven White",
      "userId": "06256629009318567325"
     },
     "user_tz": 240
    },
    "id": "P95G2O6CE64C",
    "outputId": "3d903679-41d7-4dc8-f240-16eb1bf17fae"
   },
   "outputs": [],
   "source": [
    "fracas = Fracas()\n",
    "\n",
    "fracas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XnjCoTQOE64C",
   "metadata": {
    "id": "XnjCoTQOE64C"
   },
   "source": [
    "Since the sentences are just raw strings, to get them in the form of a list of strings, you will need a tokenizer. I would suggest using the one available in the [`stanza`](https://stanfordnlp.github.io/stanza/) package. For our purposes, it is also simpler to use the lemma, rather than the token itself, because your WordNet editor library won't handle inflectional morphology (unless you explicitly engineered it to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zVHJauoyE64C",
   "metadata": {
    "id": "zVHJauoyE64C"
   },
   "outputs": [],
   "source": [
    "!pip install stanza\n",
    "\n",
    "import stanza\n",
    "\n",
    "stanza.download('en')\n",
    "lemmatizer = stanza.Pipeline('en', processors='tokenize, mwt, pos, lemma')\n",
    "\n",
    "lemmatizer('Every virtuosic synthesist loves some greyhounds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FUAo0QidE64C",
   "metadata": {
    "id": "FUAo0QidE64C"
   },
   "source": [
    "To use this dataset to test your `NaturalLogic` implementation, you will need to convert the inference produced by `__call__` into a \"yes\", \"no\", or \"don't know\" answer. (Don't worry about any items not labeled with one of these three. This will require you to define a mapping from inference types to answers. You should then compute the accuracy, precision, recall, and F1 of your system.\n",
    "\n",
    "Each of these metrics can be defined in terms of...\n",
    "\n",
    "1. The true positive count for class $c$: $$\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i = \\hat{y}^\\mathrm{test}_i = c\\}|$$\n",
    "2. The true negative count for class $c$: $$\\mathrm{tn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i \\neq c \\land \\hat{y}^\\mathrm{test}_i \\neq c\\}|$$\n",
    "3. The false positve count for class $c$: $$\\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i \\neq c \\land \\hat{y}^\\mathrm{test}_i = c\\}|$$\n",
    "4. The false negative count for class $c$: $$\\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) = |\\{i\\;:\\;y^\\mathrm{test}_i = c \\land \\hat{y}^\\mathrm{test}_i \\neq c\\}|$$\n",
    "\n",
    "...where the class is \"yes\", \"no\", or \"unknown\"; $y^\\mathrm{test}_i$ is the true label for item $i$ (found in FraCaS) and $\\hat{y}^\\mathrm{test}_i$ is your system's prediction for the class of item $i$. (Ignore cases where the class is not one of these three.)\n",
    "\n",
    "#### Accuracy\n",
    "\n",
    "For what proportion of the test data $\\{(x^\\mathrm{test}_{1}, y^\\mathrm{test}_1), ..., (x^\\mathrm{test}_N, y^\\mathrm{test}_N)\\}$ does the model's predicted class $f(x^\\mathrm{test}_i) = \\hat{y}^\\mathrm{test}_i$ for an item match the ground truth class for that item?\n",
    "\n",
    "$$\\mathrm{accuracy}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{\\sum_{c \\in \\mathcal{Y}}\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{tn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{N}$$\n",
    "\n",
    "[`sklearn.metrics`](https://scikit-learn.org/stable/modules/model_evaluation.html) technically provides an [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html#sklearn.metrics.accuracy_score) function, but generally it's just as straightforward to compute it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxxhjxCNE64C",
   "metadata": {
    "id": "pxxhjxCNE64C"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ULbeDMdE64C",
   "metadata": {
    "id": "6ULbeDMdE64C"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DImQUFN-E64C",
   "metadata": {
    "id": "DImQUFN-E64C"
   },
   "source": [
    "#### Precision\n",
    "\n",
    "For a particular class $c$, what proportion of the test items that the model said have that class actually have that class?\n",
    "\n",
    "$$\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}$$\n",
    "\n",
    "For giving an aggregate precision across classes, it's common to distinguish _micro-average_ precision and _macro-average_ precision.\n",
    "\n",
    "$$\\mathrm{microprecision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{\\sum_{c \\in \\mathcal{Y}} \\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\sum_{c \\in \\mathcal{Y}} \\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}$$\n",
    "\n",
    "$$\\mathrm{macroprecision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}\\right) = \\frac{1}{|\\mathcal{Y}|}\\sum_{c \\in \\mathcal{Y}} \\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JxyyeQcNE64C",
   "metadata": {
    "id": "JxyyeQcNE64C"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VoXJmzqFE64C",
   "metadata": {
    "id": "VoXJmzqFE64C"
   },
   "source": [
    "#### Recall\n",
    "\n",
    "For a particular class $c$, what proportion of the test items that have that class did the model correctly predict to have that class?\n",
    "\n",
    "$$\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}$$\n",
    "\n",
    "Similar definitions for micro- and macro-average recall can be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdx8i1tUE64C",
   "metadata": {
    "id": "bdx8i1tUE64C"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "idHJcBLoE64C",
   "metadata": {
    "id": "idHJcBLoE64C"
   },
   "source": [
    "#### F1\n",
    "\n",
    "For a class $c$, what is the [harmonic mean](https://en.wikipedia.org/wiki/Harmonic_mean) of precision and recall?\n",
    "\n",
    "$$F_1\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{2}{\\frac{1}{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)} + \\frac{1}{\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}} = 2\\frac{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)\\;\\cdot\\;\\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}{\\mathrm{precision}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) + \\mathrm{recall}\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right)}$$ \n",
    "\n",
    "To define micro- and macro-average $F_1$ it can be useful to reexpress it.\n",
    "\n",
    "$$F_1\\left(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c\\right) = \\frac{2\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}{2\\mathrm{tp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fp}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c) + \\mathrm{fn}(\\hat{\\mathbf{y}}^\\mathrm{test}_i, \\mathbf{y}^\\mathrm{test}, c)}$$\n",
    "\n",
    "Definitions similar to those for precision can be given for micro- and macro-average $F_1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8rdl0iLbE64D",
   "metadata": {
    "id": "8rdl0iLbE64D"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DEpwS_t8E64D",
   "metadata": {
    "id": "DEpwS_t8E64D"
   },
   "source": [
    "## Task 3\n",
    "\n",
    "Define an instance method `NaturalLogic.load_wordnet` that constructs an editor library from WordNet hypernymy, hyponymy, and antonymy relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QIKTZA4lE64D",
   "metadata": {
    "id": "QIKTZA4lE64D"
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bT_2UB2GE64D",
   "metadata": {
    "id": "bT_2UB2GE64D"
   },
   "outputs": [],
   "source": [
    "class NaturalLogic(NaturalLogic):\n",
    "\n",
    "    def load_wordnet(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    @classmethod\n",
    "    def from_wordnet(cls):\n",
    "        natlog = cls()\n",
    "        natlog.load_wordnet()\n",
    "        \n",
    "        return natlog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0Mv_NnVME64D",
   "metadata": {
    "id": "0Mv_NnVME64D"
   },
   "source": [
    "Test your new library by writing examples that require knowledge of hypernymy, hyponymy, and antonymy to correctly handle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aG7QdCX5E64D",
   "metadata": {
    "id": "aG7QdCX5E64D"
   },
   "outputs": [],
   "source": [
    "# write tests here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mmCEFr6cE64D",
   "metadata": {
    "id": "mmCEFr6cE64D"
   },
   "source": [
    "Evaluate your new library on FraCaS by computing precision, recall, and F1 for the items that are either labeled \"yes\", \"no\", or \"don't know\". Remember that this is going to require you to define a way of mapping inference types to answers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yjz1hjjME64D",
   "metadata": {
    "id": "yjz1hjjME64D"
   },
   "outputs": [],
   "source": [
    "# write evaluation here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jTRdluaUE64D",
   "metadata": {
    "id": "jTRdluaUE64D"
   },
   "source": [
    "These numbers will be bad. The point is to see that handling even the apparently simple cases in FraCaS is very difficult, even with a fairly extensive edit library. PArt of the reason for this is that we are not handling quantification or negation at all.\n",
    "\n",
    "## Task 4\n",
    "\n",
    "Update your implementation of `NaturalLogic.__call__` to correctly handle negation and the quantifiers discussed in Section 5 of M&M. Assume that \"a\" behaves as \"some\"; that \"all\" and \"each\" behave like \"every\"; that \"not all\" behaves like \"not every\"; and that \"none\" behaves like \"no\". (There are many quantifiers this won't cover—e.g. \"most\", \"many\", etc.—don't worry about trying to figure out what the projectivity signatures for these looks like.)\n",
    "\n",
    "To do this, you will need to identify the first and second arguments of the quantifier. For instance, for (3), the first argument of \"every\" is \"virtuosic synthesist\" and the second argument is \"loves a grehound\". (If you've taken semantics, you know I'm fudging a little here.)\n",
    "\n",
    "3. Every virtuosic synthesist loves some grehyound.\n",
    "\n",
    "I have provided an implementation below. This implementation will only work for simple cases, like (3). Finding the arguments of quantifiers in general can be highly nontrivial for reason you'll need to take a formal semantics course to truly appreciate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a6304d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in /opt/homebrew/lib/python3.11/site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in /opt/homebrew/lib/python3.11/site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from stanza) (1.25.2)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in /opt/homebrew/lib/python3.11/site-packages (from stanza) (4.25.1)\n",
      "Requirement already satisfied: requests in /opt/homebrew/lib/python3.11/site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /opt/homebrew/lib/python3.11/site-packages (from stanza) (2.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/lib/python3.11/site-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in /Users/awhite48/Library/Python/3.11/lib/python/site-packages (from torch>=1.3.0->stanza) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (4.7.1)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/lib/python3.11/site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/lib/python3.11/site-packages (from requests->stanza) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/lib/python3.11/site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/lib/python3.11/site-packages (from requests->stanza) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/awhite48/Library/Python/3.11/lib/python/site-packages (from requests->stanza) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/lib/python3.11/site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/lib/python3.11/site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "aaxVcBQ2E64D",
   "metadata": {
    "id": "aaxVcBQ2E64D"
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from itertools import permutations\n",
    "\n",
    "from enum import Enum\n",
    "\n",
    "class Monotonicity(Enum):\n",
    "    UPWARD = 'upward'\n",
    "    DOWNWARD = 'downward'\n",
    "    NON = 'non-monotone'\n",
    "\n",
    "class ArgumentType(Enum):\n",
    "    RESTRICTOR = \"restrictor\"\n",
    "    SCOPE = \"scope\"\n",
    "\n",
    "class Projection:\n",
    "    \n",
    "    projection_map: dict[Monotonicity, tuple[str, str]] = {\n",
    "        Monotonicity.UPWARD: {},\n",
    "        Monotonicity.DOWNWARD: {\n",
    "            '⊏': '⊐',\n",
    "            '⊐': '⊏',\n",
    "        },\n",
    "        Monotonicity.NON: {\n",
    "            '⊏': '|',\n",
    "            '⊐': '|',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def __init__(self, restrictor: Monotonicity | None, scope: Monotonicity):\n",
    "        self.restrictor = restrictor\n",
    "        self.scope = scope\n",
    "\n",
    "        if self.restrictor is not None:\n",
    "            self.restrictor_projection = self.projection_map[self.restrictor]\n",
    "        else:\n",
    "            self.restrictor_projection = None\n",
    "\n",
    "        self.scope_projection = self.projection_map[self.scope]\n",
    "\n",
    "    def __call__(self, inference_type: str, argument_type: ArgumentType) -> str:\n",
    "        if argument_type == ArgumentType.RESTRICTOR:\n",
    "            if self.restrictor_projection is None:\n",
    "                raise ValueError(\"Restrictor projection is not defined.\")\n",
    "            elif inference_type in self.restrictor_projection:\n",
    "                return self.restrictor_projection[inference_type]\n",
    "            else:\n",
    "                return inference_type\n",
    "        else:\n",
    "            if inference_type in self.scope_projection:\n",
    "                return self.scope_projection[inference_type]\n",
    "            else:\n",
    "                return inference_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "e5c2b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ScopeTaker(Enum):\n",
    "\n",
    "    @classmethod\n",
    "    def members(cls) -> list[str]:\n",
    "        return [m.value for m in cls.__members__.values()]\n",
    "\n",
    "class TwoPlaceQuantifier(ScopeTaker):\n",
    "    EVERY = \"every\"\n",
    "    ALL = \"all\"\n",
    "    EACH = \"each\"\n",
    "    SOME = \"some\"\n",
    "    A = \"a\"\n",
    "    AN = \"an\"\n",
    "    NO = \"no\"\n",
    "    NONE = \"none\"\n",
    "    ANY = \"any\"\n",
    "    MOST = \"most\"\n",
    "    MANY = \"many\"\n",
    "    FEW = \"few\"\n",
    "    SEVERAL = \"several\"\n",
    "\n",
    "class OnePlaceQuantifier(ScopeTaker):\n",
    "    SOMEONE = \"someone\"\n",
    "    SOMEBODY = \"somebody\"\n",
    "    SOMETHING = \"something\"\n",
    "    ANYONE = \"anyone\"\n",
    "    ANYBODY = \"anybody\"\n",
    "    SOMEWHERE = \"somewhere\"\n",
    "    ANYWHERE = \"anywhere\"\n",
    "    EVERYONE = \"everyone\"\n",
    "    EVERYBODY = \"everybody\"\n",
    "    EVERYTHING = \"everything\"\n",
    "    NOONE = \"noone\"\n",
    "    NOBODY = \"nobody\"\n",
    "    NOTHING = \"nothing\"\n",
    "\n",
    "class Negation(ScopeTaker):\n",
    "    NOT = \"not\"\n",
    "    NEVER = \"never\"\n",
    "    NT = \"n't\"\n",
    "    NEITHER = \"neither\"\n",
    "    NOR = \"nor\"\n",
    "    NONE = \"none\"\n",
    "\n",
    "projection_rules = {\n",
    "    # two-place universal quantifiers: downward in scope, upward in restrictor\n",
    "    TwoPlaceQuantifier.EVERY: Projection(Monotonicity.DOWNWARD, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.ALL: Projection(Monotonicity.DOWNWARD, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.EACH: Projection(Monotonicity.DOWNWARD, Monotonicity.UPWARD),\n",
    "    \n",
    "    # two-place existential quantifiers: upward in both arguments\n",
    "    TwoPlaceQuantifier.SOME: Projection(Monotonicity.UPWARD, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.A: Projection(Monotonicity.UPWARD, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.AN: Projection(Monotonicity.UPWARD, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.ANY: Projection(Monotonicity.UPWARD, Monotonicity.UPWARD),\n",
    "    \n",
    "    # two-placenegative quantifiers: downward in both arguments\n",
    "    TwoPlaceQuantifier.NO: Projection(Monotonicity.DOWNWARD, Monotonicity.DOWNWARD),\n",
    "    TwoPlaceQuantifier.NONE: Projection(Monotonicity.DOWNWARD, Monotonicity.DOWNWARD),\n",
    "    \n",
    "    # two-place non-monotone quantifiers\n",
    "    TwoPlaceQuantifier.MOST: Projection(Monotonicity.NON, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.MANY: Projection(Monotonicity.NON, Monotonicity.UPWARD),\n",
    "    TwoPlaceQuantifier.FEW: Projection(Monotonicity.NON, Monotonicity.DOWNWARD),\n",
    "    TwoPlaceQuantifier.SEVERAL: Projection(Monotonicity.NON, Monotonicity.UPWARD),\n",
    "    \n",
    "    # one-place quantifiers\n",
    "    OnePlaceQuantifier.SOMEONE: Projection(None, Monotonicity.UPWARD),\n",
    "    OnePlaceQuantifier.ANYBODY: Projection(None, Monotonicity.UPWARD),\n",
    "    OnePlaceQuantifier.EVERYONE: Projection(None, Monotonicity.UPWARD),\n",
    "    OnePlaceQuantifier.NOONE: Projection(None, Monotonicity.DOWNWARD),\n",
    "    OnePlaceQuantifier.NOTHING: Projection(None, Monotonicity.DOWNWARD),\n",
    "    \n",
    "    # negation\n",
    "    Negation.NOT: Projection(None, Monotonicity.DOWNWARD),\n",
    "    Negation.NEVER: Projection(None, Monotonicity.DOWNWARD),\n",
    "    Negation.NT: Projection(None, Monotonicity.DOWNWARD),\n",
    "    Negation.NEITHER: Projection(None, Monotonicity.DOWNWARD),\n",
    "    Negation.NOR: Projection(None, Monotonicity.DOWNWARD),\n",
    "    Negation.NONE: Projection(None, Monotonicity.DOWNWARD),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "02c3980d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanza\n",
    "\n",
    "from stanza.models.common.doc import Word\n",
    "\n",
    "MRSNodeType = TwoPlaceQuantifier | OnePlaceQuantifier | Negation | ArgumentType\n",
    "\n",
    "class MRSNode:\n",
    "    def __init__(\n",
    "        self, \n",
    "        word: Word,\n",
    "        node_type: MRSNodeType,\n",
    "        restrictor: list['MRSNode'] = [],\n",
    "        scope: list['MRSNode'] = [],\n",
    "        parent_type: Optional[MRSNodeType] = None\n",
    "    ):\n",
    "        self.word = word\n",
    "        self.node_type = node_type\n",
    "        self.restrictor: list['MRSNode'] = restrictor\n",
    "        self.scope: list['MRSNode'] = scope\n",
    "        self.parent_type: Optional[MRSNodeType] = parent_type\n",
    "        \n",
    "        self.restrictor_frozen = False\n",
    "        self.scope_frozen = False\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f'MRSNode(word={self.word.text}, type={self.node_type})'\n",
    "\n",
    "    # def __repr__(self):\n",
    "    #     if isinstance(self.node_type, TwoPlaceQuantifier):\n",
    "    #         restrictor = ', '.join(str(n) for n in self.restrictor) \n",
    "    #         scope = ', '.join(str(n) for n in self.scope)\n",
    "\n",
    "    #         return f'{self.word.text} -> ({restrictor}, {scope})'\n",
    "        \n",
    "    #     elif isinstance(self.node_type, OnePlaceQuantifier):\n",
    "    #         scope = ', '.join(str(n) for n in self.scope)\n",
    "            \n",
    "    #         return f'{self.word.text} -> (_, {scope})'\n",
    "        \n",
    "    #     elif isinstance(self.node_type, Negation):\n",
    "    #         scope = ', '.join(str(n) for n in self.scope)\n",
    "\n",
    "    #         return f'{self.word.text} -> ({scope})'\n",
    "        \n",
    "    #     else:\n",
    "    #         return f'{self.word.text}'\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (self.word, self.node_type, \n",
    "             hash(tuple(self.restrictor)),\n",
    "             hash(tuple(self.scope)))\n",
    "        )\n",
    "\n",
    "    def __eq__(self, other: 'MRSNode'):\n",
    "        return hash(self) == hash(other)\n",
    "\n",
    "    def add_restrictor_element(self, child: 'MRSNode'):\n",
    "        if self.restrictor_frozen:\n",
    "            raise ValueError(\"Restrictor is frozen.\")\n",
    "        \n",
    "        if child.parent_type is not None:\n",
    "            raise ValueError(\"Child already has a parent.\")\n",
    "        \n",
    "        if isinstance(self.node_type, ArgumentType):\n",
    "            raise ValueError(\"Non-scope taker cannot have a restrictor.\")\n",
    "\n",
    "        # if child in self.restrictor:\n",
    "        #     raise ValueError(\"Child already in restrictor.\")\n",
    "\n",
    "        self.restrictor.append(child)\n",
    "        \n",
    "        child.parent_type = self.node_type\n",
    "\n",
    "    def add_scope_element(self, child: 'MRSNode'):\n",
    "        if self.scope_frozen:\n",
    "            raise ValueError(\"Scope is frozen.\")\n",
    "        \n",
    "        if child.parent_type is not None:\n",
    "            raise ValueError(\"Child already has a parent.\")\n",
    "        \n",
    "        if isinstance(self.node_type, ArgumentType):\n",
    "            raise ValueError(\"Non-scope taker cannot have a restrictor.\")\n",
    "\n",
    "        # if child in self.scope:\n",
    "        #     raise ValueError(\"Child already in scope.\")\n",
    "\n",
    "        self.scope.append(child)\n",
    "        \n",
    "        child.parent_type = self.node_type\n",
    "        \n",
    "    def copy(self):\n",
    "        \"\"\"Create a deep copy of the node and its children\"\"\"\n",
    "\n",
    "        new_node = MRSNode(self.word, self.node_type)\n",
    "\n",
    "        for child in self.children:\n",
    "            child_copy = child.copy()\n",
    "            new_node.add_child(child_copy)\n",
    "\n",
    "        return new_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "2860e720",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 11:53:56 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f23308b8ddd94101b4578d529ad5ff0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 11:53:58 INFO: Loading these models for language: en (English):\n",
      "======================================\n",
      "| Processor    | Package             |\n",
      "--------------------------------------\n",
      "| tokenize     | combined            |\n",
      "| pos          | combined_charlm     |\n",
      "| lemma        | combined_nocharlm   |\n",
      "| constituency | ptb3-revised_charlm |\n",
      "| depparse     | combined_charlm     |\n",
      "| sentiment    | sstplus             |\n",
      "| ner          | ontonotes_charlm    |\n",
      "======================================\n",
      "\n",
      "2025-02-24 11:53:58 INFO: Using device: cpu\n",
      "2025-02-24 11:53:58 INFO: Loading: tokenize\n",
      "2025-02-24 11:53:58 INFO: Loading: pos\n",
      "2025-02-24 11:53:58 INFO: Loading: lemma\n",
      "2025-02-24 11:53:58 INFO: Loading: constituency\n",
      "2025-02-24 11:53:58 INFO: Loading: depparse\n",
      "2025-02-24 11:53:58 INFO: Loading: sentiment\n",
      "2025-02-24 11:53:58 INFO: Loading: ner\n",
      "2025-02-24 11:53:59 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "from functools import lru_cache\n",
    "\n",
    "quantifiers = (\n",
    "    TwoPlaceQuantifier.members() + \n",
    "    OnePlaceQuantifier.members()\n",
    ")\n",
    "\n",
    "scope_takers = quantifiers + Negation.members()\n",
    "\n",
    "class DependencyParse:\n",
    "    \n",
    "    parser = stanza.Pipeline('en')\n",
    "    \n",
    "    def __init__(self, sentence: str):\n",
    "        self.parsed_sentence: list[Word] = self.parser(sentence).sentences[0].words\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f'DependencyParse(sentence=\"{self.parsed_sentence}\")'\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._repr_helper(self.root)\n",
    "\n",
    "    def _repr_helper(self, word: Word) -> str:\n",
    "        if self.children_of(word):\n",
    "            children_repr = ', '.join(\n",
    "                self._repr_helper(child) for child in self.children_of(word)\n",
    "            )\n",
    "            return f\"{word.text} -> ({children_repr})\"\n",
    "        else:\n",
    "            return f\"{word.text}\"\n",
    "\n",
    "    @property\n",
    "    def root(self) -> dict[str, Union[int, str]]:\n",
    "        return [\n",
    "            word for word in self.parsed_sentence\n",
    "            if word.head == 0\n",
    "        ][0]\n",
    "\n",
    "    def parent_of(self, idx: int) -> dict[str, Union[int, str]]:\n",
    "        paridx = self.parsed_sentence[idx].head\n",
    "        \n",
    "        # correct for 1-indexing\n",
    "        return self.parsed_sentence[paridx-1]\n",
    "    \n",
    "    @lru_cache(256)\n",
    "    def children_of(self, word: Word, closure: bool=False) -> list[Word]:\n",
    "        # not correcting for 1-indexing\n",
    "        immediate_children = [\n",
    "            w for w in self.parsed_sentence\n",
    "            if w.head == word.id\n",
    "        ]\n",
    "        \n",
    "        if closure:\n",
    "            return (\n",
    "                immediate_children +\n",
    "                [w for child in immediate_children \n",
    "                 for w in self.children_of(child, closure)]\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            return immediate_children\n",
    "    \n",
    "    def get_subtree(self, root_word: Word) -> list[dict]:\n",
    "        \"\"\"Get all words in the subtree rooted at the given index.\"\"\"\n",
    "        return [root_word] + self.children_of(root_word, closure=True)\n",
    "    \n",
    "    def _construct_mrs_tree(\n",
    "        self, word: Word,\n",
    "        parent_word: Optional[Word] = None,\n",
    "        parent_word_is_predicate: bool = False,\n",
    "        parent_node: Optional['MRSNode'] = None\n",
    "    ) -> list[MRSNode]:\n",
    "\n",
    "        print(word.lemma)\n",
    "\n",
    "        pred_dependencies = [\n",
    "            \"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\", \n",
    "            \"obj\", \"dobj\", \"iobj\", \"ccomp\", \"xcomp\"\n",
    "        ]\n",
    "\n",
    "        word_is_predicate = any(\n",
    "            child.deprel in pred_dependencies\n",
    "            for child in self.children_of(word)\n",
    "        )\n",
    "\n",
    "        # example: \"every person who has a cat and who has a dog [left]\"\n",
    "        if word_is_predicate:\n",
    "            curr_node = MRSNode(word, ArgumentType.SCOPE)\n",
    "\n",
    "            nodes = [\n",
    "                n for w in self.children_of(word)\n",
    "                for n in self._construct_mrs_tree(\n",
    "                    word=w,\n",
    "                    parent_word=word,\n",
    "                    parent_word_is_predicate=True\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            nonscope_nodes = [\n",
    "                n for n in nodes \n",
    "                if isinstance(n.node_type, ArgumentType) or\n",
    "                n.scope_frozen\n",
    "            ] + [curr_node]\n",
    "            scope_nodes = [\n",
    "                n for n in nodes \n",
    "                if not isinstance(n.node_type, ArgumentType) \n",
    "                and not n.scope_frozen\n",
    "            ]\n",
    "\n",
    "            if not scope_nodes:\n",
    "                return nonscope_nodes\n",
    "            else:\n",
    "                # add all non-scope and frozen scope nodes to the lowest \n",
    "                # unfrozen node\n",
    "                for n in nonscope_nodes:\n",
    "                    scope_nodes[-1].add_scope_element(n)\n",
    "\n",
    "                # add the unfrozen nodes to each other in surface scope \n",
    "                # order\n",
    "                for i, n in reversed(list(enumerate(scope_nodes[:-1]))):\n",
    "                    n.add_scope_element(scope_nodes[i+1])\n",
    "\n",
    "                    if i > 0:\n",
    "                        n.scope_frozen = True\n",
    "\n",
    "                return scope_nodes[:1]\n",
    "\n",
    "        elif word.lemma in quantifiers:\n",
    "            if word.lemma in TwoPlaceQuantifier.members():\n",
    "                QType = TwoPlaceQuantifier\n",
    "            else:\n",
    "                QType = OnePlaceQuantifier\n",
    "\n",
    "            # Check negation on quantifiers\n",
    "            # \"[not] every person who has a cat and who has a dog\"\n",
    "            if not parent_word_is_predicate:\n",
    "                neg_nodes = [\n",
    "                    n for child in self.children_of(parent_word)\n",
    "                    if child.lemma in Negation.members()\n",
    "                    for n in self._construct_mrs_tree(\n",
    "                        word=child,\n",
    "                        parent_word=parent_word,\n",
    "                        parent_word_is_predicate=False\n",
    "                    )\n",
    "                ]\n",
    "\n",
    "                if len(neg_nodes) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"Expected zero or one negation node for a quantifier, \"\n",
    "                        f\"got {len(neg_nodes)}\"\n",
    "                    )\n",
    "                elif len(neg_nodes) == 1:\n",
    "                    neg_node = neg_nodes[0]\n",
    "\n",
    "                    neg_node = MRSNode(\n",
    "                        parent_word,\n",
    "                        Negation(parent_word.lemma)\n",
    "                    )\n",
    "\n",
    "                    node = MRSNode(\n",
    "                        word,\n",
    "                        QType(word.lemma)\n",
    "                    )\n",
    "\n",
    "                    neg_node.add_scope_element(node)\n",
    "\n",
    "                    return [neg_node]\n",
    "                \n",
    "                else:\n",
    "                    node = MRSNode(\n",
    "                        word, \n",
    "                        QType(word.lemma)\n",
    "                    )\n",
    "\n",
    "                    return [node]\n",
    "            else:\n",
    "                node = MRSNode(\n",
    "                    word, \n",
    "                    QType(word.lemma), \n",
    "                    parent=parent_node\n",
    "                )\n",
    "\n",
    "                return [node]\n",
    "                \n",
    "        elif word.lemma in Negation.members():\n",
    "            node = MRSNode(\n",
    "                word, \n",
    "                Negation(word.lemma)\n",
    "            )\n",
    "\n",
    "            return [node]\n",
    "    \n",
    "        else:\n",
    "            # get determiners on this nominal (e.g. \"every\")\n",
    "            direct_nodes = [\n",
    "                n for w in self.children_of(word)\n",
    "                if w.lemma in TwoPlaceQuantifier.members()\n",
    "                for n in self._construct_mrs_tree(\n",
    "                    word=w,\n",
    "                    parent_word=word,\n",
    "                    parent_word_is_predicate=False\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            # we only know how to handle one, even though two are possible:\n",
    "            # \"[all] or [most] people who have a cat and who have a dog\"\n",
    "            if len(direct_nodes) > 1:\n",
    "                raise ValueError(\n",
    "                    \"Expected zero or one direct node for a noun, \"\n",
    "                    f\"got {len(direct_nodes)}\"\n",
    "                )\n",
    "\n",
    "            elif len(direct_nodes) == 1:\n",
    "                quantifier = quantifier_to_modify = direct_nodes[0]\n",
    "\n",
    "                # if we have a negated quantifier\n",
    "                # \"[not every] person who has a cat and who has a dog\"\n",
    "                is_negated_quantifier = (\n",
    "                    quantifier.parent_type is not None and \n",
    "                    isinstance(quantifier.parent_type, Negation)\n",
    "                )\n",
    "\n",
    "                if is_negated_quantifier:\n",
    "                    # add the restrictor of the negated quantifier\n",
    "                    quantifier_to_modify = quantifier.scope[0]\n",
    "\n",
    "            else:\n",
    "                quantifier = quantifier_to_modify = None\n",
    "\n",
    "            # get scope-takers within this nominal (if any):\n",
    "            # \"every person who has [a] cat and who has [a] dog\"\n",
    "            recursive_nodes = [\n",
    "                n for w in self.children_of(word)\n",
    "                if w.lemma not in scope_takers\n",
    "                for n in self._construct_mrs_tree(\n",
    "                    word=w,\n",
    "                    parent_word=word,\n",
    "                    parent_word_is_predicate=False\n",
    "                )\n",
    "            ]\n",
    "\n",
    "            recursive_nonscope_takers = [\n",
    "                n for n in recursive_nodes\n",
    "                if isinstance(n.node_type, ArgumentType) or\n",
    "                n.scope_frozen\n",
    "            ]\n",
    "            recursive_scope_takers = [\n",
    "                n for n in recursive_nodes\n",
    "                if not isinstance(n.node_type, ArgumentType) and\n",
    "                not n.scope_frozen\n",
    "            ]\n",
    "\n",
    "            # if there are any scope-takers internal to the argument, \n",
    "            # add the argument itself to them\n",
    "            if recursive_scope_takers:\n",
    "                # example: \"every [person] who has a cat and who has a dog\"\n",
    "                curr_node = MRSNode(\n",
    "                    word,\n",
    "                    ArgumentType.SCOPE\n",
    "                )\n",
    "\n",
    "                # add the current node to the scope of each of those \n",
    "                # scope-takers\n",
    "                for n in recursive_scope_takers:\n",
    "                    for nonscope_taker in recursive_nonscope_takers:\n",
    "                        n.add_scope_element(nonscope_taker)\n",
    "\n",
    "                # in case there is a quantifier on this nominal:\n",
    "                # \"[every] person who has a cat and who has a dog\"\n",
    "                \n",
    "                if quantifier_to_modify is not None:\n",
    "                    for n in recursive_scope_takers:\n",
    "                        n.scope_frozen = True\n",
    "                        quantifier_to_modify.add_restrictor_element(n)\n",
    "\n",
    "                    quantifier_to_modify.restrictor_frozen = True\n",
    "\n",
    "                    return [quantifier]\n",
    "                \n",
    "                # otherwise, this nominal is just an argument:\n",
    "                # \"the [person] who has a cat and who has a dog\"\n",
    "                else:\n",
    "                    return recursive_nodes\n",
    "            \n",
    "            # example: \"every person\"\n",
    "            elif quantifier_to_modify is not None:\n",
    "                # example: \"every [person] who has a cat and who has a dog\"\n",
    "                curr_node = MRSNode(\n",
    "                    word,\n",
    "                    ArgumentType.RESTRICTOR\n",
    "                )\n",
    "\n",
    "                print(quantifier_to_modify.restrictor)\n",
    "\n",
    "                quantifier_to_modify.add_restrictor_element(curr_node)\n",
    "                quantifier_to_modify.restrictor_frozen = True\n",
    "\n",
    "                return [quantifier]\n",
    "\n",
    "            # example: \"the person\"\n",
    "            else:\n",
    "                curr_node = MRSNode(\n",
    "                    word,\n",
    "                    ArgumentType.SCOPE\n",
    "                )\n",
    "\n",
    "                return [curr_node]\n",
    "\n",
    "    @property\n",
    "    def mrs_tree(self) -> list[MRSNode]:\n",
    "        tree = self._construct_mrs_tree(self.root)\n",
    "\n",
    "        if len(tree) > 1:   \n",
    "            raise ValueError(\n",
    "                \"Expected zero or one root node, \"\n",
    "                f\"got {len(tree)}\"\n",
    "            )\n",
    "\n",
    "        return tree[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z2GphPtqE64D",
   "metadata": {
    "id": "Z2GphPtqE64D"
   },
   "source": [
    "Note that this implementation produces second arguments for one quantifier that can overlap with the first arguments of another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "P6iCHfzuE64D",
   "metadata": {
    "id": "P6iCHfzuE64D"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pet\n",
      "synthesist\n",
      "every\n",
      "virtuosic\n",
      "[MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR), MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR), MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR)]\n",
      "do\n",
      "not\n",
      "greyhound\n",
      "a\n",
      "[MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR), MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR), MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR), MRSNode(word=greyhound, type=ArgumentType.RESTRICTOR), MRSNode(word=synthesist, type=ArgumentType.RESTRICTOR)]\n",
      ".\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[MRSNode(word=did, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=., type=ArgumentType.SCOPE),\n",
       " MRSNode(word=pet, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=a, type=TwoPlaceQuantifier.A),\n",
       " MRSNode(word=n't, type=Negation.NOT),\n",
       " MRSNode(word=did, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=., type=ArgumentType.SCOPE),\n",
       " MRSNode(word=pet, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=a, type=TwoPlaceQuantifier.A),\n",
       " MRSNode(word=n't, type=Negation.NOT),\n",
       " MRSNode(word=did, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=., type=ArgumentType.SCOPE),\n",
       " MRSNode(word=pet, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=a, type=TwoPlaceQuantifier.A),\n",
       " MRSNode(word=n't, type=Negation.NOT),\n",
       " MRSNode(word=did, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=., type=ArgumentType.SCOPE),\n",
       " MRSNode(word=pet, type=ArgumentType.SCOPE),\n",
       " MRSNode(word=a, type=TwoPlaceQuantifier.A),\n",
       " MRSNode(word=n't, type=Negation.NOT)]"
      ]
     },
     "execution_count": 613,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the sentence and get quantifier arguments\n",
    "parse = DependencyParse('Every virtuosic synthesist didn\\'t pet a greyhound.')\n",
    "#len(parse.mrs_tree[0].scope)\n",
    "\n",
    "parse.mrs_tree.scope\n",
    "\n",
    "#mrs_tree[0].root.children[2].children[1].children\n",
    "\n",
    "# print('First arguments (restrictors):\\n')\n",
    "# for quant_idx, words in first_args.items():\n",
    "#     print(f'Quantifier at position {quant_idx}:')\n",
    "#     print([w.text for w in words])\n",
    "# print('\\n')\n",
    "\n",
    "# print('Second arguments (nuclear scopes):\\n') \n",
    "# for quant_idx, words in second_args.items():\n",
    "#     print(f'Quantifier at position {quant_idx}:')\n",
    "#     print([w.text for w in words])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "L0MvyFtOE64D",
   "metadata": {
    "id": "L0MvyFtOE64D"
   },
   "source": [
    "This overlap means that you are going to need to choose an order in which to project the inference relations through the quantifiers—e.g. in (3), do you first consider \"every\", then \"a\"; or vice versa. This order will necessarily be heuristic. A reasonable order might be the reverse of the linear (or \"surface\") order. Other options might be based on depth in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1OIROsCzE64E",
   "metadata": {
    "id": "1OIROsCzE64E"
   },
   "outputs": [],
   "source": [
    "class NaturalLogic(NaturalLogic):\n",
    "\n",
    "    def __call__(self, premise: list[str], hypothesis: list[str]) -> tuple[list[InferencePath], \n",
    "                                                                           set[Inference]]:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Uzd6dLZzE64E",
   "metadata": {
    "id": "Uzd6dLZzE64E"
   },
   "source": [
    "Apply the same evaluation you developed for Task 3 to your new implementation of `NaturalLogic.__call__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I5sa8co9E64E",
   "metadata": {
    "id": "I5sa8co9E64E"
   },
   "outputs": [],
   "source": [
    "# write evaluation here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
