{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQuarsHBeQM8pDKYauEq5Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["In Module 0, we mainly focused on setting up the foundations for analyzing languages as formal objects. We defined a language $L$ on an alphabet $\\Sigma$ as a subset of the set of strings $\\Sigma^* = \\bigcup_{i=0}^\\infty \\Sigma^i$ on $\\Sigma$ (i.e. $L \\in 2^{\\Sigma^*}$). We explored one way of describing languages in the form of regular expressions on $\\Sigma$ and we discussed one way of describing a relation between strings in the form of minimum edit distance. \n","\n","(In fact, we decribed minimum edit distance as a relation from tuples of strings to distances; but as we explored in Assignment 3, we can also think of a distance, in the context of a particular weighting on edit operations, as a relation on strings: namely, a relation on strings that are \\{at most, exactly, at least, ...\\} that distance apart. This idea extends the notion of a [ball](https://en.wikipedia.org/wiki/Ball_(mathematics)) or [sphere](https://en.wikipedia.org/wiki/Sphere), depending on the constraint, to strings.)\n","\n","# Levels of Abstraction\n","\n","What we will now begin to do is to deploy these tools for describing and expressing generalizations about possible languages. To do this, we are going to need to start thinking in terms of sets of sets of languages. That is, we will begin to think about subsets $\\mathcal{L}$ of the set of languages $2^{\\Sigma^*}$ on $\\Sigma$. This approach will yield four important levels of abstraction:\n","\n","1. Primitive (unanalyzed) elements $\\sigma \\in \\Sigma$\n","2. Collections of primitive elements: sets $\\Sigma$ or sequences $\\boldsymbol\\sigma \\in \\Sigma^*$\n","3. Languages: collections of sequences of primitive elements $L \\in 2^{\\Sigma^*}$\n","4. Collections of languages $\\mathcal{L} \\subseteq 2^{\\Sigma^*}$\n","\n","We are going to be particularly interested in collections of languages that share some interesting properties. We will call such collections *classes of languages* (or *families of languages*), and we will be interested in ways of compactly describing those classes that leverage the property shared by languages in the class. We will call compact descriptions of a particular language grammars and collections thereof classes of grammars $\\mathcal{G}$. \n","\n","# Generation\n","\n","We will characterize the relationship between grammars and languages as one of *generation*: a grammars $G$ generates a language $L$ if $G$ is a description of that language. To express that $G$ generates $L$, we will say that $\\mathbb{L}(G) = L$, where $\\mathbb{L}$ is some function from grammars to languages. (It is important to note that generation sounds like a [procedural](https://en.wikipedia.org/wiki/Procedural_programming) concept, but it is really [declarative](https://en.wikipedia.org/wiki/Declarative_programming). Know that $G$ generates a language $L$ does not require us to know how to build $L$ from $G$.)\n","\n","## Example: regular expressions\n","\n","We've already seen one kind of grammar under this definition: regulars expressions. Remember that regular expressions $R(\\Sigma)$ on $\\Sigma$ themselves are strings of a language on $\\Sigma \\cup\\{\\epsilon, \\emptyset, \\cup, \\circ, (, ), *\\}$. We formally define these strings recursively.\n","\n","$\\rho$ is a regular expression if and only if:\n","\n","1. $\\rho \\in \\Sigma \\cup \\{\\epsilon, \\emptyset\\}$\n","2. $\\rho$ is $(\\rho_1 \\cup \\rho_2)$ for some regular expressions $\\rho_1$ and $\\rho_2$\n","3. $\\rho$ is $(\\rho_1 \\circ \\rho_2)$ for some regular expressions $\\rho_1$ and $\\rho_2$\n","4. $\\rho$ is $\\rho_1^*$ for some regular expression $\\rho_1$\n","\n","We can generate all the regular expressions given an alphabet."],"metadata":{"id":"uENy9B09TpNm"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"3JJ6GAFpToLc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676911112070,"user_tz":300,"elapsed":4,"user":{"displayName":"Aaron Steven White","userId":"06256629009318567325"}},"outputId":"d686027b-b944-4624-ecda-e2d0e5c2dfab"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚àÖ\n","m\n","ùúñ\n","…ô\n","('‚àÖ', '*')\n","('m', '*')\n","('ùúñ', '*')\n","('…ô', '*')\n","('‚àÖ', '‚à™', '‚àÖ')\n","('‚àÖ', '‚à™', 'm')\n","('‚àÖ', '‚à™', 'ùúñ')\n","('‚àÖ', '‚à™', '…ô')\n"]}],"source":["def regular_expressions(sigma):\n","    old_regex_set = frozenset(sigma | {'‚àÖ', 'ùúñ'})\n","    \n","    for rho in old_regex_set:\n","        yield rho\n","    \n","    while True:\n","        new_regex_set = set(old_regex_set)\n","            \n","        for rho in old_regex_set:\n","            elem = (rho, '*')\n","            new_regex_set |= {elem}\n","            yield elem\n","            \n","        for rho1 in old_regex_set:\n","            for rho2 in old_regex_set:\n","                elem = (rho1, '‚à™', rho2)\n","                new_regex_set |= {elem}\n","                yield elem\n","                \n","        for rho1 in old_regex_set:\n","            for rho2 in old_regex_set:\n","                elem = (rho1, '‚àò', rho2)\n","                new_regex_set |= {elem}\n","                yield elem\n","                \n","        old_regex_set = frozenset(new_regex_set)\n","\n","for i, r in enumerate(regular_expressions({'…ô', 'm'})):\n","    print(r)\n","    \n","    if i > 10:\n","        break"]},{"cell_type":"markdown","source":["Regular expressions (so defined) *evaluate* to sets of strings on $\\Sigma$‚Äìi.e. languages on $\\Sigma$. Another way of thinking about this is that a regular expression on $\\Sigma$ *describes* a language on $\\Sigma$.\n","\n","We can define this evaluation procedure formally as a function $\\text{eval}: R(\\Sigma) \\rightarrow 2^{\\Sigma^*}$, where $R(\\Sigma)$ is the set of regular expressions on $\\Sigma$.\n","\n","$\\text{eval}(\\rho) = \\begin{cases}\\{\\} & \\text{if } \\rho = \\emptyset \\\\\\{\\_\\} & \\text{if } \\rho = \\epsilon \\\\ \\{\\rho\\} & \\text{if } \\rho \\in \\Sigma\\\\ \\text{eval}(\\rho_1) \\times \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\circ \\rho_2) \\\\ \\text{eval}(\\rho_1) \\cup \\text{eval}(\\rho_2) & \\text{if } \\rho = (\\rho_1 \\cup \\rho_2)\\\\  \\bigcup_{i = 0}^\\infty \\text{eval}(\\rho_1)^i & \\text{if } \\rho = \\rho_1^*\\\\ \\end{cases}$"],"metadata":{"id":"_d_URw5lr9UK"}},{"cell_type":"code","source":["def evaluate_regular_expression(regex):\n","    if regex == '‚àÖ':\n","        return\n","    \n","    elif regex == 'ùúñ':\n","        yield ''\n","    \n","    elif isinstance(regex, str):\n","        yield regex\n","    \n","    elif regex[1] == '*':\n","        i = 0\n","        while True:\n","            for s in evaluate_regular_expression(regex[0]):\n","                yield s*i\n","            \n","            i += 1\n","            \n","    elif regex[1] == '‚à™':\n","        for s1 in evaluate_regular_expression(regex[0]):\n","            yield s1\n","            \n","        for s2 in evaluate_regular_expression(regex[2]):\n","            yield s2\n","            \n","    elif regex[1] == '‚àò':\n","        for s1 in evaluate_regular_expression(regex[0]):\n","            for s2 in evaluate_regular_expression(regex[2]):\n","                yield s1 + s2\n","\n","for i, s in enumerate(evaluate_regular_expression(('…ô', '‚àò', ('m', '‚à™', 'g')))):\n","    if i < 10:\n","      print(s)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jj3YpAhSsGR-","executionInfo":{"status":"ok","timestamp":1676911226344,"user_tz":300,"elapsed":165,"user":{"displayName":"Aaron Steven White","userId":"06256629009318567325"}},"outputId":"d39d8586-99bb-431f-e13f-5fc7d061f354"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["…ôm\n","…ôg\n"]}]},{"cell_type":"markdown","source":["Each regular expression is thus a grammar; the set of all regular expressions is a class of grammars; and $\\text{eval}$ is an implementation of $\\mathbb{L}$. We will call the class of languages $\\mathcal{R}$ generated by the regular expressions the *regular languages*: $$\\mathcal{R} \\equiv \\mathbb{L}(R(\\Sigma)) = \\{\\mathbb{L}(r) \\mid r \\in R(\\Sigma)\\} = \\{\\text{eval}(r) \\mid r \\in R(\\Sigma)\\}$$\n","\n","We will be studying these languages in depth, because as I've mentiond before and as you will read about in Heinz 2018, it seems very likely that all phonological grammars of natural languages are a subset of this class. To study these languages, it will be useful to introduce a class of grammars that are equivalent to the regular expressions in that they generate exactly the same set of languages‚Äìi.e. the regular languages. These grammars are known as *finite state automata*.\n","\n","We will refer to this sort of equivalence‚Äìi.e. that $\\mathbb{L}(\\mathcal{G}_1) = \\mathbb{L}(\\mathcal{G}_2)$ for classes $\\mathcal{G}_1$ and $\\mathcal{G}_2$‚Äìas *weak equivalence* or *equivalence in weak generative capacity*. We will discuss another notion‚Äì*strong equivalence*‚Äìlater. First, though we should discuss the broader context.\n","\n","## The Generativist Conceit\n","\n","Why would we want to do all this? In the first lecture, I discussed what I called the *Generativist Conceit*: that we can understand natural language by studying the class of grammars that generate all + only the classes of natural languages. Though certain strong ideas associated with the Generativist Conceit are controversial, as I have stated it (relatively weakly) here, it is relatively uncontroversial: if our aim is to provide a scientific theory that characterizes natural language (whatever that might be: a colection of actual utterances, an aspect of human psychology or some abstract object with an existence independent of human minds), we at least need some method for encoding that theory in a way that makes testable predictions. Grammars are definitionally such a method as I've described them.\n","\n","Another relatively uncontroversial observation that drives the Generativist Conceit is that children must be able to start without a language and come to know one, and the one they come to know is determined in some important way from their environment (rather than their genetics) in a finite amount of time on the basis of a finite number of observations of actual language use. Why this observation is important is that it means that natural languages must be *learnable* from a finite number of examples. Grammars are usually finite objects that (may) describe countably infinite languages, and so they might be a useful tool for describing the process by which a child comes to know a language.\n","\n","### Learning\n","\n","More formally, we may say that children have some method $c$ for mapping sequences of strings (or *texts*) $\\mathbf{t} \\in \\Sigma^{**}$ along with some extralinguistic experience $E \\in \\mathcal{E}$ into a grammar $G \\in \\mathcal{G}$: $$c: \\Sigma^{**} \\times \\mathcal{E} \\rightarrow \\mathcal{G}$$. A child's extralinguistic experience is nontrivial to represent, so to make the problem more tractable, we will often ignore that part.\n","\n","To demonstrate that the problem of learning a grammar from a text, consider the following formal model, known as [*language identification in the limit*](https://en.wikipedia.org/wiki/Language_identification_in_the_limit), due to [Gold 1964](https://www.rand.org/pubs/research_memoranda/RM4136.html). \n","\n","We start from the idea that a *learner* should be modeled as a function $f: \\Sigma^{**} \\rightarrow 2^{\\Sigma^*}$ that maps an *environment* $E = \\langle \\boldsymbol\\sigma_1, \\boldsymbol\\sigma_2, \\ldots \\rangle \\in \\Sigma^{**}$ to a language $L$, where an environment is a sequence of strings in $L$. We will say that a learner $f$ learns a language $L$ given an *environment* (or *text*) $E \\subset L^*$ if the learner outputs $L$ after seeing enough examples from the environment and that it learns a language $L$ if it learns that language in any environment. That is, $f$ learns $L$ if it doesn't matter which sequence you give it: $f \\text{ learns } L \\leftrightarrow \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L$. A language family $\\mathcal{L}$ is learnable if there exists a language learner that can learn all languages in the family: $f \\text{ learns } \\mathcal{L} \\leftrightarrow \\forall L \\in \\mathcal{L}: \\forall \\mathbf{e} \\in L^*: f(\\mathbf{e}) = L$.\n","\n","[Gold (1967)](https://www.sciencedirect.com/science/article/pii/S0019995867911655) showed that, if a language family $\\mathcal{L}$ contains languages $L_1, L_2, \\ldots, L_\\infty$, such that $L_1 \\subset L_2 \\subset \\ldots \\subset L_\\infty \\bigcup _{i=1}^\\infty L_i$, then it is not learnable. Here's the idea behind the proof: suppose $f$ is a learner that can learn $L_1, L_2, \\ldots, L_\\infty$; we'll show that it cannot learn $L_\\infty$, by constructing an environment for $L_\\infty$ that \"tricks\" $f$. \n","\n","First, we construct environments $E_1, E_2, \\ldots$ such that $f(E_i) = L_i$. Next, we construct environment $E_\\infty$ for $L_\\infty$ inductively as follows:\n","\n","1. Present $f$ with $E_1$ until it outputs $L_{1}$.\n","2. Switch to presenting $f$ with an environment that alternates the rest of $E_1$ and the entirety of $E_2$. Since $L_1 \\subset L_2$, this concatenated environment is still an environment for $L_2$, so $f$ must eventually output $L_2$.\n","3. Switch to presenting the rest of $E_1 \\circ E_2$ and the entirety of $E_3$ alternatively. And so on for all $i$.\n","\n","By construction, the resulting environment $E$ contains the entirety of $E_{1},E_{2},\\ldots$, thus it contains $L_\\infty$, so it is an environment for $L_\\infty$. But since the learner always switches to $L_{i}$ for some finite $i$, it never converges to $L_{\\infty}$.\n","\n","The point here is that we need to be careful about the class of languages we pick out as possible ones the learner might select from. We'll explore this idea in the context of finite state automata, which are equivalent to regular expressions. Importantly, the full class of finite state automata have the problematic containment structure above, but the hope is that if we constrain these grammars in particular ways, we can avoid these sorts of issues while retaining coverage of natural languages.\n","\n","# Finite State Automata\n","\n","We'll start with an application of [FSAs to modeling licit English syllables](https://drive.google.com/file/d/1OreZ2dUdyDYRIKet7bzn617rYDG2NbIz/view?usp=share_link) to get a sense for what they look like, then we'll formalize them. To visualize FSAs, we'll use [`pynini`](https://www.openfst.org/twiki/bin/view/GRM/Pynini) (We'll also use `pynini` later when we define finite state transducers.)"],"metadata":{"id":"RE5npn1Mrhbg"}},{"cell_type":"code","source":["! pip install --only-binary :all: pynini"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rsJO-Fb8RBfb","executionInfo":{"status":"ok","timestamp":1677524796752,"user_tz":300,"elapsed":21158,"user":{"displayName":"Aaron Steven White","userId":"06256629009318567325"}},"outputId":"4defe7ae-eb0b-41c6-9cee-4fc595a0c0c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pynini\n","  Downloading pynini-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (161.5 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m161.5/161.5 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Cython>=0.29 in /usr/local/lib/python3.8/dist-packages (from pynini) (0.29.33)\n","Installing collected packages: pynini\n","Successfully installed pynini-2.1.5\n"]}]},{"cell_type":"code","source":["!pip install wurlitzer\n","\n","%load_ext wurlitzer"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2eCl9Tz3RpYY","executionInfo":{"status":"ok","timestamp":1677524931668,"user_tz":300,"elapsed":5322,"user":{"displayName":"Aaron Steven White","userId":"06256629009318567325"}},"outputId":"3040141b-dc9e-45ec-8c55-5445870d2c56"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wurlitzer\n","  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n","Installing collected packages: wurlitzer\n","Successfully installed wurlitzer-3.0.3\n"]}]},{"cell_type":"code","source":["import pynini"],"metadata":{"id":"eVaoTGjWRifj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## Deterministic Finite State Automaton\n","\n","A Deterministic Finite State Automaton (DFA) is a grammar with 5 components:\n","\n","1. A set of states $Q$\n","2. An alphabet $\\Sigma$\n","3. A transition function $\\delta : Q \\times \\Sigma \\rightarrow Q$\n","4. An initial state $q_0 \\in Q$\n","5. A set of final states $F \\subseteq Q$\n","\n","## Nondeterministic Finite State Automaton\n","\n","A Nondeterministic Finite State Automaton (NFA) is also a grammar with 5 components:\n","\n","1. A set of states $Q$\n","2. An alphabet $\\Sigma$\n","3. A transition function $\\delta : Q \\times (\\Sigma\\,\\color{red}{\\cup \\{\\epsilon\\}}) \\rightarrow \\color{red}{\\mathcal{P}(Q)}$\n","4. An initial state $q_0 \\in Q$\n","5. A set of final states $F \\subseteq Q$\n"],"metadata":{"id":"c0H_Cb87Q-Ux"}},{"cell_type":"code","source":["from copy import copy, deepcopy\n","from typing import Union, Optional, Dict, List\n","from functools import lru_cache\n","\n","class TransitionFunction:\n","    \"\"\"A finite state machine transition function\n","\n","    Parameters\n","    ----------\n","    transition_graph\n","\n","    Attributes\n","    ----------\n","    isdeterministic\n","    istotalfunction\n","    transition_graph\n","\n","    Methods\n","    -------\n","    validate(alphabet, states)\n","    relable_states(state_map)\n","    totalize()\n","    \"\"\"\n","\n","    def __init__(self, transition_graph: Dict[Tuple[str, str], Set[str]]):\n","        self._transition_graph = transition_graph\n","\n","    def __call__(self, state: str, symbol: str) -> Set[str]:\n","        try:\n","            return self._transition_graph[(state, symbol)]\n","        except KeyError:\n","            return set({})\n","\n","    def __or__(self, other):\n","        return TransitionFunction(dict(self._transition_graph, \n","                                       **other._transition_graph))\n","        \n","    def add_transitions(self, transition_graph):\n","        self._transition_graph.update(transition_graph)\n","\n","    def validate(self, alphabet, states):\n","        self._validate_input_values(alphabet, states)\n","        self._validate_output_types()\n","        self._homogenize_output_types()\n","        self._validate_output_values(states)\n","\n","    def _validate_input_values(self, alphabet, states):\n","        for state, symbol in self._transition_graph.keys():\n","            try:\n","                assert symbol in alphabet\n","\n","            except AssertionError:\n","                msg = 'all input symbols in transition function ' +\\\n","                      'must be in alphabet'\n","                raise ValueError(msg)\n","\n","            try:\n","                assert state in states\n","\n","            except AssertionError:\n","                msg = 'all input states in transition function ' +\\\n","                      'must be in set of states'\n","                raise ValueError(msg)\n","\n","    def _validate_output_types(self):\n","        for states in self._transition_graph.values():\n","            try:\n","                t = type(states)\n","                assert t is str or t is set\n","\n","            except AssertionError:\n","                msg = 'all outputs in transition function' +\\\n","                      'must be specified via str or set'\n","                raise ValueError(msg)            \n","\n","    def _homogenize_output_types(self):\n","        outputs = self._transition_graph.values()\n","\n","        for inp, out in self._transition_graph.items():\n","            if type(out) is str:\n","                self._transition_graph[inp] = {out}\n","\n","    def _validate_output_values(self, states):\n","        for out in self._transition_graph.values():\n","            try:\n","                assert all([state in states for state in out])\n","            except AssertionError:\n","                msg = 'all output symbols in transition function' +\\\n","                      'must be in states'\n","                raise ValueError(msg)\n","\n","    @property\n","    def transition_graph(self):\n","        return self._transition_graph\n","\n","class FiniteStateAutomaton:\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    def __init__(self, alphabet: Set[str], states: Set[str], \n","                 initial_state: str, final_states: Set[str], \n","                 transition_graph: 'TransitionFunction'):\n","        self._alphabet = {''} | alphabet\n","        self._states = states\n","        self._initial_state = initial_state\n","        self._final_states = final_states\n","        self._transition_function = TransitionFunction(transition_graph)\n","\n","        self._validate_initial_state()\n","        self._validate_final_states()\n","        self._transition_function.validate(self._alphabet, states)\n","\n","        self._generator = self._build_generator()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        return next(self._generator)\n","\n","    def _build_generator(self):\n","        string_buffer = [(self._initial_state, '')]\n","        \n","        if self._initial_state in self._final_states:\n","            stack = ['']\n","        else:\n","            stack = []\n","\n","        while string_buffer:\n","            if stack:\n","                yield stack.pop()\n","\n","            else:\n","                # this is very inefficient when we have total\n","                # transition functions with many transitions to the\n","                # sink state; could be optimized by removing a string\n","                # if it's looping in a sink state, but we don't know\n","                # in general what the sink state is\n","                new_buffer = []\n","                for symb in self._alphabet:\n","                    for old_state, string in string_buffer:\n","                        new_states = self._transition_function(old_state, symb)\n","                        for st in new_states:\n","                            new_elem = (st, string+symb)\n","                            new_buffer.append(new_elem)\n","\n","                stack += [string\n","                          for state, string in new_buffer\n","                          if state in self._final_states]\n","\n","                string_buffer = new_buffer"],"metadata":{"id":"CDHQSsAsCTE9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","We often define (and draw) FSAs s.t. their transition functions are partial. We can assume all FSA transition functions $\\delta$ are in fact total by adding a *sink state* $q_\\text{sink} \\not\\in F$ to the FSA and mapping all $\\langle q, \\sigma \\rangle$ pairs for which $\\delta$ is undefined to $q_\\text{sink}$."],"metadata":{"id":"nN2WUjffJuXM"}},{"cell_type":"code","source":["class TransitionFunction(TransitionFunction):\n","    \"\"\"A finite state machine transition function\n","\n","    Parameters\n","    ----------\n","    transition_graph\n","\n","    Attributes\n","    ----------\n","    isdeterministic\n","    istotalfunction\n","    transition_graph\n","\n","    Methods\n","    -------\n","    validate(alphabet, states)\n","    relable_states(state_map)\n","    totalize()\n","    \"\"\"\n","\n","    def istotalfunction(self, states, alphabet):\n","        return all((s, a) in self._transition_graph\n","                    for s in states\n","                    for a in alphabet)\n","\n","    def totalize(self, states, alphabet):\n","        if not self.istotalfunction(states, alphabet):\n","            domain = {(s, a) for s in states for a in alphabet}\n","\n","            sink_state = 'qsink'\n","\n","            while sink_state in states:\n","                sink_state += 'sink'\n","\n","            for inp in domain:\n","                if inp not in self._transition_graph:\n","                    self._transition_graph[inp] = {sink_state}"],"metadata":{"id":"d-bYDFKQKLXR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["It turns out that these two ways of defining FSAs are at least weakly equivalent. That is, the class of languages generated by NFAs is the same as the class generated by DFAs; therefore, both generate exactly the regular languages.\n","\n","## DFAs are (strongly) equivalent to NFAs \n","\n","Every DFA can be converted to a strongly equivalent NFA. A DFA $G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle$ can be converted to an NFA $G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle$ by defining the $G'$ transition function $\\delta'(q,\\sigma) = \\{\\delta(q,\\sigma)\\}$"],"metadata":{"id":"gz9KR4pgCRRQ"}},{"cell_type":"code","source":["class TransitionFunction(TransitionFunction):\n","    \"\"\"A finite state machine transition function\n","\n","    Parameters\n","    ----------\n","    transition_graph\n","\n","    Attributes\n","    ----------\n","    isdeterministic\n","    istotalfunction\n","    transition_graph\n","\n","    Methods\n","    -------\n","    validate(alphabet, states)\n","    relable_states(state_map)\n","    totalize()\n","    \"\"\"\n","\n","    @property\n","    def isdeterministic(self):\n","        return all(len(v) < 2 for v in self._transition_graph.values())\n","\n","class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    @property\n","    def isdeterministic(self) -> bool:\n","        return self._transition_function.isdeterministic"],"metadata":{"id":"-BLPOsSoJCUq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## NFAs are (at least weakly) equivalent to DFAs\n","\n","Every NFA can be converted to a weakly equivalent DFA\n","\n","The $\\epsilon$-closure of $\\delta$ is $$\\delta_\\epsilon(q, \\sigma) =\n","\\begin{cases}\n","\\emptyset & \\text{if }  \\delta(q, \\sigma) \\text{ is undefined}\\\\\n","\\delta(q, \\sigma) \\cup \\bigcup_{r \\in \\delta(q, \\sigma)} \\delta_\\epsilon(r, \\epsilon) & \\text{otherwise} \\\\\n","\\end{cases}$$\n","\n","An NFA $G = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle$ can be converted to a DFA $G' = \\langle Q', \\Sigma, \\delta', q'_0, F' \\rangle$ by defining:\n","\n","1. $Q' = \\mathcal{P}(Q)$\n","2. $\\delta'(R, \\sigma) = \\bigcup_{r\\in R}\\delta_\\epsilon(r, \\sigma)$\n","3. $q'_0 = \\{q_0\\} \\cup \\delta_\\epsilon(q_0, \\epsilon)$\n","4. $F' = \\{R\\;|\\; \\exists r \\in R: r \\in F\\}$\n","\n"],"metadata":{"id":"Rgse7O9UI-7e"}},{"cell_type":"code","source":["from functools import reduce\n","from itertools import chain, combinations\n","\n","from typing import Set, Tuple\n","\n","def powerset(iterable):\n","    \"\"\"https://docs.python.org/3/library/itertools.html#recipes\"\"\"\n","    s = list(iterable)\n","    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))\n","\n","def transitive_closure(edges: Set[Tuple[str]]) -> Set[Tuple[str]]:\n","    \"\"\"\n","    the transitive closure of a graph\n","\n","    Parameters\n","    ----------\n","    edges\n","        the graph to compute the closure of\n","\n","    Returns\n","    ----------\n","    set(tuple)\n","    \"\"\"\n","    while True:\n","        new_edges = {(x, w)\n","                     for x, y in edges\n","                     for q, w in edges\n","                     if q == y}\n","\n","        all_edges = edges | new_edges\n","\n","        if all_edges == edges:\n","            return edges\n","\n","        edges = all_edges\n","\n","class TransitionFunction(TransitionFunction):\n","    \"\"\"A finite state machine transition function\n","\n","    Parameters\n","    ----------\n","    transition_graph\n","\n","    Attributes\n","    ----------\n","    isdeterministic\n","    istotalfunction\n","    transition_graph\n","\n","    Methods\n","    -------\n","    validate(alphabet, states)\n","    relable_states(state_map)\n","    totalize()\n","    \"\"\"\n","\n","    def validate(self, alphabet, states):\n","        self._validate_input_values(alphabet, states)\n","        self._validate_output_types()\n","        self._homogenize_output_types()\n","        self._validate_output_values(states)\n","\n","        self._add_epsilon_transitive_closure()\n","\n","    def _add_epsilon_transitive_closure(self):\n","        # get the state graph of all epsilon transitions\n","        transitions = {(instate, outstate)\n","                       for (instate, insymb), outs in self._transition_graph.items()\n","                       for outstate in outs if not insymb}\n","        \n","        # compute the transitive closure of the epsilon transition\n","        # state graph; requires homogenization beforehand\n","        for instate, outstate in transitive_closure(transitions):\n","            self._transition_graph[(instate, '')] |= {outstate}\n","\n","        new_graph = dict(self._transition_graph)\n","\n","        # add alphabet transitions from all states q_i that exit\n","        # with an alphabet transition (q_i, a) and enter states q_j\n","        # with epsilon transitions to states q_k\n","        for (instate1, insymb1), outs1 in self._transition_graph.items():\n","            for (instate2, insymb2), outs2 in self._transition_graph.items():\n","                if instate2 in outs1 and not insymb2:\n","                    # vacuously adds the already added epsilon\n","                    # transitions as well\n","                    try:\n","                        new_graph[(instate1,insymb1)] |= outs2\n","                    except KeyError:\n","                        new_graph[(instate1,insymb1)] = outs2\n","\n","        self._transition_graph = new_graph\n","\n","    def determinize(self, initial_state):\n","        new_initial_state = {initial_state} | self(initial_state, \"\")\n","        new_transition_graph = {(instate, insymb): outstates\n","                                for (instate, insymb), outstates \n","                                in self._transition_graph.items()\n","                                if insymb}\n","        \n","        return new_initial_state, TransitionFunction(new_transition_graph)\n","\n","\n","class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","        \n","    def determinize(self) -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        if nondeterministic, determinize the FSA\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        new_states = {'_'.join(sorted(s)) for s in powerset(self._states) if s}\n","        # doesn't handle epsilons coming from the start state\n","        new_init, new_trans = self._transition_function.determinize(self._initial_state)\n","        new_final = {'_'.join(sorted(s)) for s in powerset(self._states)\n","                     if any([t in s for t in self._final_states])\n","                     if s}\n","        \n","        new_transition = {('_'.join(sorted(s)), a): {'_'.join(sorted(t))}\n","                          for s in powerset(self._states)\n","                          for t in powerset(self._states)\n","                          for a in self._alphabet\n","                          if any([tprime in new_trans(sprime,a)\n","                                  for sprime in s for tprime in t])\n","                          if s and t}\n","\n","        return FiniteStateAutomaton(self._alphabet-{''}, new_states,\n","                                    '_'.join(sorted(new_init)), new_final,\n","                                    new_transition)\n","\n","    @property\n","    def isdeterministic(self) -> bool:\n","        return self._transition_function.isdeterministic"],"metadata":{"id":"dKfvqypDIGIA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","# The regular operations\n","\n","## Union\n","\n","Given NFAs $M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle$ recognizing $A = \\mathbb{L}(M_1)$ and $M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle$ recognizing $B = \\mathbb{L}(M_2)$, we construct $M = \\text{union}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle$ recognizing $A \\cup B = \\mathbb{L}(M) = \\mathbb{L}(\\text{union}(M_1, M_2))$:\n","\n","1. Relabel $Q_1$ and $Q_2$ so they are mutually exclusive and do not contain $q_0$; $Q = Q'_1 \\cup Q'_2 \\cup \\{q_0\\}$ and $\\Sigma = \\Sigma_1 \\cup \\Sigma_2$\n","2. Define $$\\delta(q, \\sigma) = \\begin{cases}\n","\\{q_1, q_2\\} & \\text{if } q=q_0 \\land \\sigma=\\epsilon \\\\\n","\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\\\\n","\\delta_2(q, \\sigma) & \\text{if } q\\in Q_2 \\\\\n","\\text{undefined} & \\text{otherwise} \\\\\n","\\end{cases}$$\n","3. Define $F = F_1 \\cup F_2$"],"metadata":{"id":"_9OYabfFCuI_"}},{"cell_type":"code","source":["class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    def __or__(self, other):\n","        return self.union(other)\n","\n","    def _deepcopy(self):\n","        fsa = copy(self)\n","        del fsa._generator\n","\n","        return deepcopy(fsa)\n","        \n","    def _relabel_fsas(self, other):\n","        \"\"\"\n","        append tag to the input/ouput states throughout two FSAs\n","\n","        Parameters\n","        ----------\n","        other : FiniteStateAutomaton\n","        \"\"\"\n","\n","        fsa1 = self._deepcopy()._relabel_states(str(id(self)))\n","        fsa2 = other._deepcopy()._relabel_states(str(id(other)))\n","\n","        return fsa1, fsa2\n","\n","    def _relabel_states(self, tag):\n","        \"\"\"\n","        append tag to the input/ouput states throughout the FSA\n","\n","        Parameters\n","        ----------\n","        tag : str\n","        \"\"\"\n","\n","        state_map = {s: s+'_'+tag for s in self._states}\n","        \n","        self._states = {state_map[s] for s in self._states}    \n","        self._initial_state += '_'+tag\n","        self._final_states = {state_map[s] for s in self._final_states}\n","\n","        self._transition_function = self._transition_function\n","        self._transition_function.relabel_states(state_map)\n","        \n","        return self\n","\n","    def union(self, other: 'FiniteStateAutomaton') -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        union this FSA with another\n","\n","        Parameters\n","        ----------\n","        other : FiniteStateAutomaton\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        msg = \"you still need to implement FiniteStateAutomaton.union\"\n","        raise NotImplementedError(msg)"],"metadata":{"id":"ZG5I--AtK4vg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## Concatenation\n","\n","Given NFAs $M_1 = \\langle Q_1, \\Sigma_1, \\delta_1, q_1, F_1 \\rangle$ recognizing $A = \\mathbb{L}(M_1)$ and $M_2 = \\langle Q_2, \\Sigma_2, \\delta_2, q_2, F_2 \\rangle$ recognizing $B = \\mathbb{L}(M_2)$, we construct $M = \\text{concatenate}(M_1, M_2) = \\langle Q, \\Sigma, \\delta, q_1, F_2 \\rangle$ recognizing $A \\circ B = \\mathbb{L}(M) = \\mathbb{L}(\\text{concatenate}(M_1, M_2))$:\n","\n","1. Relabel $Q_1$ and $Q_2$ so they are mutually exclusive; $Q = Q'_1 \\cup Q'_2$ and $\\Sigma = \\Sigma_1 \\cup \\Sigma_2$\n","2. Define $$\\delta(q, \\sigma) = \\begin{cases}\n","\\delta_1(q, \\sigma) & \\text{if } q\\in Q_1 \\land q\\not\\in F_1 \\\\\n","\\delta_1(q, \\sigma) & \\text{if } q\\in F_1 \\land \\sigma \\neq \\epsilon \\\\\n","\\delta_1(q, \\sigma) \\cup \\{q_2\\} & \\text{if } q\\in F_1 \\land \\sigma = \\epsilon \\\\\n","\\delta_2(q, \\sigma)& \\text{if } q\\in Q_2 \\\\\n","\\end{cases}$$\n"],"metadata":{"id":"KI-fE8bHK3Bw"}},{"cell_type":"code","source":["class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","        \n","    def __add__(self, other):\n","        return self.concatenate(other)\n","\n","    def __pow__(self, k):\n","        return self.exponentiate(k)\n","\n","    def concatenate(self, other: 'FiniteStateAutomaton'):\n","        \"\"\"\n","        concatenate this FSA with another\n","\n","        Parameters\n","        ----------\n","        other : FiniteStateAutomaton\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        msg = \"you still need to implement FiniteStateAutomaton.concatenate\"\n","        raise NotImplementedError(msg)\n","\n","    def exponentiate(self, k: int) -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        concatenate this FSA k times\n","\n","        Parameters\n","        ----------\n","        k : int\n","            the number of times to repeat; must be >1\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        if k <= 1:\n","            raise ValueError(\"must be >1\")\n","\n","        new = self\n","\n","        for i in range(1,k):\n","            new += self\n","\n","        return new"],"metadata":{"id":"W5fW2cUyNPTQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","## Kleene Closure\n","\n","Given an NFA $M = \\langle Q, \\Sigma, \\delta, q_0, F \\rangle$ recognizing $A = \\mathbb{L}(M)$, the NFA $N = \\text{kleene}(M) = \\langle Q \\cup \\{q'_0\\}, \\Sigma, \\delta', q'_0\\not\\in Q, F' \\rangle$ recognizing $A^* = \\mathbb{L}(N) = \\mathbb{L}(\\text{kleene}(M))$:\n","\n","1. Define $$\\delta'(q, \\sigma) = \\begin{cases}\n","q_0 & \\text{if } (q = q'_0 \\lor q \\in F) \\land \\sigma = \\epsilon \\\\\n","\\delta(q, \\sigma) & \\text{otherwise} \\\\\n","\\end{cases}$$\n","\n"],"metadata":{"id":"yji-_PXQNOD8"}},{"cell_type":"code","source":["class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    def kleene_star(self)  -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        construct the kleene closure machine\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        fsa = self._deepcopy()\n","        \n","        new_transition = fsa._transition_function\n","        new_transition.add_transitions({(s, ''): fsa._initial_state\n","                                        for s in fsa._final_states})\n","\n","        return FiniteStateAutomaton(fsa._alphabet, fsa._states,\n","                                    fsa._initial_state, fsa._final_states,\n","                                    new_transition.transition_graph)"],"metadata":{"id":"CNk2Rv7CNSkk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Mapping between regular expressions and FSAs\n","\n","It turns out that FSAs can generate exactly the same set of languages as regular expressions. To show this weak equivalence, we must show that every regular expression can be mapped to an equivalent FSA (specifically, NFA) and that every FSA can be mapped to an equivalent regular expression. \n","\n","This fact is useful for our purposes because (as we will see when we cover finite state transducers) it will provide the basis for compactly stating phonological rules using a regular expression-like metalanguage and *compiling* those rules to finite state automata. Thus, the mapping from regular expressions to FSAs will be the most important for us, though in principle the inverse mapping could be useful if we have an FSA and we wish to attempt to extract phonological generalizations from it. \n","\n","## Regular expressions to FSAs\n","\n","Compilation takes advantages of the operations on FSAs that we just just covered.\n","\n","$\\text{compile}_\\Sigma(\\rho) = \\begin{cases}\\langle \\{q_0\\}, \\Sigma, [], q_0, \\{\\} \\rangle & \\text{if } \\rho = \\emptyset\\\\\\langle \\{q_0\\}, \\Sigma, [], q_0, \\{q_0\\} \\rangle & \\text{if } \\rho = \\epsilon\\\\\\langle \\{q_0, q_1\\}, \\Sigma, [\\langle q_0, \\rho \\rangle \\rightarrow q_1], q_0, \\{q_1\\} \\rangle & \\text{if } \\rho \\in \\Sigma\\\\\\text{union}\\left(\\text{compile}_\\Sigma(\\rho_1), \\text{compile}_\\Sigma(\\rho_2)\\right) & \\text{if } \\rho = (\\rho_1 \\cup \\rho_2)\\\\\\text{concatenate}\\left(\\text{compile}_\\Sigma(\\rho_1), \\text{compile}_\\Sigma(\\rho_2)\\right) & \\text{if } \\rho = (\\rho_1 \\circ \\rho_2)\\\\\\text{kleene}\\left(\\text{compile}_\\Sigma(\\rho_0)\\right) & \\text{if } \\rho = \\rho_0^*\\\\\\text{undefined} & \\text{otherwise} \\end{cases}$\n","\n","## FSAs to regular expressions\n","\n","A standard way of converting FSAs to regular expressions is to take an FSA generating a language on $\\Sigma$ and mapping it to an FSA generating a regular expression using [*Kleene's algorithm*](https://en.wikipedia.org/wiki/Kleene%27s_algorithm). I will not cover the algorithm here. "],"metadata":{"id":"HIbYcx0Hfiqd"}},{"cell_type":"markdown","source":["\n","\n","# Further operations\n","\n","## Complementation\n","\n","Given and NFA $M_1 = \\langle Q_1, \\Sigma, \\delta_1, q_1, F_1 \\rangle$ recognizing $A = \\mathbb{L}(M_1)$, we construct NFA $M_2 = \\langle Q_2, \\Sigma, \\delta_2, q_2, F_2 \\rangle$ recognizes $\\overline{A} = \\mathbb{L}(M_2)$\n","\n","1. Determinize $M_1$ to $M'_1 = \\langle Q', \\Sigma', \\delta', q_0', F'_1 \\rangle$ and define $Q_2 = Q'_1 \\cup q_\\text{sink}$ and $q_2 = q'_1$.\n","2. Define $\\delta_2 = \\mathrm{totalize}(\\delta'_1)$, where $\\mathrm{totalize}$ adds mappings to $q_\\text{sink}$ where $\\delta'_1$ is undefined\n","3. Define $F_2 = Q'_1 - F'_1$\n","\n"],"metadata":{"id":"ASm94PNzNRjG"}},{"cell_type":"code","source":["class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    def __neg__(self):\n","        return self.complement()\n","\n","    def complement(self) -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        complement this FSA\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        fsa = self._deepcopy()\n","        fsa = fsa.determinize()\n","        fsa._transition_function.totalize(fsa._states, fsa._alphabet)\n","        fsa._final_states = fsa._states - fsa._final_states\n","\n","        return fsa"],"metadata":{"id":"JR-tQ6kONX-3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","## Intersection\n","\n","Once we can compute the union and complementation of two machines, we get intersection for free. To see this, note that, according to [de Morgan's Laws](https://en.wikipedia.org/wiki/De_Morgan%27s_laws), $\\overline{A \\cup B} = \\overline{A} \\cap \\overline{B}$. Thus, if we want the intersection $A \\cap B = \\overline{\\overline{A \\cap B}} = \\overline{\\overline{A} \\cup \\overline{B}}$.\n","\n","The ability to compute intersection is very important for us because it allows us to express constraints on licit sequences. This ability in turn means that we can build grammars piecemeal. For instance, suppose we compute the Kleene closure for our syllable machine. We might view this as an English word machine. But the problem if that impossible words will end up generated by this machine: words with double consonants (geminates). \n","\n","The idea behind imposing a constraint in this context is that we can build another machine to filter geminates then intersect it with the word machine. How?\n","\n"],"metadata":{"id":"AuvG54giNUwi"}},{"cell_type":"code","source":["class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","\n","    def __and__(self, other):\n","        return self.intersect(other)\n","\n","    def intersect(self, other: 'FiniteStateAutomaton') -> 'FiniteStateAutomaton':\n","        \"\"\"\n","        intersect this FSA with another\n","\n","        Parameters\n","        ----------\n","        other : FiniteStateAutomaton\n","\n","        Returns\n","        -------\n","        FiniteStateAutomaton\n","        \"\"\"\n","        fsa1 = self.complement()\n","        fsa2 = other.complement()\n","\n","        return fsa1.union(fsa2).complement()"],"metadata":{"id":"Efn-DxYANZSd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Recognition and Parsing\n","\n","A recognizer for a grammar $G$ determines whether a string $\\mathbf{s} \\in \\mathbb{L}(G)$. For FSAs, we can define a recognizer recursivesly.\n","\n","$\\text{recognize}_{\\langle Q, \\Sigma, \\delta, q_0, F \\rangle}(\\mathbf{s}, q) = \\begin{cases}\\exists q''' \\in \\delta(\\text{head}(\\mathbf{s}), q) \\cup \\{q''\\mid q' \\in \\delta_\\epsilon(\\epsilon, q) \\text{ and } q'' \\in \\delta(\\text{head}(\\mathbf{s}), q')\\}: \\text{recognize}_{\\langle Q, \\Sigma, \\delta, q_0, F \\rangle}(\\text{tail}(\\mathbf{s}), q''') & \\text{if } |\\mathbf{s}| > 0\\\\\\{q\\} \\cup \\delta_\\epsilon(\\epsilon, q) \\subseteq F & \\text{if } |\\mathbf{s}| = 0\\\\\\end{cases}$\n","\n","A parser looks very similar to a recognizer except that it additionally tracks the states that are traversed in recognizing the string. Because there are possibly multiple such traversals, a parser needs to produce a collection of such state traversal."],"metadata":{"id":"uLLzE09tDQMX"}},{"cell_type":"code","source":["FiniteStateAutomatonParse = List[Tuple[str, str]]\n","\n","\n","class FiniteStateAutomaton(FiniteStateAutomaton):\n","    \"\"\"A finite state automaton\n","\n","    Parameters\n","    ----------\n","    alphabet\n","    states\n","    initial_state\n","    final states\n","    transition_graph\n","    \"\"\"\n","      def __call__(self, string: List[str], mode: str=\"recognize\") -> Union[bool, List[FiniteStateAutomatonParse]]:\n","        \"\"\"\n","        whether/how a string is accepted/parsed by the FSA\n","\n","        Parameters\n","        ----------\n","        string\n","            the string to recognize or parse\n","        mode\n","            whether to run in \"recognize\" or \"parse\" mode\n","        \"\"\"\n","\n","        if mode == 'recognize':\n","            return self._recognize(string)\n","        elif mode == 'parse':\n","            return self._parse(string)\n","        else:\n","            msg = 'mode must be \"recognize\" or \"parse\"'\n","            raise ValueError(msg)\n","\n","    @lru_cache(512)\n","    def _recognize(self, string: List[str], prev_state: Optional[str]=None) -> bool:\n","        \"\"\"Whether a string is accepted by the FSA\n","\n","        Parameters\n","        ----------\n","        string\n","            the string to recognize or parse\n","        \"\"\"\n","        prev_states = {self._initial_state} if prev_state is None else {prev_state}\n","        prev_states |= {s for s in self._transition_function(prev_state, '')}\n","        \n","        if string:\n","            return any(self._recognize(string[1:], state)\n","                       for s in prev_states\n","                       for state in self._transition_function(s, string[0]))\n","\n","        else:\n","            return any(s in self._final_states for s in prev_states)\n","\n","    @lru_cache(512)\n","    def _parse(self, string: List[str], \n","               prev_state: Optional[str]=None) -> List[List[Tuple[str, Optional[str]]]]:\n","        \"\"\"How a string is parsed by the FSA\n","        \n","        This should return the list of transitions that \n","        the machine could go through to parse a string\n","\n","        Parameters\n","        ----------\n","        string\n","            the string to recognize or parse\n","        \"\"\"\n","        prev_state = self._initial_state if prev_state is None else prev_state\n","        prev_states_epsilon = {s for s in self._transition_function(prev_state, '')}\n","        \n","        if string:\n","            parses = []\n","        \n","            for state in self._transition_function(prev_state, string[0]):\n","                parses.extend([[(prev_state, string[0])] + p \n","                               for p in self._parse(string[1:], state)])\n","            \n","            for s in prev_states_epsilon:\n","                for state in self._transition_function(s, string[0]):\n","                    parses.extend([[(prev_state, ''), (s, string[0])] + p \n","                                   for p in self._parse(string[1:], state)])\n","            \n","            return parses\n","\n","        else:\n","            return [[] \n","                    for s in {prev_state} | prev_states_epsilon \n","                    if s in self._final_states]\n"," "],"metadata":{"id":"TCGf2lzdDUzG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Transduction\n","\n","So far, we have constructed morphophonological grammar in a [constraint-based](https://en.wikipedia.org/wiki/Model-theoretic_grammar) way. In this sort of approch, we define a set that contains all the strings we want (in the limit, $\\Sigma^*$ itself); then, we filter out strings that violate a constraint (such as the one against gemination by complement and intersection operation. \n","\n","We can alterntively model morphophonological grammars using a [generative](https://en.wikipedia.org/wiki/Generative_grammar) approach. This could be accomplished using vanilla FSAs, like those discussed above; but it is often more productive to approach the problem as one of transduction: wherein we define some set of *underlying forms* that are mapped to surface forms by machines that implement morphophonological rules. These machines are generalizations of FSAs that we refer to as Finite State Transducers (FSTs). The main thing we add to an FSA to get an FSTs is that every edge is annotated with a mapping from a character in the input alphabet $\\Sigma$ (or $\\epsilon$) to one in the output alphabet $\\Gamma$ (or $\\epsilon$). The entire machine can then be viewed as a method for defining a relation between $\\Sigma^*$ (the underlying forms) and $\\Gamma^*$ (the surface forms). All the relations definable by an FST are known as the *regular relations*.\n","\n","There are two ways that we can define FSTs: one that focuses on their use as methods for defining a relation and another that focuses on their use as methods for defining mappings from $\\Sigma^*$ to $2^{\\Gamma^*}$‚Äìi.e. as mappings from strings in $\\Sigma^*$ to languages on $\\Gamma$.\n","\n","## The relation view\n","\n","A Finite State Transducer (FST) is a grammar with 6 components:\n","\n","1. A set of states $Q$\n","2. An input alphabet $\\Sigma$\n","3. An output alphabet $\\Gamma$\n","4. A transition function $\\delta : Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\times (\\Gamma \\cup \\{\\epsilon\\}) \\rightarrow \\mathcal{P}(Q)$\n","5. An initial state $q_0$\n","6. A set of final states $F \\subseteq Q$\n","\n","## The mapping view\n","\n","Alternatively, a Finite State Transducer (FST) is a grammar with 7 components:\n","\n","1. A set of states $Q$\n","2. An input alphabet $\\Sigma$\n","3. An output alphabet $\\Gamma$\n","4. A transition function $\\delta : Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\rightarrow \\mathcal{P}(Q)$\n","5. An output function $\\omega : Q \\times (\\Sigma \\cup \\{\\epsilon\\}) \\times Q \\rightarrow \\Gamma$\n","6. An initial state $q_0 \\in Q$\n","7. A set of final states $F \\subseteq Q$\n","\n","FSTs in general are similar to vanilla FSAs:\n","\n","1. Closed under union\n","2. Closed under concatenation\n","3. Closed under Kleene star\n","\n","But they are different in a few respects:\n","\n","1. Not all FSTs are determinizable.\n","2. FSTs aren't closed under intersection or complementation\n","3. Conceptualizing FSTs as language tuple recognizers/generators, we can define *projection*.\n","4. Conceptualizing FSTs as maps, we can define *composition* and *inversion*."],"metadata":{"id":"G7Q-ZoF2yFyg"}}]}